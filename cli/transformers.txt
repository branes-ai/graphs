(p311) stillwater@sw-21:~/dev/branes/clones/graphs$ python cli/discover_transformers.py
====================================================================================================
HUGGINGFACE TRANSFORMER MODEL DISCOVERY
====================================================================================================

Total models to test: 22
Sequence length: 128

Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

====================================================================================================
SUMMARY
====================================================================================================

Traceable:     22 models
  - FX trace:    0 models
  - Dynamo:      22 models
Failed:        0 models

====================================================================================================
TRACEABLE MODELS BY FAMILY
====================================================================================================

ELEUTHERAI (2):
  EleutherAI/gpt-neo-125m                       [dynamo         ] (no mask)
  EleutherAI/gpt-neo-1.3B                       [dynamo         ] (no mask)

ALBERT (2):
  albert-base-v2                                [dynamo         ] (w/ mask)
  albert-large-v2                               [dynamo         ] (w/ mask)

BERT (3):
  bert-base-uncased                             [dynamo         ] (w/ mask)
  bert-base-cased                               [dynamo         ] (w/ mask)
  bert-large-uncased                            [dynamo         ] (w/ mask)

DISTILBERT (2):
  distilbert-base-uncased                       [dynamo         ] (w/ mask)
  distilbert-base-cased                         [dynamo         ] (w/ mask)

DISTILGPT2 (1):
  distilgpt2                                    [dynamo         ] (no mask)

FACEBOOK (2):
  facebook/opt-125m                             [dynamo         ] (no mask)
  facebook/opt-350m                             [dynamo         ] (no mask)

GOOGLE (2):
  google/electra-small-discriminator            [dynamo         ] (w/ mask)
  google/electra-base-discriminator             [dynamo         ] (w/ mask)

GPT2 (3):
  gpt2                                          [dynamo         ] (no mask)
  gpt2-medium                                   [dynamo         ] (no mask)
  gpt2-large                                    [dynamo         ] (no mask)

MICROSOFT (2):
  microsoft/deberta-v3-small                    [dynamo         ] (w/ mask)
  microsoft/deberta-v3-base                     [dynamo         ] (w/ mask)

ROBERTA (2):
  roberta-base                                  [dynamo         ] (w/ mask)
  roberta-large                                 [dynamo         ] (w/ mask)

XLM (1):
  xlm-roberta-base                              [dynamo         ] (w/ mask)


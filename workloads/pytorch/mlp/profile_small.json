{"traceEvents": [{"ph": "M", "pid": 15268, "tid": 15268, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 15268, "tid": 6692, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 15268, "tid": 6692, "ts": 2894645185.007, "ph": "X", "dur": 0.9, "name": "Sequential.__iter__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:230)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645192.207, "ph": "X", "dur": 0.5, "name": "Module.__getattr__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645193.207, "ph": "X", "dur": 0.2, "name": "Module.__getattr__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645189.507, "ph": "X", "dur": 1032.4, "name": "Linear.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:124)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645187.707, "ph": "X", "dur": 1034.7, "name": "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645186.707, "ph": "X", "dur": 1036.3, "name": "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646230.107, "ph": "X", "dur": 477.0, "name": "Tanh.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:391)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646225.907, "ph": "X", "dur": 481.5, "name": "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646224.707, "ph": "X", "dur": 483.1, "name": "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646716.107, "ph": "X", "dur": 0.8, "name": "Module.__getattr__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646717.507, "ph": "X", "dur": 0.2, "name": "Module.__getattr__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646714.307, "ph": "X", "dur": 28.5, "name": "Linear.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:124)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646712.307, "ph": "X", "dur": 30.7, "name": "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646711.507, "ph": "X", "dur": 31.8, "name": "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646750.207, "ph": "X", "dur": 60.2, "name": "softmax (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\functional.py:2103)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646747.307, "ph": "X", "dur": 63.3, "name": "Softmax.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1671)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646745.207, "ph": "X", "dur": 65.5, "name": "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894646744.407, "ph": "X", "dur": 66.5, "name": "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645184.007, "ph": "X", "dur": 1627.9, "name": "Sequential.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:238)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645177.007, "ph": "X", "dur": 1635.1, "name": "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)", "cat": "FEE"}, {"pid": 15268, "tid": 6692, "ts": 2894645171.407, "ph": "X", "dur": 1641.0, "name": "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1755087371843663500}, "file_info": {"files": {"C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py": ["# mypy: allow-untyped-decorators\n# mypy: allow-untyped-defs\nimport operator\nfrom collections import abc as container_abcs, OrderedDict\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom itertools import chain, islice\nfrom typing import Any, Optional, overload, TypeVar, Union\nfrom typing_extensions import deprecated, Self\n\nimport torch\nfrom torch._jit_internal import _copy_to_script_wrapper\nfrom torch.nn.parameter import Parameter\n\nfrom .module import Module\n\n\n__all__ = [\n    \"Container\",\n    \"Sequential\",\n    \"ModuleList\",\n    \"ModuleDict\",\n    \"ParameterList\",\n    \"ParameterDict\",\n]\n\nT = TypeVar(\"T\", bound=Module)\n\n\n# Copied from torch.nn.modules.module, required for a custom __repr__ for ModuleList\ndef _addindent(s_, numSpaces):\n    s = s_.split(\"\\n\")\n    # don't do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * \" \") + line for line in s]\n    s = \"\\n\".join(s)\n    s = first + \"\\n\" + s\n    return s\n\n\n@deprecated(\n    \"`nn.Container` is deprecated. \"\n    \"All of it's functionality is now implemented in `nn.Module`. Subclass that instead.\",\n    category=FutureWarning,\n)\nclass Container(Module):\n    def __init__(self, **kwargs: Any) -> None:\n        super().__init__()\n        for key, value in kwargs.items():\n            self.add_module(key, value)\n\n\nclass Sequential(Module):\n    r\"\"\"A sequential container.\n\n    Modules will be added to it in the order they are passed in the\n    constructor. Alternatively, an ``OrderedDict`` of modules can be\n    passed in. The ``forward()`` method of ``Sequential`` accepts any\n    input and forwards it to the first module it contains. It then\n    \"chains\" outputs to inputs sequentially for each subsequent module,\n    finally returning the output of the last module.\n\n    The value a ``Sequential`` provides over manually calling a sequence\n    of modules is that it allows treating the whole container as a\n    single module, such that performing a transformation on the\n    ``Sequential`` applies to each of the modules it stores (which are\n    each a registered submodule of the ``Sequential``).\n\n    What's the difference between a ``Sequential`` and a\n    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n    sounds like--a list for storing ``Module`` s! On the other hand,\n    the layers in a ``Sequential`` are connected in a cascading way.\n\n    Example::\n\n        # Using Sequential to create a small model. When `model` is run,\n        # input will first be passed to `Conv2d(1,20,5)`. The output of\n        # `Conv2d(1,20,5)` will be used as the input to the first\n        # `ReLU`; the output of the first `ReLU` will become the input\n        # for `Conv2d(20,64,5)`. Finally, the output of\n        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n        model = nn.Sequential(\n                  nn.Conv2d(1,20,5),\n                  nn.ReLU(),\n                  nn.Conv2d(20,64,5),\n                  nn.ReLU()\n                )\n\n        # Using Sequential with OrderedDict. This is functionally the\n        # same as the above code\n        model = nn.Sequential(OrderedDict([\n                  ('conv1', nn.Conv2d(1,20,5)),\n                  ('relu1', nn.ReLU()),\n                  ('conv2', nn.Conv2d(20,64,5)),\n                  ('relu2', nn.ReLU())\n                ]))\n    \"\"\"\n\n    _modules: dict[str, Module]  # type: ignore[assignment]\n\n    @overload\n    def __init__(self, *args: Module) -> None:\n        ...\n\n    @overload\n    def __init__(self, arg: \"OrderedDict[str, Module]\") -> None:\n        ...\n\n    def __init__(self, *args):\n        super().__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n\n    def _get_item_by_idx(self, iterator, idx) -> T:  # type: ignore[misc, type-var]\n        \"\"\"Get the idx-th item of the iterator.\"\"\"\n        size = len(self)\n        idx = operator.index(idx)\n        if not -size <= idx < size:\n            raise IndexError(f\"index {idx} is out of range\")\n        idx %= size\n        return next(islice(iterator, idx, None))\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, idx: Union[slice, int]) -> Union[\"Sequential\", T]:\n        if isinstance(idx, slice):\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n        else:\n            return self._get_item_by_idx(self._modules.values(), idx)\n\n    def __setitem__(self, idx: int, module: Module) -> None:\n        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx: Union[slice, int]) -> None:\n        if isinstance(idx, slice):\n            for key in list(self._modules.keys())[idx]:\n                delattr(self, key)\n        else:\n            key = self._get_item_by_idx(self._modules.keys(), idx)\n            delattr(self, key)\n        # To preserve numbering\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    def __add__(self, other) -> \"Sequential\":\n        if isinstance(other, Sequential):\n            ret = Sequential()\n            for layer in self:\n                ret.append(layer)\n            for layer in other:\n                ret.append(layer)\n            return ret\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {str(type(other))} is given.\"\n            )\n\n    def pop(self, key: Union[int, slice]) -> Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def __iadd__(self, other) -> Self:\n        if isinstance(other, Sequential):\n            offset = len(self)\n            for i, module in enumerate(other):\n                self.add_module(str(i + offset), module)\n            return self\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {str(type(other))} is given.\"\n            )\n\n    def __mul__(self, other: int) -> \"Sequential\":\n        if not isinstance(other, int):\n            raise TypeError(\n                f\"unsupported operand type(s) for *: {type(self)} and {type(other)}\"\n            )\n        elif other <= 0:\n            raise ValueError(\n                f\"Non-positive multiplication factor {other} for {type(self)}\"\n            )\n        else:\n            combined = Sequential()\n            offset = 0\n            for _ in range(other):\n                for module in self:\n                    combined.add_module(str(offset), module)\n                    offset += 1\n            return combined\n\n    def __rmul__(self, other: int) -> \"Sequential\":\n        return self.__mul__(other)\n\n    def __imul__(self, other: int) -> Self:\n        if not isinstance(other, int):\n            raise TypeError(\n                f\"unsupported operand type(s) for *: {type(self)} and {type(other)}\"\n            )\n        elif other <= 0:\n            raise ValueError(\n                f\"Non-positive multiplication factor {other} for {type(self)}\"\n            )\n        else:\n            len_original = len(self)\n            offset = len(self)\n            for _ in range(other - 1):\n                for i in range(len_original):\n                    self.add_module(str(i + offset), self._modules[str(i)])\n                offset += len_original\n            return self\n\n    @_copy_to_script_wrapper\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[Module]:\n        return iter(self._modules.values())\n\n    # NB: We can't really type check this function as the type of input\n    # may change dynamically (as is tested in\n    # TestScript.test_sequential_intermediary_types).  Cannot annotate\n    # with Any as TorchScript expects a more precise type\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n        return input\n\n    def append(self, module: Module) -> \"Sequential\":\n        r\"\"\"Append a given module to the end.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def insert(self, index: int, module: Module) -> \"Sequential\":\n        if not isinstance(module, Module):\n            raise AssertionError(f\"module should be of type: {Module}\")\n        n = len(self._modules)\n        if not (-n <= index <= n):\n            raise IndexError(f\"Index out of range: {index}\")\n        if index < 0:\n            index += n\n        for i in range(n, index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n        return self\n\n    def extend(self, sequential) -> \"Sequential\":\n        for layer in sequential:\n            self.append(layer)\n        return self\n\n\nclass ModuleList(Module):\n    r\"\"\"Holds submodules in a list.\n\n    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n    modules it contains are properly registered, and will be visible by all\n    :class:`~torch.nn.Module` methods.\n\n    Args:\n        modules (iterable, optional): an iterable of modules to add\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n            def forward(self, x):\n                # ModuleList can act as an iterable, or be indexed using ints\n                for i, l in enumerate(self.linears):\n                    x = self.linears[i // 2](x) + l(x)\n                return x\n    \"\"\"\n\n    _modules: dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Iterable[Module]] = None) -> None:\n        super().__init__()\n        if modules is not None:\n            self += modules\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) <= idx < len(self)):\n            raise IndexError(f\"index {idx} is out of range\")\n        if idx < 0:\n            idx += len(self)\n        return str(idx)\n\n    @overload\n    def __getitem__(self, idx: slice) -> \"ModuleList\":\n        ...\n\n    @overload\n    def __getitem__(self, idx: int) -> Module:\n        ...\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, idx: Union[int, slice]) -> Union[Module, \"ModuleList\"]:\n        if isinstance(idx, slice):\n            return self.__class__(list(self._modules.values())[idx])\n        else:\n            return self._modules[self._get_abs_string_index(idx)]\n\n    def __setitem__(self, idx: int, module: Module) -> None:\n        idx = self._get_abs_string_index(idx)\n        return setattr(self, str(idx), module)\n\n    def __delitem__(self, idx: Union[int, slice]) -> None:\n        if isinstance(idx, slice):\n            for k in range(len(self._modules))[idx]:\n                delattr(self, str(k))\n        else:\n            delattr(self, self._get_abs_string_index(idx))\n        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[Module]:\n        return iter(self._modules.values())\n\n    def __iadd__(self, modules: Iterable[Module]) -> Self:\n        return self.extend(modules)\n\n    def __add__(self, other: Iterable[Module]) -> \"ModuleList\":\n        combined = ModuleList()\n        for i, module in enumerate(chain(self, other)):\n            combined.add_module(str(i), module)\n        return combined\n\n    def __repr__(self):\n        \"\"\"Return a custom repr for ModuleList that compresses repeated module representations.\"\"\"\n        list_of_reprs = [repr(item) for item in self]\n        if len(list_of_reprs) == 0:\n            return self._get_name() + \"()\"\n\n        start_end_indices = [[0, 0]]\n        repeated_blocks = [list_of_reprs[0]]\n        for i, r in enumerate(list_of_reprs[1:], 1):\n            if r == repeated_blocks[-1]:\n                start_end_indices[-1][1] += 1\n                continue\n\n            start_end_indices.append([i, i])\n            repeated_blocks.append(r)\n\n        lines = []\n        main_str = self._get_name() + \"(\"\n        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n            local_repr = f\"({start_id}): {b}\"  # default repr\n\n            if start_id != end_id:\n                n = end_id - start_id + 1\n                local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n            local_repr = _addindent(local_repr, 2)\n            lines.append(local_repr)\n\n        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n        main_str += \")\"\n        return main_str\n\n    @_copy_to_script_wrapper\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def insert(self, index: int, module: Module) -> None:\n        r\"\"\"Insert a given module before a given index in the list.\n\n        Args:\n            index (int): index to insert.\n            module (nn.Module): module to insert\n        \"\"\"\n        for i in range(len(self._modules), index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n\n    def append(self, module: Module) -> \"ModuleList\":\n        r\"\"\"Append a given module to the end of the list.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def pop(self, key: Union[int, slice]) -> Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def extend(self, modules: Iterable[Module]) -> Self:\n        r\"\"\"Append modules from a Python iterable to the end of the list.\n\n        Args:\n            modules (iterable): iterable of modules to append\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleList.extend should be called with an \"\n                \"iterable, but got \" + type(modules).__name__\n            )\n        offset = len(self)\n        for i, module in enumerate(modules):\n            self.add_module(str(offset + i), module)\n        return self\n\n    # remove forward alltogether to fallback on Module's _forward_unimplemented\n\n\nclass ModuleDict(Module):\n    r\"\"\"Holds submodules in a dictionary.\n\n    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,\n    but modules it contains are properly registered, and will be visible by all\n    :class:`~torch.nn.Module` methods.\n\n    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects\n\n    * the order of insertion, and\n\n    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged\n      ``OrderedDict``, ``dict`` (started from Python 3.6) or another\n      :class:`~torch.nn.ModuleDict` (the argument to\n      :meth:`~torch.nn.ModuleDict.update`).\n\n    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping\n    types (e.g., Python's plain ``dict`` before Python version 3.6) does not\n    preserve the order of the merged mapping.\n\n    Args:\n        modules (iterable, optional): a mapping (dictionary) of (string: module)\n            or an iterable of key-value pairs of type (string, module)\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.choices = nn.ModuleDict({\n                        'conv': nn.Conv2d(10, 10, 3),\n                        'pool': nn.MaxPool2d(3)\n                })\n                self.activations = nn.ModuleDict([\n                        ['lrelu', nn.LeakyReLU()],\n                        ['prelu', nn.PReLU()]\n                ])\n\n            def forward(self, x, choice, act):\n                x = self.choices[choice](x)\n                x = self.activations[act](x)\n                return x\n    \"\"\"\n\n    _modules: dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -> None:\n        super().__init__()\n        if modules is not None:\n            self.update(modules)\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, key: str) -> Module:\n        return self._modules[key]\n\n    def __setitem__(self, key: str, module: Module) -> None:\n        self.add_module(key, module)\n\n    def __delitem__(self, key: str) -> None:\n        del self._modules[key]\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._modules)\n\n    @_copy_to_script_wrapper\n    def __contains__(self, key: str) -> bool:\n        return key in self._modules\n\n    def clear(self) -> None:\n        \"\"\"Remove all items from the ModuleDict.\"\"\"\n        self._modules.clear()\n\n    def pop(self, key: str) -> Module:\n        r\"\"\"Remove key from the ModuleDict and return its module.\n\n        Args:\n            key (str): key to pop from the ModuleDict\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    @_copy_to_script_wrapper\n    def keys(self) -> Iterable[str]:\n        r\"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n        return self._modules.keys()\n\n    @_copy_to_script_wrapper\n    def items(self) -> Iterable[tuple[str, Module]]:\n        r\"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n        return self._modules.items()\n\n    @_copy_to_script_wrapper\n    def values(self) -> Iterable[Module]:\n        r\"\"\"Return an iterable of the ModuleDict values.\"\"\"\n        return self._modules.values()\n\n    def update(self, modules: Mapping[str, Module]) -> None:\n        r\"\"\"Update the :class:`~torch.nn.ModuleDict` with key-value pairs from a mapping, overwriting existing keys.\n\n        .. note::\n            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or\n            an iterable of key-value pairs, the order of new elements in it is preserved.\n\n        Args:\n            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,\n                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleDict.update should be called with an \"\n                \"iterable of key/value pairs, but got \" + type(modules).__name__\n            )\n\n        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n            for key, module in modules.items():\n                self[key] = module\n        else:\n            # modules here can be a list with two items\n            for j, m in enumerate(modules):\n                if not isinstance(m, container_abcs.Iterable):\n                    raise TypeError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                    )\n                if not len(m) == 2:\n                    raise ValueError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                    )\n                # modules can be Mapping (what it's typed at), or a list: [(name1, module1), (name2, module2)]\n                # that's too cumbersome to type correctly with overloads, so we add an ignore here\n                self[m[0]] = m[1]  # type: ignore[assignment]\n\n    # remove forward alltogether to fallback on Module's _forward_unimplemented\n\n\nclass ParameterList(Module):\n    r\"\"\"Holds parameters in a list.\n\n    :class:`~torch.nn.ParameterList` can be used like a regular Python\n    list, but Tensors that are :class:`~torch.nn.Parameter` are properly registered,\n    and will be visible by all :class:`~torch.nn.Module` methods.\n\n    Note that the constructor, assigning an element of the list, the\n    :meth:`~torch.nn.ParameterList.append` method and the :meth:`~torch.nn.ParameterList.extend`\n    method will convert any :class:`~torch.Tensor` into :class:`~torch.nn.Parameter`.\n\n    Args:\n        parameters (iterable, optional): an iterable of elements to add to the list.\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n            def forward(self, x):\n                # ParameterList can act as an iterable, or be indexed using ints\n                for i, p in enumerate(self.params):\n                    x = self.params[i // 2].mm(x) + p.mm(x)\n                return x\n    \"\"\"\n\n    def __init__(self, values: Optional[Iterable[Any]] = None) -> None:\n        super().__init__()\n        self._size = 0\n        if values is not None:\n            self += values\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) <= idx < len(self)):\n            raise IndexError(f\"index {idx} is out of range\")\n        if idx < 0:\n            idx += len(self)\n        return str(idx)\n\n    @overload\n    def __getitem__(self, idx: int) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self: T, idx: slice) -> T:\n        ...\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            out = self.__class__()\n            for i in range(start, stop, step):\n                out.append(self[i])\n            return out\n        else:\n            idx = self._get_abs_string_index(idx)\n            return getattr(self, str(idx))\n\n    def __setitem__(self, idx: int, param: Any) -> None:\n        # Note that all other function that add an entry to the list part of\n        # the ParameterList end up here. So this is the only place where we need\n        # to wrap things into Parameter if needed.\n        # Objects added via setattr() are not in the list part and thus won't\n        # call into this function.\n        idx = self._get_abs_string_index(idx)\n        if isinstance(param, torch.Tensor) and not isinstance(param, Parameter):\n            param = Parameter(param)\n        return setattr(self, str(idx), param)\n\n    def __len__(self) -> int:\n        return self._size\n\n    def __iter__(self) -> Iterator[Any]:\n        return iter(self[i] for i in range(len(self)))\n\n    def __iadd__(self, parameters: Iterable[Any]) -> Self:\n        return self.extend(parameters)\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def append(self, value: Any) -> \"ParameterList\":\n        \"\"\"Append a given value at the end of the list.\n\n        Args:\n            value (Any): value to append\n        \"\"\"\n        new_idx = len(self)\n        self._size += 1\n        self[new_idx] = value\n        return self\n\n    def extend(self, values: Iterable[Any]) -> Self:\n        \"\"\"Append values from a Python iterable to the end of the list.\n\n        Args:\n            values (iterable): iterable of values to append\n        \"\"\"\n        # Tensor is an iterable but we never want to unpack it here\n        if not isinstance(values, container_abcs.Iterable) or isinstance(\n            values, torch.Tensor\n        ):\n            raise TypeError(\n                \"ParameterList.extend should be called with an \"\n                \"iterable, but got \" + type(values).__name__\n            )\n        for value in values:\n            self.append(value)\n        return self\n\n    def extra_repr(self) -> str:\n        child_lines = []\n        for k, p in enumerate(self):\n            if isinstance(p, torch.Tensor):\n                size_str = \"x\".join(str(size) for size in p.size())\n                if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n                    device_str = f\" ({p.device})\"\n                else:\n                    device_str = \"\"\n                parastr = \"{} containing: [{} of size {}{}]\".format(\n                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n                    p.dtype,\n                    size_str,\n                    device_str,\n                )\n                child_lines.append(\"  (\" + str(k) + \"): \" + parastr)\n            else:\n                child_lines.append(\n                    \"  (\" + str(k) + \"): Object of type: \" + type(p).__name__\n                )\n\n        tmpstr = \"\\n\".join(child_lines)\n        return tmpstr\n\n    def __call__(self, *args, **kwargs):\n        raise RuntimeError(\"ParameterList should not be called.\")\n\n\nclass ParameterDict(Module):\n    r\"\"\"Holds parameters in a dictionary.\n\n    ParameterDict can be indexed like a regular Python dictionary, but Parameters it\n    contains are properly registered, and will be visible by all Module methods.\n    Other objects are treated as would be done by a regular Python dictionary\n\n    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary.\n    :meth:`~torch.nn.ParameterDict.update` with other unordered mapping\n    types (e.g., Python's plain ``dict``) does not preserve the order of the\n    merged mapping. On the other hand, ``OrderedDict`` or another :class:`~torch.nn.ParameterDict`\n    will preserve their ordering.\n\n    Note that the constructor, assigning an element of the dictionary and the\n    :meth:`~torch.nn.ParameterDict.update` method will convert any :class:`~torch.Tensor` into\n    :class:`~torch.nn.Parameter`.\n\n    Args:\n        values (iterable, optional): a mapping (dictionary) of\n            (string : Any) or an iterable of key-value pairs\n            of type (string, Any)\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.params = nn.ParameterDict({\n                        'left': nn.Parameter(torch.randn(5, 10)),\n                        'right': nn.Parameter(torch.randn(5, 10))\n                })\n\n            def forward(self, x, choice):\n                x = self.params[choice].mm(x)\n                return x\n    \"\"\"\n\n    def __init__(self, parameters: Any = None) -> None:\n        super().__init__()\n        self._keys: dict[str, None] = {}\n        if parameters is not None:\n            self.update(parameters)\n\n    def _key_to_attr(self, key: str) -> str:\n        if not isinstance(key, str):\n            raise TypeError(\n                \"Index given to ParameterDict cannot be used as a key as it is \"\n                f\"not a string (type is '{type(key).__name__}'). Open an issue on \"\n                \"github if you need non-string keys.\"\n            )\n        else:\n            # Use the key as-is so that `.named_parameters()` returns the right thing\n            return key\n\n    def __getitem__(self, key: str) -> Any:\n        attr = self._key_to_attr(key)\n        return getattr(self, attr)\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        # Note that all other function that add an entry to the dictionary part of\n        # the ParameterDict end up here. So this is the only place where we need\n        # to wrap things into Parameter if needed.\n        # Objects added via setattr() are not in the dictionary part and thus won't\n        # call into this function.\n        self._keys[key] = None\n        attr = self._key_to_attr(key)\n        if isinstance(value, torch.Tensor) and not isinstance(value, Parameter):\n            value = Parameter(value)\n        setattr(self, attr, value)\n\n    def __delitem__(self, key: str) -> None:\n        del self._keys[key]\n        attr = self._key_to_attr(key)\n        delattr(self, attr)\n\n    def __len__(self) -> int:\n        return len(self._keys)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._keys)\n\n    def __reversed__(self) -> Iterator[str]:\n        return reversed(list(self._keys))\n\n    def copy(self) -> \"ParameterDict\":\n        \"\"\"Return a copy of this :class:`~torch.nn.ParameterDict` instance.\"\"\"\n        # We have to use an OrderedDict because the ParameterDict constructor\n        # behaves differently on plain dict vs OrderedDict\n        return ParameterDict(OrderedDict((k, self[k]) for k in self._keys))\n\n    def __contains__(self, key: str) -> bool:\n        return key in self._keys\n\n    def setdefault(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"Set the default for a key in the Parameterdict.\n\n        If key is in the ParameterDict, return its value.\n        If not, insert `key` with a parameter `default` and return `default`.\n        `default` defaults to `None`.\n\n        Args:\n            key (str): key to set default for\n            default (Any): the parameter set to the key\n        \"\"\"\n        if key not in self:\n            self[key] = default\n        return self[key]\n\n    def clear(self) -> None:\n        \"\"\"Remove all items from the ParameterDict.\"\"\"\n        for k in self._keys.copy():\n            del self[k]\n\n    def pop(self, key: str) -> Any:\n        r\"\"\"Remove key from the ParameterDict and return its parameter.\n\n        Args:\n            key (str): key to pop from the ParameterDict\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    def popitem(self) -> tuple[str, Any]:\n        \"\"\"Remove and return the last inserted `(key, parameter)` pair from the ParameterDict.\"\"\"\n        k, _ = self._keys.popitem()\n        # We need the key in the _keys to be able to access/del\n        self._keys[k] = None\n        val = self[k]\n        del self[k]\n        return k, val\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        r\"\"\"Return the parameter associated with key if present. Otherwise return default if provided, None if not.\n\n        Args:\n            key (str): key to get from the ParameterDict\n            default (Parameter, optional): value to return if key not present\n        \"\"\"\n        return self[key] if key in self else default\n\n    def fromkeys(\n        self, keys: Iterable[str], default: Optional[Any] = None\n    ) -> \"ParameterDict\":\n        r\"\"\"Return a new ParameterDict with the keys provided.\n\n        Args:\n            keys (iterable, string): keys to make the new ParameterDict from\n            default (Parameter, optional): value to set for all keys\n        \"\"\"\n        return ParameterDict((k, default) for k in keys)\n\n    def keys(self) -> Iterable[str]:\n        r\"\"\"Return an iterable of the ParameterDict keys.\"\"\"\n        return self._keys.keys()\n\n    def items(self) -> Iterable[tuple[str, Any]]:\n        r\"\"\"Return an iterable of the ParameterDict key/value pairs.\"\"\"\n        return ((k, self[k]) for k in self._keys)\n\n    def values(self) -> Iterable[Any]:\n        r\"\"\"Return an iterable of the ParameterDict values.\"\"\"\n        return (self[k] for k in self._keys)\n\n    def update(self, parameters: Union[Mapping[str, Any], \"ParameterDict\"]) -> None:\n        r\"\"\"Update the :class:`~torch.nn.ParameterDict` with key-value pairs from ``parameters``, overwriting existing keys.\n\n        .. note::\n            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or\n            an iterable of key-value pairs, the order of new elements in it is preserved.\n\n        Args:\n            parameters (iterable): a mapping (dictionary) from string to\n                :class:`~torch.nn.Parameter`, or an iterable of\n                key-value pairs of type (string, :class:`~torch.nn.Parameter`)\n        \"\"\"\n        if not isinstance(parameters, container_abcs.Iterable):\n            raise TypeError(\n                \"ParametersDict.update should be called with an \"\n                \"iterable of key/value pairs, but got \" + type(parameters).__name__\n            )\n\n        if isinstance(parameters, (OrderedDict, ParameterDict)):\n            for key, parameter in parameters.items():\n                self[key] = parameter\n        elif isinstance(parameters, container_abcs.Mapping):\n            for key, parameter in sorted(parameters.items()):\n                self[key] = parameter\n        else:\n            for j, p in enumerate(parameters):\n                if not isinstance(p, container_abcs.Iterable):\n                    raise TypeError(\n                        \"ParameterDict update sequence element \"\n                        \"#\" + str(j) + \" should be Iterable; is\" + type(p).__name__\n                    )\n                if not len(p) == 2:\n                    raise ValueError(\n                        \"ParameterDict update sequence element \"\n                        \"#\" + str(j) + \" has length \" + str(len(p)) + \"; 2 is required\"\n                    )\n                # parameters as length-2 list too cumbersome to type, see ModuleDict.update comment\n                self[p[0]] = p[1]  # type: ignore[assignment]\n\n    def extra_repr(self) -> str:\n        child_lines = []\n        for k, p in self.items():\n            if isinstance(p, torch.Tensor):\n                size_str = \"x\".join(str(size) for size in p.size())\n                if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n                    device_str = f\" ({p.device})\"\n                else:\n                    device_str = \"\"\n                parastr = \"{} containing: [{} of size {}{}]\".format(\n                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n                    torch.typename(p),\n                    size_str,\n                    device_str,\n                )\n                child_lines.append(\"  (\" + str(k) + \"): \" + parastr)\n            else:\n                child_lines.append(\n                    \"  (\" + str(k) + \"): Object of type: \" + type(p).__name__\n                )\n        tmpstr = \"\\n\".join(child_lines)\n        return tmpstr\n\n    def __call__(self, input):\n        raise RuntimeError(\"ParameterDict should not be called.\")\n\n    def __or__(self, other: \"ParameterDict\") -> \"ParameterDict\":\n        copy = self.copy()\n        copy.update(other)\n        return copy\n\n    def __ror__(self, other: \"ParameterDict\") -> \"ParameterDict\":\n        copy = other.copy()\n        copy.update(self)\n        return copy\n\n    def __ior__(self, other: \"ParameterDict\") -> Self:\n        self.update(other)\n        return self\n", 966], "C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py": ["# mypy: allow-untyped-defs\n\nimport functools\nimport inspect\nimport itertools\nimport warnings\nimport weakref\nfrom collections import namedtuple, OrderedDict\nfrom collections.abc import Iterator, Mapping\nfrom typing import Any, Callable, Optional, overload, TypeVar, Union\nfrom typing_extensions import Self\n\nimport torch\nfrom torch import device, dtype, Tensor\nfrom torch._prims_common import DeviceLikeType\nfrom torch.nn.parameter import Buffer, Parameter\nfrom torch.utils._python_dispatch import is_traceable_wrapper_subclass\nfrom torch.utils.hooks import BackwardHook, RemovableHandle\n\n\n__all__ = [\n    \"register_module_forward_pre_hook\",\n    \"register_module_forward_hook\",\n    \"register_module_full_backward_pre_hook\",\n    \"register_module_backward_hook\",\n    \"register_module_full_backward_hook\",\n    \"register_module_buffer_registration_hook\",\n    \"register_module_module_registration_hook\",\n    \"register_module_parameter_registration_hook\",\n    \"Module\",\n]\n\n_grad_t = Union[tuple[Tensor, ...], Tensor]\n# See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use\n# of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be\n# the type of the subclass, not the looser type of `Module`.\nT = TypeVar(\"T\", bound=\"Module\")\n\n\nclass _IncompatibleKeys(\n    namedtuple(\"IncompatibleKeys\", [\"missing_keys\", \"unexpected_keys\"]),\n):\n    __slots__ = ()\n\n    def __repr__(self):\n        if not self.missing_keys and not self.unexpected_keys:\n            return \"<All keys matched successfully>\"\n        return super().__repr__()\n\n    __str__ = __repr__\n\n\ndef _addindent(s_, numSpaces):\n    s = s_.split(\"\\n\")\n    # don't do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * \" \") + line for line in s]\n    s = \"\\n\".join(s)\n    s = first + \"\\n\" + s\n    return s\n\n\nr\"\"\"This tracks hooks common to all modules that are executed immediately before\n.registering the buffer/module/parameter\"\"\"\n_global_buffer_registration_hooks: dict[int, Callable] = OrderedDict()\n_global_module_registration_hooks: dict[int, Callable] = OrderedDict()\n_global_parameter_registration_hooks: dict[int, Callable] = OrderedDict()\n\n\nclass _WrappedHook:\n    def __init__(self, hook: Callable, module: Optional[\"Module\"] = None):\n        self.hook: Callable = hook\n        functools.update_wrapper(self, hook)\n\n        self.with_module: bool = False\n\n        if module is not None:\n            self.module: weakref.ReferenceType[Module] = weakref.ref(module)\n            self.with_module = True\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        if self.with_module:\n            module = self.module()\n            if module is None:\n                raise RuntimeError(\"You are trying to call the hook of a dead Module!\")\n            return self.hook(module, *args, **kwargs)\n        return self.hook(*args, **kwargs)\n\n    def __getstate__(self) -> dict:\n        result = {\"hook\": self.hook, \"with_module\": self.with_module}\n        if self.with_module:\n            result[\"module\"] = self.module()\n\n        return result\n\n    def __setstate__(self, state: dict):\n        self.hook = state[\"hook\"]\n        self.with_module = state[\"with_module\"]\n\n        if self.with_module:\n            if state[\"module\"] is None:\n                raise RuntimeError(\n                    \"You are trying to revive the hook of a dead Module!\"\n                )\n            self.module = weakref.ref(state[\"module\"])\n\n\nr\"\"\"This tracks hooks common to all modules that are executed before/after\ncalling forward and backward. This is global state used for debugging/profiling\npurposes\"\"\"\n_global_backward_pre_hooks: dict[int, Callable] = OrderedDict()\n_global_backward_hooks: dict[int, Callable] = OrderedDict()\n_global_is_full_backward_hook: Optional[bool] = None\n_global_forward_pre_hooks: dict[int, Callable] = OrderedDict()\n_global_forward_hooks: dict[int, Callable] = OrderedDict()\n_global_forward_hooks_always_called: dict[int, bool] = OrderedDict()\n_global_forward_hooks_with_kwargs: dict[int, bool] = OrderedDict()\n\n_EXTRA_STATE_KEY_SUFFIX = \"_extra_state\"\n\n\ndef register_module_buffer_registration_hook(\n    hook: Callable[..., None],\n) -> RemovableHandle:\n    r\"\"\"Register a buffer registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_buffer` is invoked.\n    It should have the following signature::\n\n        hook(module, name, buffer) -> None or new buffer\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(_global_buffer_registration_hooks)\n    _global_buffer_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_module_registration_hook(\n    hook: Callable[..., None],\n) -> RemovableHandle:\n    r\"\"\"Register a module registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_module` is invoked.\n    It should have the following signature::\n\n        hook(module, name, submodule) -> None or new submodule\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(_global_module_registration_hooks)\n    _global_module_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_parameter_registration_hook(\n    hook: Callable[..., None],\n) -> RemovableHandle:\n    r\"\"\"Register a parameter registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_parameter` is invoked.\n    It should have the following signature::\n\n        hook(module, name, param) -> None or new parameter\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(_global_parameter_registration_hooks)\n    _global_parameter_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle:\n    r\"\"\"Register a forward pre-hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time before :func:`forward` is invoked.\n    It should have the following signature::\n\n        hook(module, input) -> None or modified input\n\n    The input contains only the positional arguments given to the module.\n    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n    The hook can modify the input. User can either return a tuple or a\n    single modified value in the hook. We will wrap the value into a tuple\n    if a single value is returned(unless that value is already a tuple).\n\n    This hook has precedence over the specific module hooks registered with\n    ``register_forward_pre_hook``.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(_global_forward_pre_hooks)\n    _global_forward_pre_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_forward_hook(\n    hook: Callable[..., None],\n    *,\n    with_kwargs: bool = False,\n    always_call: bool = False,\n) -> RemovableHandle:\n    r\"\"\"Register a global forward hook for all the modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time after :func:`forward` has computed an output.\n    It should have the following signature::\n\n        hook(module, input, output) -> None or modified output\n\n    The input contains only the positional arguments given to the module.\n    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n    You can optionally modify the output of the module by returning a new value\n    that will replace the output from the :func:`forward` function.\n\n    Parameters:\n        hook (Callable): The user defined hook to be registered.\n        always_call (bool): If ``True`` the ``hook`` will be run regardless of\n            whether an exception is raised while calling the Module.\n            Default: ``False``\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    This hook will be executed before specific module hooks registered with\n    ``register_forward_hook``.\n    \"\"\"\n    handle = RemovableHandle(\n        _global_forward_hooks, extra_dict=_global_forward_hooks_always_called\n    )\n    _global_forward_hooks[handle.id] = hook\n    if with_kwargs:\n        _global_forward_hooks_with_kwargs[handle.id] = True\n    if always_call:\n        _global_forward_hooks_always_called[handle.id] = True\n    return handle\n\n\ndef register_module_backward_hook(\n    hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]],\n) -> RemovableHandle:\n    r\"\"\"Register a backward hook common to all the modules.\n\n    This function is deprecated in favor of\n    :func:`torch.nn.modules.module.register_module_full_backward_hook`\n    and the behavior of this function will change in future versions.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    global _global_is_full_backward_hook\n    if _global_is_full_backward_hook is True:\n        raise RuntimeError(\n            \"Cannot use both regular backward hooks and full backward hooks as a \"\n            \"global Module hook. Please use only one of them.\"\n        )\n\n    _global_is_full_backward_hook = False\n\n    handle = RemovableHandle(_global_backward_hooks)\n    _global_backward_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_full_backward_pre_hook(\n    hook: Callable[[\"Module\", _grad_t], Union[None, _grad_t]],\n) -> RemovableHandle:\n    r\"\"\"Register a backward pre-hook common to all the modules.\n\n    .. warning ::\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    Hooks registered using this function behave in the same way as those\n    registered by :meth:`torch.nn.Module.register_full_backward_pre_hook`.\n    Refer to its documentation for more details.\n\n    Hooks registered using this function will be called before hooks registered\n    using :meth:`torch.nn.Module.register_full_backward_pre_hook`.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    handle = RemovableHandle(_global_backward_pre_hooks)\n    _global_backward_pre_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_full_backward_hook(\n    hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]],\n) -> RemovableHandle:\n    r\"\"\"Register a backward hook common to all the modules.\n\n    .. warning ::\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    Hooks registered using this function behave in the same way as those\n    registered by :meth:`torch.nn.Module.register_full_backward_hook`.\n    Refer to its documentation for more details.\n\n    Hooks registered using this function will be called before hooks registered\n    using :meth:`torch.nn.Module.register_full_backward_hook`.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    global _global_is_full_backward_hook\n    if _global_is_full_backward_hook is False:\n        raise RuntimeError(\n            \"Cannot use both regular backward hooks and full backward hooks as a \"\n            \"global Module hook. Please use only one of them.\"\n        )\n\n    _global_is_full_backward_hook = True\n\n    handle = RemovableHandle(_global_backward_hooks)\n    _global_backward_hooks[handle.id] = hook\n    return handle\n\n\n# Trick mypy into not applying contravariance rules to inputs by defining\n# forward as a value, rather than a function.  See also\n# https://github.com/python/mypy/issues/8795\ndef _forward_unimplemented(self, *input: Any) -> None:\n    r\"\"\"Define the computation performed at every call.\n\n    Should be overridden by all subclasses.\n\n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        registered hooks while the latter silently ignores them.\n    \"\"\"\n    raise NotImplementedError(\n        f'Module [{type(self).__name__}] is missing the required \"forward\" function'\n    )\n\n\nclass Module:\n    r\"\"\"Base class for all neural network modules.\n\n    Your models should also subclass this class.\n\n    Modules can also contain other Modules, allowing them to be nested in\n    a tree structure. You can assign the submodules as regular attributes::\n\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        class Model(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.conv1 = nn.Conv2d(1, 20, 5)\n                self.conv2 = nn.Conv2d(20, 20, 5)\n\n            def forward(self, x):\n                x = F.relu(self.conv1(x))\n                return F.relu(self.conv2(x))\n\n    Submodules assigned in this way will be registered, and will also have their\n    parameters converted when you call :meth:`to`, etc.\n\n    .. note::\n        As per the example above, an ``__init__()`` call to the parent class\n        must be made before assignment on the child.\n\n    :ivar training: Boolean represents whether this module is in training or\n                    evaluation mode.\n    :vartype training: bool\n    \"\"\"\n\n    dump_patches: bool = False\n\n    _version: int = 1\n    r\"\"\"This allows better BC support for :meth:`load_state_dict`. In\n    :meth:`state_dict`, the version number will be saved as in the attribute\n    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n    dictionary with keys that follow the naming convention of state dict. See\n    ``_load_from_state_dict`` on how to use this information in loading.\n\n    If new parameters/buffers are added/removed from a module, this number shall\n    be bumped, and the module's `_load_from_state_dict` method can compare the\n    version number and do appropriate changes if the state dict is from before\n    the change.\"\"\"\n\n    training: bool\n    _parameters: dict[str, Optional[Parameter]]\n    _buffers: dict[str, Optional[Tensor]]\n    _non_persistent_buffers_set: set[str]\n    _backward_pre_hooks: dict[int, Callable]\n    _backward_hooks: dict[int, Callable]\n    _is_full_backward_hook: Optional[bool]\n    _forward_hooks: dict[int, Callable]\n    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n    # As JIT does not support set[int], this dict is used as a set, where all\n    # hooks represented in this dict accept kwargs.\n    _forward_hooks_with_kwargs: dict[int, bool]\n    # forward hooks that should always be called even if an exception is raised\n    _forward_hooks_always_called: dict[int, bool]\n    _forward_pre_hooks: dict[int, Callable]\n    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n    # As JIT does not support set[int], this dict is used as a set, where all\n    # hooks represented in this dict accept kwargs.\n    _forward_pre_hooks_with_kwargs: dict[int, bool]\n    _state_dict_hooks: dict[int, Callable]\n    _load_state_dict_pre_hooks: dict[int, Callable]\n    _state_dict_pre_hooks: dict[int, Callable]\n    _load_state_dict_post_hooks: dict[int, Callable]\n    _modules: dict[str, Optional[\"Module\"]]\n    call_super_init: bool = False\n    _compiled_call_impl: Optional[Callable] = None\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"\"\"\n        torch._C._log_api_usage_once(\"python.nn_module\")\n\n        # Backward compatibility: no args used to be allowed when call_super_init=False\n        if self.call_super_init is False and bool(kwargs):\n            raise TypeError(\n                f\"{type(self).__name__}.__init__() got an unexpected keyword argument '{next(iter(kwargs))}'\"\n                \"\"\n            )\n\n        if self.call_super_init is False and bool(args):\n            raise TypeError(\n                f\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\n                \" given\"\n            )\n\n        \"\"\"\n        Calls super().__setattr__('a', a) instead of the typical self.a = a\n        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n        handling for parameters, submodules, and buffers but simply calls into\n        super().__setattr__ for all other attributes.\n        \"\"\"\n        super().__setattr__(\"training\", True)\n        super().__setattr__(\"_parameters\", {})\n        super().__setattr__(\"_buffers\", {})\n        super().__setattr__(\"_non_persistent_buffers_set\", set())\n        super().__setattr__(\"_backward_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_backward_hooks\", OrderedDict())\n        super().__setattr__(\"_is_full_backward_hook\", None)\n        super().__setattr__(\"_forward_hooks\", OrderedDict())\n        super().__setattr__(\"_forward_hooks_with_kwargs\", OrderedDict())\n        super().__setattr__(\"_forward_hooks_always_called\", OrderedDict())\n        super().__setattr__(\"_forward_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_forward_pre_hooks_with_kwargs\", OrderedDict())\n        super().__setattr__(\"_state_dict_hooks\", OrderedDict())\n        super().__setattr__(\"_state_dict_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_load_state_dict_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_load_state_dict_post_hooks\", OrderedDict())\n        super().__setattr__(\"_modules\", {})\n\n        if self.call_super_init:\n            super().__init__(*args, **kwargs)\n\n    forward: Callable[..., Any] = _forward_unimplemented\n\n    def register_buffer(\n        self, name: str, tensor: Optional[Tensor], persistent: bool = True\n    ) -> None:\n        r\"\"\"Add a buffer to the module.\n\n        This is typically used to register a buffer that should not to be\n        considered a model parameter. For example, BatchNorm's ``running_mean``\n        is not a parameter, but is part of the module's state. Buffers, by\n        default, are persistent and will be saved alongside parameters. This\n        behavior can be changed by setting :attr:`persistent` to ``False``. The\n        only difference between a persistent buffer and a non-persistent buffer\n        is that the latter will not be a part of this module's\n        :attr:`state_dict`.\n\n        Buffers can be accessed as attributes using given names.\n\n        Args:\n            name (str): name of the buffer. The buffer can be accessed\n                from this module using the given name\n            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n                the buffer is **not** included in the module's :attr:`state_dict`.\n            persistent (bool): whether the buffer is part of this module's\n                :attr:`state_dict`.\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n\n        \"\"\"\n        if persistent is False and isinstance(self, torch.jit.ScriptModule):\n            raise RuntimeError(\"ScriptModule does not support non-persistent buffers\")\n\n        if \"_buffers\" not in self.__dict__:\n            raise AttributeError(\"cannot assign buffer before Module.__init__() call\")\n        elif not isinstance(name, str):\n            raise TypeError(\n                f\"buffer name should be a string. Got {torch.typename(name)}\"\n            )\n        elif \".\" in name:\n            raise KeyError('buffer name can\\'t contain \".\"')\n        elif name == \"\":\n            raise KeyError('buffer name can\\'t be empty string \"\"')\n        elif hasattr(self, name) and name not in self._buffers:\n            raise KeyError(f\"attribute '{name}' already exists\")\n        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n            raise TypeError(\n                f\"cannot assign '{torch.typename(tensor)}' object to buffer '{name}' \"\n                \"(torch Tensor or None required)\"\n            )\n        else:\n            for hook in _global_buffer_registration_hooks.values():\n                output = hook(self, name, tensor)\n                if output is not None:\n                    tensor = output\n            self._buffers[name] = tensor\n            if persistent:\n                self._non_persistent_buffers_set.discard(name)\n            else:\n                self._non_persistent_buffers_set.add(name)\n\n    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:\n        r\"\"\"Add a parameter to the module.\n\n        The parameter can be accessed as an attribute using given name.\n\n        Args:\n            name (str): name of the parameter. The parameter can be accessed\n                from this module using the given name\n            param (Parameter or None): parameter to be added to the module. If\n                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n                are ignored. If ``None``, the parameter is **not** included in the\n                module's :attr:`state_dict`.\n        \"\"\"\n        if \"_parameters\" not in self.__dict__:\n            raise AttributeError(\n                \"cannot assign parameter before Module.__init__() call\"\n            )\n\n        elif not isinstance(name, str):\n            raise TypeError(\n                f\"parameter name should be a string. Got {torch.typename(name)}\"\n            )\n        elif \".\" in name:\n            raise KeyError('parameter name can\\'t contain \".\"')\n        elif name == \"\":\n            raise KeyError('parameter name can\\'t be empty string \"\"')\n        elif hasattr(self, name) and name not in self._parameters:\n            raise KeyError(f\"attribute '{name}' already exists\")\n\n        if param is None:\n            self._parameters[name] = None\n        elif not isinstance(param, Parameter):\n            raise TypeError(\n                f\"cannot assign '{torch.typename(param)}' object to parameter '{name}' \"\n                \"(torch.nn.Parameter or None required)\"\n            )\n        elif param.grad_fn:\n            raise ValueError(\n                f\"Cannot assign non-leaf Tensor to parameter '{name}'. Model \"\n                f\"parameters must be created explicitly. To express '{name}' \"\n                \"as a function of another Tensor, compute the value in \"\n                \"the forward() method.\"\n            )\n        else:\n            for hook in _global_parameter_registration_hooks.values():\n                output = hook(self, name, param)\n                if output is not None:\n                    param = output\n            self._parameters[name] = param\n\n    def add_module(self, name: str, module: Optional[\"Module\"]) -> None:\n        r\"\"\"Add a child module to the current module.\n\n        The module can be accessed as an attribute using the given name.\n\n        Args:\n            name (str): name of the child module. The child module can be\n                accessed from this module using the given name\n            module (Module): child module to be added to the module.\n        \"\"\"\n        if not isinstance(module, Module) and module is not None:\n            raise TypeError(f\"{torch.typename(module)} is not a Module subclass\")\n        elif not isinstance(name, str):\n            raise TypeError(\n                f\"module name should be a string. Got {torch.typename(name)}\"\n            )\n        elif hasattr(self, name) and name not in self._modules:\n            raise KeyError(f\"attribute '{name}' already exists\")\n        elif \".\" in name:\n            raise KeyError(f'module name can\\'t contain \".\", got: {name}')\n        elif name == \"\":\n            raise KeyError('module name can\\'t be empty string \"\"')\n        for hook in _global_module_registration_hooks.values():\n            output = hook(self, name, module)\n            if output is not None:\n                module = output\n        self._modules[name] = module\n\n    def register_module(self, name: str, module: Optional[\"Module\"]) -> None:\n        r\"\"\"Alias for :func:`add_module`.\"\"\"\n        self.add_module(name, module)\n\n    def get_submodule(self, target: str) -> \"Module\":\n        \"\"\"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\n        For example, let's say you have an ``nn.Module`` ``A`` that\n        looks like this:\n\n        .. code-block:: text\n\n            A(\n                (net_b): Module(\n                    (net_c): Module(\n                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                    )\n                    (linear): Linear(in_features=100, out_features=200, bias=True)\n                )\n            )\n\n        (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\n        submodule ``net_b``, which itself has two submodules ``net_c``\n        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n        To check whether or not we have the ``linear`` submodule, we\n        would call ``get_submodule(\"net_b.linear\")``. To check whether\n        we have the ``conv`` submodule, we would call\n        ``get_submodule(\"net_b.net_c.conv\")``.\n\n        The runtime of ``get_submodule`` is bounded by the degree\n        of module nesting in ``target``. A query against\n        ``named_modules`` achieves the same result, but it is O(N) in\n        the number of transitive modules. So, for a simple check to see\n        if some submodule exists, ``get_submodule`` should always be\n        used.\n\n        Args:\n            target: The fully-qualified string name of the submodule\n                to look for. (See above example for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.nn.Module: The submodule referenced by ``target``\n\n        Raises:\n            AttributeError: If at any point along the path resulting from\n                the target string the (sub)path resolves to a non-existent\n                attribute name or an object that is not an instance of ``nn.Module``.\n        \"\"\"\n        if target == \"\":\n            return self\n\n        atoms: list[str] = target.split(\".\")\n        mod: torch.nn.Module = self\n\n        for item in atoms:\n            if not hasattr(mod, item):\n                raise AttributeError(\n                    mod._get_name() + \" has no attribute `\" + item + \"`\"\n                )\n\n            mod = getattr(mod, item)\n\n            if not isinstance(mod, torch.nn.Module):\n                raise AttributeError(\"`\" + item + \"` is not an nn.Module\")\n\n        return mod\n\n    def set_submodule(\n        self, target: str, module: \"Module\", strict: bool = False\n    ) -> None:\n        \"\"\"\n        Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n        .. note::\n            If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\n            or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\n            the method will only attempt to replace an existing submodule and throw an error if\n            the submodule does not exist.\n\n        For example, let's say you have an ``nn.Module`` ``A`` that\n        looks like this:\n\n        .. code-block:: text\n\n            A(\n                (net_b): Module(\n                    (net_c): Module(\n                        (conv): Conv2d(3, 3, 3)\n                    )\n                    (linear): Linear(3, 3)\n                )\n            )\n\n        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n        submodule ``net_b``, which itself has two submodules ``net_c``\n        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n        To override the ``Conv2d`` with a new submodule ``Linear``, you\n        could call ``set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))``\n        where ``strict`` could be ``True`` or ``False``\n\n        To add a new submodule ``Conv2d`` to the existing ``net_b`` module,\n        you would call ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))``.\n\n        In the above if you set ``strict=True`` and call\n        ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)``, an AttributeError\n        will be raised because ``net_b`` does not have a submodule named ``conv``.\n\n        Args:\n            target: The fully-qualified string name of the submodule\n                to look for. (See above example for how to specify a\n                fully-qualified string.)\n            module: The module to set the submodule to.\n            strict: If ``False``, the method will replace an existing submodule\n                or create a new submodule if the parent module exists. If ``True``,\n                the method will only attempt to replace an existing submodule and throw an error\n                if the submodule doesn't already exist.\n\n        Raises:\n            ValueError: If the ``target`` string is empty or if ``module`` is not an instance of ``nn.Module``.\n            AttributeError: If at any point along the path resulting from\n                the ``target`` string the (sub)path resolves to a non-existent\n                attribute name or an object that is not an instance of ``nn.Module``.\n        \"\"\"\n        if target == \"\":\n            raise ValueError(\"Cannot set the submodule without a target name!\")\n\n        atoms: list[str] = target.split(\".\")\n        if not isinstance(module, torch.nn.Module):\n            raise ValueError(\n                \"`\" + \"module\" + f\"` is not an nn.Module, found {type(module)}\"\n            )\n        if len(atoms) == 1:\n            parent: torch.nn.Module = self\n        else:\n            parent_key = \".\".join(atoms[:-1])\n            parent = self.get_submodule(parent_key)\n\n        if strict and not hasattr(parent, atoms[-1]):\n            raise AttributeError(\n                parent._get_name() + \" has no attribute `\" + atoms[-1] + \"`\"\n            )\n        if hasattr(parent, atoms[-1]):\n            mod = getattr(parent, atoms[-1])\n            if not isinstance(mod, torch.nn.Module):\n                raise AttributeError(\"`\" + atoms[-1] + \"` is not an nn.Module\")\n        setattr(parent, atoms[-1], module)\n\n    def get_parameter(self, target: str) -> \"Parameter\":\n        \"\"\"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the Parameter\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.nn.Parameter: The Parameter referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Parameter``\n        \"\"\"\n        module_path, _, param_name = target.rpartition(\".\")\n\n        mod: torch.nn.Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, param_name):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + param_name + \"`\"\n            )\n\n        param: torch.nn.Parameter = getattr(mod, param_name)\n\n        if not isinstance(param, torch.nn.Parameter):\n            raise AttributeError(\"`\" + param_name + \"` is not an nn.Parameter\")\n\n        return param\n\n    def get_buffer(self, target: str) -> \"Tensor\":\n        \"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the buffer\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.Tensor: The buffer referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not a\n                buffer\n        \"\"\"\n        module_path, _, buffer_name = target.rpartition(\".\")\n\n        mod: torch.nn.Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, buffer_name):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + buffer_name + \"`\"\n            )\n\n        buffer: torch.Tensor = getattr(mod, buffer_name)\n\n        if buffer_name not in mod._buffers:\n            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n\n        return buffer\n\n    def get_extra_state(self) -> Any:\n        \"\"\"Return any extra state to include in the module's state_dict.\n\n        Implement this and a corresponding :func:`set_extra_state` for your module\n        if you need to store extra state. This function is called when building the\n        module's `state_dict()`.\n\n        Note that extra state should be picklable to ensure working serialization\n        of the state_dict. We only provide backwards compatibility guarantees\n        for serializing Tensors; other objects may break backwards compatibility if\n        their serialized pickled form changes.\n\n        Returns:\n            object: Any extra state to store in the module's state_dict\n        \"\"\"\n        raise RuntimeError(\n            \"Reached a code path in Module.get_extra_state() that should never be called. \"\n            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n            \"to report this bug.\"\n        )\n\n    def set_extra_state(self, state: Any) -> None:\n        \"\"\"Set extra state contained in the loaded `state_dict`.\n\n        This function is called from :func:`load_state_dict` to handle any extra state\n        found within the `state_dict`. Implement this function and a corresponding\n        :func:`get_extra_state` for your module if you need to store extra state within its\n        `state_dict`.\n\n        Args:\n            state (dict): Extra state from the `state_dict`\n        \"\"\"\n        raise RuntimeError(\n            \"Reached a code path in Module.set_extra_state() that should never be called. \"\n            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n            \"to report this bug.\"\n        )\n\n    def _apply(self, fn, recurse=True):\n        if recurse:\n            for module in self.children():\n                module._apply(fn)\n\n        def compute_should_use_set_data(tensor, tensor_applied):\n            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n                # If the new tensor has compatible tensor type as the existing tensor,\n                # the current behavior is to change the tensor in-place using `.data =`,\n                # and the future behavior is to overwrite the existing tensor. However,\n                # changing the current behavior is a BC-breaking change, and we want it\n                # to happen in future releases. So for now we introduce the\n                # `torch.__future__.get_overwrite_module_params_on_conversion()`\n                # global flag to let the user control whether they want the future\n                # behavior of overwriting the existing tensor or not.\n                return not torch.__future__.get_overwrite_module_params_on_conversion()\n            else:\n                return False\n\n        should_use_swap_tensors = (\n            torch.__future__.get_swap_module_params_on_conversion()\n        )\n\n        for key, param in self._parameters.items():\n            if param is None:\n                continue\n            # Tensors stored in modules are graph leaves, and we don't want to\n            # track autograd history of `param_applied`, so we have to use\n            # `with torch.no_grad():`\n            with torch.no_grad():\n                param_applied = fn(param)\n            p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\n            # subclasses may have multiple child tensors so we need to use swap_tensors\n            p_should_use_swap_tensors = (\n                should_use_swap_tensors or is_traceable_wrapper_subclass(param_applied)\n            )\n\n            param_grad = param.grad\n            if p_should_use_swap_tensors:\n                try:\n                    if param_grad is not None:\n                        # Accessing param.grad makes its at::Tensor's use_count 2, which will prevent swapping.\n                        # Decrement use count of the gradient by setting to None\n                        param.grad = None\n                    param_applied = torch.nn.Parameter(\n                        param_applied, requires_grad=param.requires_grad\n                    )\n                    torch.utils.swap_tensors(param, param_applied)\n                except Exception as e:\n                    if param_grad is not None:\n                        param.grad = param_grad\n                    raise RuntimeError(\n                        f\"_apply(): Couldn't swap {self._get_name()}.{key}\"\n                    ) from e\n                out_param = param\n            elif p_should_use_set_data:\n                param.data = param_applied\n                out_param = param\n            else:\n                assert isinstance(param, Parameter)\n                assert param.is_leaf\n                out_param = Parameter(param_applied, param.requires_grad)\n                self._parameters[key] = out_param\n\n            if param_grad is not None:\n                with torch.no_grad():\n                    grad_applied = fn(param_grad)\n                g_should_use_set_data = compute_should_use_set_data(\n                    param_grad, grad_applied\n                )\n                if p_should_use_swap_tensors:\n                    grad_applied.requires_grad_(param_grad.requires_grad)\n                    try:\n                        torch.utils.swap_tensors(param_grad, grad_applied)\n                    except Exception as e:\n                        raise RuntimeError(\n                            f\"_apply(): Couldn't swap {self._get_name()}.{key}.grad\"\n                        ) from e\n                    out_param.grad = param_grad\n                elif g_should_use_set_data:\n                    assert out_param.grad is not None\n                    out_param.grad.data = grad_applied\n                else:\n                    assert param_grad.is_leaf\n                    out_param.grad = grad_applied.requires_grad_(\n                        param_grad.requires_grad\n                    )\n\n        for key, buf in self._buffers.items():\n            if buf is not None:\n                self._buffers[key] = fn(buf)\n\n        return self\n\n    def apply(self: T, fn: Callable[[\"Module\"], None]) -> T:\n        r\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\n        Typical use includes initializing the parameters of a model\n        (see also :ref:`nn-init-doc`).\n\n        Args:\n            fn (:class:`Module` -> None): function to be applied to each submodule\n\n        Returns:\n            Module: self\n\n        Example::\n\n            >>> @torch.no_grad()\n            >>> def init_weights(m):\n            >>>     print(m)\n            >>>     if type(m) == nn.Linear:\n            >>>         m.weight.fill_(1.0)\n            >>>         print(m.weight)\n            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n            >>> net.apply(init_weights)\n            Linear(in_features=2, out_features=2, bias=True)\n            Parameter containing:\n            tensor([[1., 1.],\n                    [1., 1.]], requires_grad=True)\n            Linear(in_features=2, out_features=2, bias=True)\n            Parameter containing:\n            tensor([[1., 1.],\n                    [1., 1.]], requires_grad=True)\n            Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            )\n\n        \"\"\"\n        for module in self.children():\n            module.apply(fn)\n        fn(self)\n        return self\n\n    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the GPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing the optimizer if the module will\n        live on GPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.cuda(device))\n\n    def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the IPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing the optimizer if the module will\n        live on IPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Arguments:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.ipu(device))\n\n    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the XPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing optimizer if the module will\n        live on XPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Arguments:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.xpu(device))\n\n    def mtia(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the MTIA.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing the optimizer if the module will\n        live on MTIA while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Arguments:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.mtia(device))\n\n    def cpu(self: T) -> T:\n        r\"\"\"Move all model parameters and buffers to the CPU.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.cpu())\n\n    def type(self: T, dst_type: Union[dtype, str]) -> T:\n        r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            dst_type (type or string): the desired type\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.type(dst_type))\n\n    def float(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n\n    def double(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n\n    def half(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n\n    def bfloat16(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)\n\n    def to_empty(\n        self: T, *, device: Optional[DeviceLikeType], recurse: bool = True\n    ) -> T:\n        r\"\"\"Move the parameters and buffers to the specified device without copying storage.\n\n        Args:\n            device (:class:`torch.device`): The desired device of the parameters\n                and buffers in this module.\n            recurse (bool): Whether parameters and buffers of submodules should\n                be recursively moved to the specified device.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(\n            lambda t: torch.empty_like(t, device=device), recurse=recurse\n        )\n\n    @overload\n    def to(\n        self,\n        device: Optional[DeviceLikeType] = ...,\n        dtype: Optional[dtype] = ...,\n        non_blocking: bool = ...,\n    ) -> Self:\n        ...\n\n    @overload\n    def to(self, dtype: dtype, non_blocking: bool = ...) -> Self:\n        ...\n\n    @overload\n    def to(self, tensor: Tensor, non_blocking: bool = ...) -> Self:\n        ...\n\n    def to(self, *args, **kwargs):\n        r\"\"\"Move and/or cast the parameters and buffers.\n\n        This can be called as\n\n        .. function:: to(device=None, dtype=None, non_blocking=False)\n           :noindex:\n\n        .. function:: to(dtype, non_blocking=False)\n           :noindex:\n\n        .. function:: to(tensor, non_blocking=False)\n           :noindex:\n\n        .. function:: to(memory_format=torch.channels_last)\n           :noindex:\n\n        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n        floating point or complex :attr:`dtype`\\ s. In addition, this method will\n        only cast the floating point or complex parameters and buffers to :attr:`dtype`\n        (if given). The integral parameters and buffers will be moved\n        :attr:`device`, if that is given, but with dtypes unchanged. When\n        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n        with respect to the host if possible, e.g., moving CPU Tensors with\n        pinned memory to CUDA devices.\n\n        See below for examples.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            device (:class:`torch.device`): the desired device of the parameters\n                and buffers in this module\n            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n                the parameters and buffers in this module\n            tensor (torch.Tensor): Tensor whose dtype and device are the desired\n                dtype and device for all parameters and buffers in this module\n            memory_format (:class:`torch.memory_format`): the desired memory\n                format for 4D parameters and buffers in this module (keyword\n                only argument)\n\n        Returns:\n            Module: self\n\n        Examples::\n\n            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n            >>> linear = nn.Linear(2, 2)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1913, -0.3420],\n                    [-0.5113, -0.2325]])\n            >>> linear.to(torch.double)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1913, -0.3420],\n                    [-0.5113, -0.2325]], dtype=torch.float64)\n            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n            >>> gpu1 = torch.device(\"cuda:1\")\n            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1914, -0.3420],\n                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n            >>> cpu = torch.device(\"cpu\")\n            >>> linear.to(cpu)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1914, -0.3420],\n                    [-0.5112, -0.2324]], dtype=torch.float16)\n\n            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.3741+0.j,  0.2382+0.j],\n                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n            tensor([[0.6122+0.j, 0.1150+0.j],\n                    [0.6122+0.j, 0.1150+0.j],\n                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n\n        \"\"\"\n        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(\n            *args, **kwargs\n        )\n\n        if dtype is not None:\n            if not (dtype.is_floating_point or dtype.is_complex):\n                raise TypeError(\n                    \"nn.Module.to only accepts floating point or complex \"\n                    f\"dtypes, but got desired dtype={dtype}\"\n                )\n            if dtype.is_complex:\n                warnings.warn(\n                    \"Complex modules are a new feature under active development whose design may change, \"\n                    \"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\n                    \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n                    \"if a complex module does not work as expected.\"\n                )\n\n        def convert(t):\n            try:\n                if convert_to_format is not None and t.dim() in (4, 5):\n                    return t.to(\n                        device,\n                        dtype if t.is_floating_point() or t.is_complex() else None,\n                        non_blocking,\n                        memory_format=convert_to_format,\n                    )\n                return t.to(\n                    device,\n                    dtype if t.is_floating_point() or t.is_complex() else None,\n                    non_blocking,\n                )\n            except NotImplementedError as e:\n                if str(e) == \"Cannot copy out of meta tensor; no data!\":\n                    raise NotImplementedError(\n                        f\"{e} Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \"\n                        f\"when moving module from meta to a different device.\"\n                    ) from None\n                else:\n                    raise\n\n        return self._apply(convert)\n\n    def register_full_backward_pre_hook(\n        self,\n        hook: Callable[[\"Module\", _grad_t], Union[None, _grad_t]],\n        prepend: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward pre-hook on the module.\n\n        The hook will be called every time the gradients for the module are computed.\n        The hook should have the following signature::\n\n            hook(module, grad_output) -> tuple[Tensor] or None\n\n        The :attr:`grad_output` is a tuple. The hook should\n        not modify its arguments, but it can optionally return a new gradient with\n        respect to the output that will be used in place of :attr:`grad_output` in\n        subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n        all non-Tensor arguments.\n\n        For technical reasons, when this hook is applied to a Module, its forward function will\n        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n        of each Tensor returned by the Module's forward function.\n\n        .. warning ::\n            Modifying inputs inplace is not allowed when using backward hooks and\n            will raise an error.\n\n        Args:\n            hook (Callable): The user-defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``backward_pre`` hooks on this\n                :class:`torch.nn.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``backward_pre`` hooks\n                on this :class:`torch.nn.Module`. Note that global\n                ``backward_pre`` hooks registered with\n                :func:`register_module_full_backward_pre_hook` will fire before\n                all hooks registered by this method.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        handle = RemovableHandle(self._backward_pre_hooks)\n        self._backward_pre_hooks[handle.id] = hook\n        if prepend:\n            self._backward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def register_backward_hook(\n        self, hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]]\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward hook on the module.\n\n        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n        the behavior of this function will change in future versions.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        if self._is_full_backward_hook is True:\n            raise RuntimeError(\n                \"Cannot use both regular backward hooks and full backward hooks on a \"\n                \"single Module. Please use only one of them.\"\n            )\n\n        self._is_full_backward_hook = False\n\n        handle = RemovableHandle(self._backward_hooks)\n        self._backward_hooks[handle.id] = hook\n        return handle\n\n    def register_full_backward_hook(\n        self,\n        hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]],\n        prepend: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward hook on the module.\n\n        The hook will be called every time the gradients with respect to a module\n        are computed, i.e. the hook will execute if and only if the gradients with\n        respect to module outputs are computed. The hook should have the following\n        signature::\n\n            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\n        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n        with respect to the inputs and outputs respectively. The hook should\n        not modify its arguments, but it can optionally return a new gradient with\n        respect to the input that will be used in place of :attr:`grad_input` in\n        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n        as positional arguments and all kwarg arguments are ignored. Entries\n        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n        arguments.\n\n        For technical reasons, when this hook is applied to a Module, its forward function will\n        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n        of each Tensor returned by the Module's forward function.\n\n        .. warning ::\n            Modifying inputs or outputs inplace is not allowed when using backward hooks and\n            will raise an error.\n\n        Args:\n            hook (Callable): The user-defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``backward`` hooks on this\n                :class:`torch.nn.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``backward`` hooks on\n                this :class:`torch.nn.Module`. Note that global\n                ``backward`` hooks registered with\n                :func:`register_module_full_backward_hook` will fire before\n                all hooks registered by this method.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        if self._is_full_backward_hook is False:\n            raise RuntimeError(\n                \"Cannot use both regular backward hooks and full backward hooks on a \"\n                \"single Module. Please use only one of them.\"\n            )\n\n        self._is_full_backward_hook = True\n\n        handle = RemovableHandle(self._backward_hooks)\n        self._backward_hooks[handle.id] = hook\n        if prepend:\n            self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def _get_backward_hooks(self):\n        r\"\"\"Return the backward hooks for use in the call function.\n\n        It returns two lists, one with the full backward hooks and one with the non-full\n        backward hooks.\n        \"\"\"\n        full_backward_hooks: list[Callable] = []\n        if _global_is_full_backward_hook is True:\n            full_backward_hooks += _global_backward_hooks.values()\n        if self._is_full_backward_hook is True:\n            full_backward_hooks += self._backward_hooks.values()\n\n        non_full_backward_hooks: list[Callable] = []\n        if _global_is_full_backward_hook is False:\n            non_full_backward_hooks += _global_backward_hooks.values()\n        if self._is_full_backward_hook is False:\n            non_full_backward_hooks += self._backward_hooks.values()\n\n        return full_backward_hooks, non_full_backward_hooks\n\n    def _get_backward_pre_hooks(self):\n        backward_pre_hooks: list[Callable] = []\n        backward_pre_hooks += _global_backward_pre_hooks.values()\n        backward_pre_hooks += self._backward_pre_hooks.values()\n\n        return backward_pre_hooks\n\n    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):\n        if not isinstance(result, torch.Tensor):\n            if not (\n                isinstance(result, tuple)\n                and all(isinstance(r, torch.Tensor) for r in result)\n            ):\n                warnings.warn(\n                    \"Using non-full backward hooks on a Module that does not return a \"\n                    \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n                    \"in future versions. This hook will be missing some of the grad_output. \"\n                    \"Please use register_full_backward_hook to get the documented behavior.\",\n                    FutureWarning,\n                    stacklevel=2,\n                )\n                return\n        else:\n            result = (result,)\n\n        if not isinstance(inputs, torch.Tensor):\n            if not (\n                isinstance(inputs, tuple)\n                and all(isinstance(i, torch.Tensor) for i in inputs)\n            ):\n                warnings.warn(\n                    \"Using non-full backward hooks on a Module that does not take as input a \"\n                    \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n                    \"in future versions. This hook will be missing some of the grad_input. \"\n                    \"Please use register_full_backward_hook to get the documented behavior.\",\n                    FutureWarning,\n                    stacklevel=2,\n                )\n                return\n        else:\n            inputs = (inputs,)\n\n        # At this point we are sure that inputs and result are tuple of Tensors\n        out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}\n        if len(out_grad_fn) == 0 or (\n            len(out_grad_fn) == 1 and grad_fn not in out_grad_fn\n        ):\n            warnings.warn(\n                \"Using a non-full backward hook when outputs are nested in python data structure \"\n                \"is deprecated and will be removed in future versions. This hook will be missing \"\n                \"some grad_output.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        elif len(out_grad_fn) > 1:\n            warnings.warn(\n                \"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n                \"is deprecated and will be removed in future versions. This hook will be missing \"\n                \"some grad_output. Please use register_full_backward_hook to get the documented behavior.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            # At this point the grad_output part of the hook will most likely be correct\n            inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}\n\n            next_functions = {n[0] for n in grad_fn.next_functions}\n\n            if inputs_grad_fn != next_functions:\n                warnings.warn(\n                    \"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n                    \"is deprecated and will be removed in future versions. This hook will be missing \"\n                    \"some grad_input. Please use register_full_backward_hook to get the documented \"\n                    \"behavior.\",\n                    FutureWarning,\n                    stacklevel=2,\n                )\n\n    def register_forward_pre_hook(\n        self,\n        hook: Union[\n            Callable[[T, tuple[Any, ...]], Optional[Any]],\n            Callable[\n                [T, tuple[Any, ...], dict[str, Any]],\n                Optional[tuple[Any, dict[str, Any]]],\n            ],\n        ],\n        *,\n        prepend: bool = False,\n        with_kwargs: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a forward pre-hook on the module.\n\n        The hook will be called every time before :func:`forward` is invoked.\n\n\n        If ``with_kwargs`` is false or not specified, the input contains only\n        the positional arguments given to the module. Keyword arguments won't be\n        passed to the hooks and only to the ``forward``. The hook can modify the\n        input. User can either return a tuple or a single modified value in the\n        hook. We will wrap the value into a tuple if a single value is returned\n        (unless that value is already a tuple). The hook should have the\n        following signature::\n\n            hook(module, args) -> None or modified input\n\n        If ``with_kwargs`` is true, the forward pre-hook will be passed the\n        kwargs given to the forward function. And if the hook modifies the\n        input, both the args and kwargs should be returned. The hook should have\n        the following signature::\n\n            hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``forward_pre`` hooks on this\n                :class:`torch.nn.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward_pre`` hooks\n                on this :class:`torch.nn.Module`. Note that global\n                ``forward_pre`` hooks registered with\n                :func:`register_module_forward_pre_hook` will fire before all\n                hooks registered by this method.\n                Default: ``False``\n            with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n                given to the forward function.\n                Default: ``False``\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(\n            self._forward_pre_hooks, extra_dict=self._forward_pre_hooks_with_kwargs\n        )\n        self._forward_pre_hooks[handle.id] = hook\n        if with_kwargs:\n            self._forward_pre_hooks_with_kwargs[handle.id] = True\n\n        if prepend:\n            self._forward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def register_forward_hook(\n        self,\n        hook: Union[\n            Callable[[T, tuple[Any, ...], Any], Optional[Any]],\n            Callable[[T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]],\n        ],\n        *,\n        prepend: bool = False,\n        with_kwargs: bool = False,\n        always_call: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a forward hook on the module.\n\n        The hook will be called every time after :func:`forward` has computed an output.\n\n        If ``with_kwargs`` is ``False`` or not specified, the input contains only\n        the positional arguments given to the module. Keyword arguments won't be\n        passed to the hooks and only to the ``forward``. The hook can modify the\n        output. It can modify the input inplace but it will not have effect on\n        forward since this is called after :func:`forward` is called. The hook\n        should have the following signature::\n\n            hook(module, args, output) -> None or modified output\n\n        If ``with_kwargs`` is ``True``, the forward hook will be passed the\n        ``kwargs`` given to the forward function and be expected to return the\n        output possibly modified. The hook should have the following signature::\n\n            hook(module, args, kwargs, output) -> None or modified output\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If ``True``, the provided ``hook`` will be fired\n                before all existing ``forward`` hooks on this\n                :class:`torch.nn.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward`` hooks on\n                this :class:`torch.nn.Module`. Note that global\n                ``forward`` hooks registered with\n                :func:`register_module_forward_hook` will fire before all hooks\n                registered by this method.\n                Default: ``False``\n            with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n                kwargs given to the forward function.\n                Default: ``False``\n            always_call (bool): If ``True`` the ``hook`` will be run regardless of\n                whether an exception is raised while calling the Module.\n                Default: ``False``\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(\n            self._forward_hooks,\n            extra_dict=[\n                self._forward_hooks_with_kwargs,\n                self._forward_hooks_always_called,\n            ],\n        )\n        self._forward_hooks[handle.id] = hook\n        if with_kwargs:\n            self._forward_hooks_with_kwargs[handle.id] = True\n        if always_call:\n            self._forward_hooks_always_called[handle.id] = True\n        if prepend:\n            self._forward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def _slow_forward(self, *input, **kwargs):\n        tracing_state = torch._C._get_tracing_state()\n        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):\n            return self.forward(*input, **kwargs)\n        recording_scopes = torch.jit._trace._trace_module_map is not None\n        if recording_scopes:\n            # type ignore was added because at this point one knows that\n            # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n            name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n            if name:\n                tracing_state.push_scope(name)\n            else:\n                recording_scopes = False\n        try:\n            result = self.forward(*input, **kwargs)\n        finally:\n            if recording_scopes:\n                tracing_state.pop_scope()\n        return result\n\n    def _wrapped_call_impl(self, *args, **kwargs):\n        if self._compiled_call_impl is not None:\n            return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n        else:\n            return self._call_impl(*args, **kwargs)\n\n    # torchrec tests the code consistency with the following code\n    # fmt: off\n    def _call_impl(self, *args, **kwargs):\n        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n        # If we don't have any hooks, we want to skip the rest of the logic in\n        # this function, and just call forward.\n        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n                or _global_backward_pre_hooks or _global_backward_hooks\n                or _global_forward_hooks or _global_forward_pre_hooks):\n            return forward_call(*args, **kwargs)\n\n        result = None\n        called_always_called_hooks = set()\n\n        def inner():\n            nonlocal result, args, kwargs\n\n            full_backward_hooks, non_full_backward_hooks = [], []\n            backward_pre_hooks = []\n            if self._backward_pre_hooks or _global_backward_pre_hooks:\n                backward_pre_hooks = self._get_backward_pre_hooks()\n\n            if self._backward_hooks or _global_backward_hooks:\n                full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n\n            if _global_forward_pre_hooks or self._forward_pre_hooks:\n                for hook_id, hook in (\n                    *_global_forward_pre_hooks.items(),\n                    *self._forward_pre_hooks.items(),\n                ):\n                    if hook_id in self._forward_pre_hooks_with_kwargs:\n                        args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]\n                        if args_kwargs_result is not None:\n                            if isinstance(args_kwargs_result, tuple) and len(args_kwargs_result) == 2:\n                                args, kwargs = args_kwargs_result\n                            else:\n                                raise RuntimeError(\n                                    \"forward pre-hook must return None or a tuple \"\n                                    f\"of (new_args, new_kwargs), but got {args_kwargs_result}.\"\n                                )\n                    else:\n                        args_result = hook(self, args)\n                        if args_result is not None:\n                            if not isinstance(args_result, tuple):\n                                args_result = (args_result,)\n                            args = args_result\n\n            bw_hook = None\n            if full_backward_hooks or backward_pre_hooks:\n                bw_hook = BackwardHook(self, full_backward_hooks, backward_pre_hooks)\n                args = bw_hook.setup_input_hook(args)\n\n            result = forward_call(*args, **kwargs)\n            if _global_forward_hooks or self._forward_hooks:\n                for hook_id, hook in (\n                    *_global_forward_hooks.items(),\n                    *self._forward_hooks.items(),\n                ):\n                    # mark that always called hook is run\n                    if hook_id in self._forward_hooks_always_called or hook_id in _global_forward_hooks_always_called:\n                        called_always_called_hooks.add(hook_id)\n\n                    if hook_id in self._forward_hooks_with_kwargs or hook_id in _global_forward_hooks_with_kwargs:\n                        hook_result = hook(self, args, kwargs, result)\n                    else:\n                        hook_result = hook(self, args, result)\n\n                    if hook_result is not None:\n                        result = hook_result\n\n            if bw_hook:\n                if not isinstance(result, (torch.Tensor, tuple)):\n                    warnings.warn(\"For backward hooks to be called,\"\n                                  \" module output should be a Tensor or a tuple of Tensors\"\n                                  f\" but received {type(result)}\")\n                result = bw_hook.setup_output_hook(result)\n\n            # Handle the non-full backward hooks\n            if non_full_backward_hooks:\n                var = result\n                while not isinstance(var, torch.Tensor):\n                    if isinstance(var, dict):\n                        var = next(v for v in var.values() if isinstance(v, torch.Tensor))\n                    else:\n                        var = var[0]\n                grad_fn = var.grad_fn\n                if grad_fn is not None:\n                    for hook in non_full_backward_hooks:\n                        grad_fn.register_hook(_WrappedHook(hook, self))\n                    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n\n            return result\n\n        # This is technically not behavior equivalent when compiling, but it's\n        # incredibly unlikely we will ever support throwing an exception in NN\n        # module, and then catching it here, and then reraising it, and then\n        # catching it again, and expecting the resulting frame to be compiled.\n        # The reraise here just gunks up our exception handling for no good\n        # reason.  Don't try to run the always called hooks in event of\n        # exception.\n        if torch.compiler.is_compiling():\n            return inner()\n\n        try:\n            return inner()\n        except Exception:\n            # run always called hooks if they have not already been run\n            # For now only forward hooks have the always_call option but perhaps\n            # this functionality should be added to full backward hooks as well.\n            for hook_id, hook in _global_forward_hooks.items():\n                if hook_id in _global_forward_hooks_always_called and hook_id not in called_always_called_hooks:  # type: ignore[possibly-undefined]\n                    try:\n                        hook_result = hook(self, args, result)  # type: ignore[possibly-undefined]\n                        if hook_result is not None:\n                            result = hook_result\n                    except Exception as e:\n                        warnings.warn(\"global module forward hook with ``always_call=True`` raised an exception \"\n                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n                        continue\n\n            for hook_id, hook in self._forward_hooks.items():\n                if hook_id in self._forward_hooks_always_called and hook_id not in called_always_called_hooks:  # type: ignore[possibly-undefined]\n                    try:\n                        if hook_id in self._forward_hooks_with_kwargs:\n                            hook_result = hook(self, args, kwargs, result)  # type: ignore[possibly-undefined]\n                        else:\n                            hook_result = hook(self, args, result)  # type: ignore[possibly-undefined]\n                        if hook_result is not None:\n                            result = hook_result\n                    except Exception as e:\n                        warnings.warn(\"module forward hook with ``always_call=True`` raised an exception \"\n                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n                        continue\n            # raise exception raised in try block\n            raise\n    # fmt: on\n\n    __call__: Callable[..., Any] = _wrapped_call_impl\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state.pop(\"_compiled_call_impl\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n        # Support loading old checkpoints that don't have the following attrs:\n        if \"_forward_pre_hooks\" not in self.__dict__:\n            self._forward_pre_hooks = OrderedDict()\n        if \"_forward_pre_hooks_with_kwargs\" not in self.__dict__:\n            self._forward_pre_hooks_with_kwargs = OrderedDict()\n        if \"_forward_hooks_with_kwargs\" not in self.__dict__:\n            self._forward_hooks_with_kwargs = OrderedDict()\n        if \"_forward_hooks_always_called\" not in self.__dict__:\n            self._forward_hooks_always_called = OrderedDict()\n        if \"_state_dict_hooks\" not in self.__dict__:\n            self._state_dict_hooks = OrderedDict()\n        if \"_state_dict_pre_hooks\" not in self.__dict__:\n            self._state_dict_pre_hooks = OrderedDict()\n        if \"_load_state_dict_pre_hooks\" not in self.__dict__:\n            self._load_state_dict_pre_hooks = OrderedDict()\n        if \"_load_state_dict_post_hooks\" not in self.__dict__:\n            self._load_state_dict_post_hooks = OrderedDict()\n        if \"_non_persistent_buffers_set\" not in self.__dict__:\n            self._non_persistent_buffers_set = set()\n        if \"_is_full_backward_hook\" not in self.__dict__:\n            self._is_full_backward_hook = None\n        if \"_backward_pre_hooks\" not in self.__dict__:\n            self._backward_pre_hooks = OrderedDict()\n\n    # It is crucial that the return type is not annotated as `Any`, otherwise type checking\n    # on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\n    # https://github.com/pytorch/pytorch/pull/115074\n    def __getattr__(self, name: str) -> Union[Tensor, \"Module\"]:\n        if \"_parameters\" in self.__dict__:\n            _parameters = self.__dict__[\"_parameters\"]\n            if name in _parameters:\n                return _parameters[name]\n        if \"_buffers\" in self.__dict__:\n            _buffers = self.__dict__[\"_buffers\"]\n            if name in _buffers:\n                return _buffers[name]\n        if \"_modules\" in self.__dict__:\n            modules = self.__dict__[\"_modules\"]\n            if name in modules:\n                return modules[name]\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setattr__(self, name: str, value: Union[Tensor, \"Module\"]) -> None:\n        def remove_from(*dicts_or_sets):\n            for d in dicts_or_sets:\n                if name in d:\n                    if isinstance(d, dict):\n                        del d[name]\n                    else:\n                        d.discard(name)\n\n        params = self.__dict__.get(\"_parameters\")\n        if isinstance(value, Parameter):\n            if params is None:\n                raise AttributeError(\n                    \"cannot assign parameters before Module.__init__() call\"\n                )\n            remove_from(\n                self.__dict__,\n                self._buffers,\n                self._modules,\n                self._non_persistent_buffers_set,\n            )\n            self.register_parameter(name, value)\n        elif params is not None and name in params:\n            if value is not None:\n                raise TypeError(\n                    f\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\n                    \"(torch.nn.Parameter or None expected)\"\n                )\n            self.register_parameter(name, value)\n        else:\n            modules = self.__dict__.get(\"_modules\")\n            if isinstance(value, Module):\n                if modules is None:\n                    raise AttributeError(\n                        \"cannot assign module before Module.__init__() call\"\n                    )\n                remove_from(\n                    self.__dict__,\n                    self._parameters,\n                    self._buffers,\n                    self._non_persistent_buffers_set,\n                )\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            elif modules is not None and name in modules:\n                if value is not None:\n                    raise TypeError(\n                        f\"cannot assign '{torch.typename(value)}' as child module '{name}' \"\n                        \"(torch.nn.Module or None expected)\"\n                    )\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            else:\n                buffers = self.__dict__.get(\"_buffers\")\n                if isinstance(value, Buffer) or buffers is not None and name in buffers:\n                    if value is not None and not isinstance(value, torch.Tensor):\n                        raise TypeError(\n                            f\"cannot assign '{torch.typename(value)}' as buffer '{name}' \"\n                            \"(torch.nn.Buffer, torch.Tensor or None expected)\"\n                        )\n                    if isinstance(value, Buffer):\n                        persistent = value.persistent\n                    else:\n                        persistent = name not in self._non_persistent_buffers_set\n                    # === HACK ===\n                    # This whole block below should just be:\n                    # self.register_buffer(name, value, persistent)\n\n                    # But to support subclasses of nn.Module that (wrongfully) implement a\n                    # register_buffer() method that doesn't have the \"persistent\"\n                    # argument. Only pass it in if it is accepted otherwise assume\n                    # it is always true\n                    if self.register_buffer is torch.nn.Module.register_buffer:\n                        self.register_buffer(name, value, persistent)\n                    else:\n                        sign = inspect.signature(self.register_buffer)\n                        if \"persistent\" in sign.parameters:\n                            self.register_buffer(name, value, persistent)\n                        else:\n                            if not persistent:\n                                raise RuntimeError(\n                                    \"Registering a non-persistent buffer \"\n                                    \"on a Module subclass that implements \"\n                                    \"register_buffer() without the persistent \"\n                                    \"argument is not allowed.\"\n                                )\n                            # Assume that the implementation without the argument has the\n                            # behavior from before the argument was added: persistent=True\n                            self.register_buffer(name, value)\n                    # === HACK END ===\n                else:\n                    super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        if name in self._parameters:\n            del self._parameters[name]\n        elif name in self._buffers:\n            del self._buffers[name]\n            self._non_persistent_buffers_set.discard(name)\n        elif name in self._modules:\n            del self._modules[name]\n        else:\n            super().__delattr__(name)\n\n    def _register_state_dict_hook(self, hook):\n        r\"\"\"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata) -> None or state_dict\n\n        The registered hooks can modify the ``state_dict`` inplace or return a new one.\n        If a new ``state_dict`` is returned, it will only be respected if it is the root\n        module that :meth:`~nn.Module.state_dict` is called from.\n        \"\"\"\n        if getattr(hook, \"_from_public_api\", False):\n            raise RuntimeError(\n                \"Cannot register the same function as the state dict post hook that was \"\n                \"previously registered via register_state_dict_post_hook\"\n            )\n        handle = RemovableHandle(self._state_dict_hooks)\n        self._state_dict_hooks[handle.id] = hook\n        return handle\n\n    def register_state_dict_post_hook(self, hook):\n        r\"\"\"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata) -> None\n\n        The registered hooks can modify the ``state_dict`` inplace.\n        \"\"\"\n        # In _register_state_dict_hook there was a bug described in\n        # https://github.com/pytorch/pytorch/issues/117437 where the return value\n        # was only respected for the root module but not child submodules.\n        # We fix this in this public version by only allowing inplace modifications on\n        # the state_dict by the hook. However, since hooks registered via both these\n        # APIs will be added to `_state_dict_hooks` and the type of `_state_dict_hooks`\n        # cannot be changed due to many dependencies on it, we mark a hook\n        # as being registered via the public API by setting `_from_public_api` on it.\n        # In the implementation of `state_dict`, if the callable does not have this\n        # flag, the old behavior of respecting the return value will be preserved\n        # for the root module, otherwise, we ensure that the hook returns None.\n        hook._from_public_api = True\n        handle = RemovableHandle(self._state_dict_hooks)\n        self._state_dict_hooks[handle.id] = hook\n        return handle\n\n    def register_state_dict_pre_hook(self, hook):\n        r\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\n        It should have the following signature::\n            hook(module, prefix, keep_vars) -> None\n\n        The registered hooks can be used to perform pre-processing before the ``state_dict``\n        call is made.\n        \"\"\"\n        handle = RemovableHandle(self._state_dict_pre_hooks)\n        self._state_dict_pre_hooks[handle.id] = hook\n        return handle\n\n    def _save_to_state_dict(self, destination, prefix, keep_vars):\n        r\"\"\"Save module state to the `destination` dictionary.\n\n        The `destination` dictionary will contain the state\n        of the module, but not its descendants. This is called on every\n        submodule in :meth:`~torch.nn.Module.state_dict`.\n\n        In rare cases, subclasses can achieve class-specific behavior by\n        overriding this method with custom logic.\n\n        Args:\n            destination (dict): a dict where state will be stored\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n        \"\"\"\n        for name, param in self._parameters.items():\n            if param is not None:\n                destination[prefix + name] = param if keep_vars else param.detach()\n        for name, buf in self._buffers.items():\n            if buf is not None and name not in self._non_persistent_buffers_set:\n                destination[prefix + name] = buf if keep_vars else buf.detach()\n        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n        if (\n            getattr(self.__class__, \"get_extra_state\", Module.get_extra_state)\n            is not Module.get_extra_state\n        ):\n            destination[extra_state_key] = self.get_extra_state()\n\n    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\n    # back that same object. But if they pass nothing, an `OrderedDict` is created and returned.\n    T_destination = TypeVar(\"T_destination\", bound=dict[str, Any])\n\n    @overload\n    def state_dict(\n        self, *, destination: T_destination, prefix: str = ..., keep_vars: bool = ...\n    ) -> T_destination:\n        ...\n\n    @overload\n    def state_dict(self, *, prefix: str = ..., keep_vars: bool = ...) -> dict[str, Any]:\n        ...\n\n    # TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\n    # Also remove the logic for arg parsing together.\n    def state_dict(self, *args, destination=None, prefix=\"\", keep_vars=False):\n        r\"\"\"Return a dictionary containing references to the whole state of the module.\n\n        Both parameters and persistent buffers (e.g. running averages) are\n        included. Keys are corresponding parameter and buffer names.\n        Parameters and buffers set to ``None`` are not included.\n\n        .. note::\n            The returned object is a shallow copy. It contains references\n            to the module's parameters and buffers.\n\n        .. warning::\n            Currently ``state_dict()`` also accepts positional arguments for\n            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n            this is being deprecated and keyword arguments will be enforced in\n            future releases.\n\n        .. warning::\n            Please avoid the use of argument ``destination`` as it is not\n            designed for end-users.\n\n        Args:\n            destination (dict, optional): If provided, the state of module will\n                be updated into the dict and the same object is returned.\n                Otherwise, an ``OrderedDict`` will be created and returned.\n                Default: ``None``.\n            prefix (str, optional): a prefix added to parameter and buffer\n                names to compose the keys in state_dict. Default: ``''``.\n            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n                returned in the state dict are detached from autograd. If it's\n                set to ``True``, detaching will not be performed.\n                Default: ``False``.\n\n        Returns:\n            dict:\n                a dictionary containing a whole state of the module\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> module.state_dict().keys()\n            ['bias', 'weight']\n\n        \"\"\"\n        # TODO: Remove `args` and the parsing logic when BC allows.\n        if len(args) > 0:\n            # DeprecationWarning is ignored by default\n            warnings.warn(\n                \"Positional args are being deprecated, use kwargs instead. Refer to \"\n                \"https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\n                \" for details.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            if destination is None:\n                destination = args[0]\n            if len(args) > 1 and prefix == \"\":\n                prefix = args[1]\n            if len(args) > 2 and keep_vars is False:\n                keep_vars = args[2]\n\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n\n        local_metadata = dict(version=self._version)\n        if hasattr(destination, \"_metadata\"):\n            destination._metadata[prefix[:-1]] = local_metadata\n\n        for hook in self._state_dict_pre_hooks.values():\n            hook(self, prefix, keep_vars)\n        self._save_to_state_dict(destination, prefix, keep_vars)\n        for name, module in self._modules.items():\n            if module is not None:\n                module.state_dict(\n                    destination=destination,\n                    prefix=prefix + name + \".\",\n                    keep_vars=keep_vars,\n                )\n        for hook in self._state_dict_hooks.values():\n            hook_result = hook(self, destination, prefix, local_metadata)\n            if not getattr(hook, \"_from_public_api\", False):\n                if hook_result is not None:\n                    destination = hook_result\n            else:\n                if hook_result is not None:\n                    raise RuntimeError(\"state_dict post-hook must return None\")\n        return destination\n\n    def _register_load_state_dict_pre_hook(self, hook, with_module=False):\n        r\"\"\"See :meth:`~torch.nn.Module.register_load_state_dict_pre_hook` for details.\n\n        A subtle difference is that if ``with_module`` is set to ``False``, then the\n        hook will not take the ``module`` as the first argument whereas\n        :meth:`~torch.nn.Module.register_load_state_dict_pre_hook` always takes the\n        ``module`` as the first argument.\n\n        Arguments:\n            hook (Callable): Callable hook that will be invoked before\n                loading the state dict.\n            with_module (bool, optional): Whether or not to pass the module\n                instance to the hook as the first parameter.\n        \"\"\"\n        handle = RemovableHandle(self._load_state_dict_pre_hooks)\n        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(\n            hook, self if with_module else None\n        )\n        return handle\n\n    def register_load_state_dict_pre_hook(self, hook):\n        r\"\"\"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n\n        Arguments:\n            hook (Callable): Callable hook that will be invoked before\n                loading the state dict.\n        \"\"\"\n        return self._register_load_state_dict_pre_hook(hook, with_module=True)\n\n    def register_load_state_dict_post_hook(self, hook):\n        r\"\"\"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\n        It should have the following signature::\n            hook(module, incompatible_keys) -> None\n\n        The ``module`` argument is the current module that this hook is registered\n        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n        is a ``list`` of ``str`` containing the missing keys and\n        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\n        The given incompatible_keys can be modified inplace if needed.\n\n        Note that the checks performed when calling :func:`load_state_dict` with\n        ``strict=True`` are affected by modifications the hook makes to\n        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n        set of keys will result in an error being thrown when ``strict=True``, and\n        clearing out both missing and unexpected keys will avoid an error.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(self._load_state_dict_post_hooks)\n        self._load_state_dict_post_hooks[handle.id] = hook\n        return handle\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        r\"\"\"Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.\n\n        This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n        For state dicts without metadata, :attr:`local_metadata` is empty.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at `local_metadata.get(\"version\", None)`.\n        Additionally, :attr:`local_metadata` can also contain the key\n        `assign_to_params_buffers` that indicates whether keys should be\n        assigned their corresponding tensor in the state_dict.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Args:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            local_metadata (dict): a dict containing the metadata for this module.\n                See\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=True``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=True``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        for hook in self._load_state_dict_pre_hooks.values():\n            hook(\n                state_dict,\n                prefix,\n                local_metadata,\n                strict,\n                missing_keys,\n                unexpected_keys,\n                error_msgs,\n            )\n\n        persistent_buffers = {\n            k: v\n            for k, v in self._buffers.items()\n            if k not in self._non_persistent_buffers_set\n        }\n        local_name_params = itertools.chain(\n            self._parameters.items(), persistent_buffers.items()\n        )\n        local_state = {k: v for k, v in local_name_params if v is not None}\n        assign_to_params_buffers = local_metadata.get(\"assign_to_params_buffers\", False)\n        use_swap_tensors = torch.__future__.get_swap_module_params_on_conversion()\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n                if not torch.overrides.is_tensor_like(input_param):\n                    error_msgs.append(\n                        f'While copying the parameter named \"{key}\", '\n                        \"expected torch.Tensor or Tensor-like object from checkpoint but \"\n                        f\"received {type(input_param)}\"\n                    )\n                    continue\n\n                # This is used to avoid copying uninitialized parameters into\n                # non-lazy modules, since they dont have the hook to do the checks\n                # in such case, it will error when accessing the .shape attribute.\n                is_param_lazy = torch.nn.parameter.is_lazy(param)\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n                if (\n                    not is_param_lazy\n                    and len(param.shape) == 0\n                    and len(input_param.shape) == 1\n                ):\n                    input_param = input_param[0]\n\n                if not is_param_lazy and input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append(\n                        f\"size mismatch for {key}: copying a param with shape {input_param.shape} from checkpoint, \"\n                        f\"the shape in current model is {param.shape}.\"\n                    )\n                    continue\n\n                if (\n                    param.is_meta\n                    and not input_param.is_meta\n                    and not assign_to_params_buffers\n                ):\n                    warnings.warn(\n                        f\"for {key}: copying from a non-meta parameter in the checkpoint to a meta \"\n                        \"parameter in the current model, which is a no-op. (Did you mean to \"\n                        \"pass `assign=True` to assign items in the state dictionary to their \"\n                        \"corresponding key in the module instead of copying them in place?)\"\n                    )\n\n                try:\n                    with torch.no_grad():\n                        if use_swap_tensors:\n                            new_input_param = param.module_load(\n                                input_param, assign=assign_to_params_buffers\n                            )\n                            if id(new_input_param) == id(input_param) or id(\n                                new_input_param\n                            ) == id(param):\n                                raise RuntimeError(\n                                    \"module_load returned one of self or other, please .detach() \"\n                                    \"the result if returning one of the inputs in module_load\"\n                                )\n                            if isinstance(param, torch.nn.Parameter):\n                                if not isinstance(new_input_param, torch.nn.Parameter):\n                                    new_input_param = torch.nn.Parameter(\n                                        new_input_param,\n                                        requires_grad=param.requires_grad,\n                                    )\n                                else:\n                                    new_input_param.requires_grad_(param.requires_grad)\n                            torch.utils.swap_tensors(param, new_input_param)\n                            del new_input_param\n                        elif assign_to_params_buffers:\n                            # Shape checks are already done above\n                            if isinstance(param, torch.nn.Parameter):\n                                if not isinstance(input_param, torch.nn.Parameter):\n                                    input_param = torch.nn.Parameter(\n                                        input_param, requires_grad=param.requires_grad\n                                    )\n                                else:\n                                    input_param.requires_grad_(param.requires_grad)\n                            setattr(self, name, input_param)\n                        else:\n                            param.copy_(input_param)\n                except Exception as ex:\n                    action = \"swapping\" if use_swap_tensors else \"copying\"\n                    error_msgs.append(\n                        f'While {action} the parameter named \"{key}\", '\n                        f\"whose dimensions in the model are {param.size()} and \"\n                        f\"whose dimensions in the checkpoint are {input_param.size()}, \"\n                        f\"an exception occurred : {ex.args}.\"\n                    )\n            elif strict:\n                missing_keys.append(key)\n\n        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n        if (\n            getattr(self.__class__, \"set_extra_state\", Module.set_extra_state)\n            is not Module.set_extra_state\n        ):\n            if extra_state_key in state_dict:\n                self.set_extra_state(state_dict[extra_state_key])\n            elif strict:\n                missing_keys.append(extra_state_key)\n        elif strict and (extra_state_key in state_dict):\n            unexpected_keys.append(extra_state_key)\n\n        if strict:\n            for key in state_dict.keys():\n                if key.startswith(prefix) and key != extra_state_key:\n                    input_name = key[len(prefix) :].split(\".\", 1)\n                    # Must be Module if it have attributes\n                    if len(input_name) > 1:\n                        if input_name[0] not in self._modules:\n                            unexpected_keys.append(key)\n                    elif input_name[0] not in local_state:\n                        unexpected_keys.append(key)\n\n    def load_state_dict(\n        self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False\n    ):\n        r\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\n        If :attr:`strict` is ``True``, then\n        the keys of :attr:`state_dict` must exactly match the keys returned\n        by this module's :meth:`~torch.nn.Module.state_dict` function.\n\n        .. warning::\n            If :attr:`assign` is ``True`` the optimizer must be created after\n            the call to :attr:`load_state_dict` unless\n            :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n\n        Args:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            strict (bool, optional): whether to strictly enforce that the keys\n                in :attr:`state_dict` match the keys returned by this module's\n                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n            assign (bool, optional): When set to ``False``, the properties of the tensors\n                in the current module are preserved whereas setting it to ``True`` preserves\n                properties of the Tensors in the state dict. The only\n                exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n                for which the value from the module is preserved.\n                Default: ``False``\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n                * **missing_keys** is a list of str containing any keys that are expected\n                    by this module but missing from the provided ``state_dict``.\n                * **unexpected_keys** is a list of str containing the keys that are not\n                    expected by this module but present in the provided ``state_dict``.\n\n        Note:\n            If a parameter or buffer is registered as ``None`` and its corresponding key\n            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n            ``RuntimeError``.\n        \"\"\"\n        if not isinstance(state_dict, Mapping):\n            raise TypeError(\n                f\"Expected state_dict to be dict-like, got {type(state_dict)}.\"\n            )\n\n        missing_keys: list[str] = []\n        unexpected_keys: list[str] = []\n        error_msgs: list[str] = []\n\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \"_metadata\", None)\n        state_dict = OrderedDict(state_dict)\n        if metadata is not None:\n            # mypy isn't aware that \"_metadata\" exists in state_dict\n            state_dict._metadata = metadata  # type: ignore[attr-defined]\n\n        def load(module, local_state_dict, prefix=\"\"):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            if assign:\n                local_metadata[\"assign_to_params_buffers\"] = assign\n            module._load_from_state_dict(\n                local_state_dict,\n                prefix,\n                local_metadata,\n                True,\n                missing_keys,\n                unexpected_keys,\n                error_msgs,\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    child_prefix = prefix + name + \".\"\n                    child_state_dict = {\n                        k: v\n                        for k, v in local_state_dict.items()\n                        if k.startswith(child_prefix)\n                    }\n                    load(child, child_state_dict, child_prefix)  # noqa: F821\n\n            # Note that the hook can modify missing_keys and unexpected_keys.\n            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n            for hook in module._load_state_dict_post_hooks.values():\n                out = hook(module, incompatible_keys)\n                assert out is None, (\n                    \"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\n                    \"expected to return new values, if incompatible_keys need to be modified,\"\n                    \"it should be done inplace.\"\n                )\n\n        load(self, state_dict)\n        del load\n\n        if strict:\n            if len(unexpected_keys) > 0:\n                error_msgs.insert(\n                    0,\n                    \"Unexpected key(s) in state_dict: {}. \".format(\n                        \", \".join(f'\"{k}\"' for k in unexpected_keys)\n                    ),\n                )\n            if len(missing_keys) > 0:\n                error_msgs.insert(\n                    0,\n                    \"Missing key(s) in state_dict: {}. \".format(\n                        \", \".join(f'\"{k}\"' for k in missing_keys)\n                    ),\n                )\n\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n                    self.__class__.__name__, \"\\n\\t\".join(error_msgs)\n                )\n            )\n        return _IncompatibleKeys(missing_keys, unexpected_keys)\n\n    def _named_members(\n        self, get_members_fn, prefix=\"\", recurse=True, remove_duplicate: bool = True\n    ):\n        r\"\"\"Help yield various names + members of modules.\"\"\"\n        memo = set()\n        modules = (\n            self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n            if recurse\n            else [(prefix, self)]\n        )\n        for module_prefix, module in modules:\n            members = get_members_fn(module)\n            for k, v in members:\n                if v is None or v in memo:\n                    continue\n                if remove_duplicate:\n                    memo.add(v)\n                name = module_prefix + (\".\" if module_prefix else \"\") + k\n                yield name, v\n\n    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n        r\"\"\"Return an iterator over module parameters.\n\n        This is typically passed to an optimizer.\n\n        Args:\n            recurse (bool): if True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n\n        Yields:\n            Parameter: module parameter\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for param in model.parameters():\n            >>>     print(type(param), param.size())\n            <class 'torch.Tensor'> (20L,)\n            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n        \"\"\"\n        for _name, param in self.named_parameters(recurse=recurse):\n            yield param\n\n    def named_parameters(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[tuple[str, Parameter]]:\n        r\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n\n        Args:\n            prefix (str): prefix to prepend to all parameter names.\n            recurse (bool): if True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n            remove_duplicate (bool, optional): whether to remove the duplicated\n                parameters in the result. Defaults to True.\n\n        Yields:\n            (str, Parameter): Tuple containing the name and parameter\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, param in self.named_parameters():\n            >>>     if name in ['bias']:\n            >>>         print(param.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._parameters.items(),\n            prefix=prefix,\n            recurse=recurse,\n            remove_duplicate=remove_duplicate,\n        )\n        yield from gen\n\n    def buffers(self, recurse: bool = True) -> Iterator[Tensor]:\n        r\"\"\"Return an iterator over module buffers.\n\n        Args:\n            recurse (bool): if True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module.\n\n        Yields:\n            torch.Tensor: module buffer\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for buf in model.buffers():\n            >>>     print(type(buf), buf.size())\n            <class 'torch.Tensor'> (20L,)\n            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n        \"\"\"\n        for _, buf in self.named_buffers(recurse=recurse):\n            yield buf\n\n    def named_buffers(\n        self, prefix: str = \"\", recurse: bool = True, remove_duplicate: bool = True\n    ) -> Iterator[tuple[str, Tensor]]:\n        r\"\"\"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n\n        Args:\n            prefix (str): prefix to prepend to all buffer names.\n            recurse (bool, optional): if True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module. Defaults to True.\n            remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\n        Yields:\n            (str, torch.Tensor): Tuple containing the name and buffer\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, buf in self.named_buffers():\n            >>>     if name in ['running_var']:\n            >>>         print(buf.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._buffers.items(),\n            prefix=prefix,\n            recurse=recurse,\n            remove_duplicate=remove_duplicate,\n        )\n        yield from gen\n\n    def children(self) -> Iterator[\"Module\"]:\n        r\"\"\"Return an iterator over immediate children modules.\n\n        Yields:\n            Module: a child module\n        \"\"\"\n        for _name, module in self.named_children():\n            yield module\n\n    def named_children(self) -> Iterator[tuple[str, \"Module\"]]:\n        r\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\n        Yields:\n            (str, Module): Tuple containing a name and child module\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, module in model.named_children():\n            >>>     if name in ['conv4', 'conv5']:\n            >>>         print(module)\n\n        \"\"\"\n        memo = set()\n        for name, module in self._modules.items():\n            if module is not None and module not in memo:\n                memo.add(module)\n                yield name, module\n\n    def modules(self) -> Iterator[\"Module\"]:\n        r\"\"\"Return an iterator over all modules in the network.\n\n        Yields:\n            Module: a module in the network\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n\n        Example::\n\n            >>> l = nn.Linear(2, 2)\n            >>> net = nn.Sequential(l, l)\n            >>> for idx, m in enumerate(net.modules()):\n            ...     print(idx, '->', m)\n\n            0 -> Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            )\n            1 -> Linear(in_features=2, out_features=2, bias=True)\n\n        \"\"\"\n        for _, module in self.named_modules():\n            yield module\n\n    def named_modules(\n        self,\n        memo: Optional[set[\"Module\"]] = None,\n        prefix: str = \"\",\n        remove_duplicate: bool = True,\n    ):\n        r\"\"\"Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n\n        Args:\n            memo: a memo to store the set of modules already added to the result\n            prefix: a prefix that will be added to the name of the module\n            remove_duplicate: whether to remove the duplicated module instances in the result\n                or not\n\n        Yields:\n            (str, Module): Tuple of name and module\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n\n        Example::\n\n            >>> l = nn.Linear(2, 2)\n            >>> net = nn.Sequential(l, l)\n            >>> for idx, m in enumerate(net.named_modules()):\n            ...     print(idx, '->', m)\n\n            0 -> ('', Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            ))\n            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n\n        \"\"\"\n        if memo is None:\n            memo = set()\n        if self not in memo:\n            if remove_duplicate:\n                memo.add(self)\n            yield prefix, self\n            for name, module in self._modules.items():\n                if module is None:\n                    continue\n                submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n                yield from module.named_modules(\n                    memo, submodule_prefix, remove_duplicate\n                )\n\n    def train(self: T, mode: bool = True) -> T:\n        r\"\"\"Set the module in training mode.\n\n        This has an effect only on certain modules. See the documentation of\n        particular modules for details of their behaviors in training/evaluation\n        mode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n        etc.\n\n        Args:\n            mode (bool): whether to set training mode (``True``) or evaluation\n                         mode (``False``). Default: ``True``.\n\n        Returns:\n            Module: self\n        \"\"\"\n        if not isinstance(mode, bool):\n            raise ValueError(\"training mode is expected to be boolean\")\n        self.training = mode\n        for module in self.children():\n            module.train(mode)\n        return self\n\n    def eval(self: T) -> T:\n        r\"\"\"Set the module in evaluation mode.\n\n        This has an effect only on certain modules. See the documentation of\n        particular modules for details of their behaviors in training/evaluation\n        mode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n        etc.\n\n        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.eval()` and several similar mechanisms that may be confused with it.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self.train(False)\n\n    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n        r\"\"\"Change if autograd should record operations on parameters in this module.\n\n        This method sets the parameters' :attr:`requires_grad` attributes\n        in-place.\n\n        This method is helpful for freezing part of the module for finetuning\n        or training parts of a model individually (e.g., GAN training).\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n\n        Args:\n            requires_grad (bool): whether autograd should record operations on\n                                  parameters in this module. Default: ``True``.\n\n        Returns:\n            Module: self\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad_(requires_grad)\n        return self\n\n    def zero_grad(self, set_to_none: bool = True) -> None:\n        r\"\"\"Reset gradients of all model parameters.\n\n        See similar function under :class:`torch.optim.Optimizer` for more context.\n\n        Args:\n            set_to_none (bool): instead of setting to zero, set the grads to None.\n                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n        \"\"\"\n        if getattr(self, \"_is_replica\", False):\n            warnings.warn(\n                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n                \"The parameters are copied (in a differentiable manner) from the original module. \"\n                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n                \"If you need gradients in your forward method, consider using autograd.grad instead.\"\n            )\n\n        for p in self.parameters():\n            if p.grad is not None:\n                if set_to_none:\n                    p.grad = None\n                else:\n                    if p.grad.grad_fn is not None:\n                        p.grad.detach_()\n                    else:\n                        p.grad.requires_grad_(False)\n                    p.grad.zero_()\n\n    def share_memory(self: T) -> T:\n        r\"\"\"See :meth:`torch.Tensor.share_memory_`.\"\"\"\n        return self._apply(lambda t: t.share_memory_())\n\n    def _get_name(self):\n        return self.__class__.__name__\n\n    def extra_repr(self) -> str:\n        r\"\"\"Return the extra representation of the module.\n\n        To print customized extra information, you should re-implement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        \"\"\"\n        return \"\"\n\n    def __repr__(self):\n        # We treat the extra repr like the sub-module, one item per line\n        extra_lines = []\n        extra_repr = self.extra_repr()\n        # empty string will be split into list ['']\n        if extra_repr:\n            extra_lines = extra_repr.split(\"\\n\")\n        child_lines = []\n        for key, module in self._modules.items():\n            mod_str = repr(module)\n            mod_str = _addindent(mod_str, 2)\n            child_lines.append(\"(\" + key + \"): \" + mod_str)\n        lines = extra_lines + child_lines\n\n        main_str = self._get_name() + \"(\"\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            if len(extra_lines) == 1 and not child_lines:\n                main_str += extra_lines[0]\n            else:\n                main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def __dir__(self):\n        module_attrs = dir(self.__class__)\n        attrs = list(self.__dict__.keys())\n        parameters = list(self._parameters.keys())\n        modules = list(self._modules.keys())\n        buffers = list(self._buffers.keys())\n        keys = module_attrs + attrs + parameters + modules + buffers\n\n        # Eliminate attrs that are not legal Python variable names\n        keys = [key for key in keys if not key[0].isdigit()]\n\n        return sorted(keys)\n\n    def _replicate_for_data_parallel(self):\n        replica = self.__new__(type(self))\n        replica.__dict__ = self.__dict__.copy()\n\n        # replicas do not have parameters themselves, the replicas reference the original\n        # module.\n        replica._parameters = {}\n        replica._buffers = replica._buffers.copy()\n        replica._modules = replica._modules.copy()\n        replica._is_replica = True  # type: ignore[assignment]\n\n        return replica\n\n    def compile(self, *args, **kwargs):\n        \"\"\"\n        Compile this Module's forward using :func:`torch.compile`.\n\n        This Module's `__call__` method is compiled and all arguments are passed as-is\n        to :func:`torch.compile`.\n\n        See :func:`torch.compile` for details on the arguments for this function.\n        \"\"\"\n        self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)\n", 3003], "C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py": ["# mypy: allow-untyped-defs\nimport math\nfrom typing import Any\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import functional as F, init\nfrom torch.nn.parameter import Parameter, UninitializedParameter\n\nfrom .lazy import LazyModuleMixin\nfrom .module import Module\n\n\n__all__ = [\n    \"Bilinear\",\n    \"Identity\",\n    \"LazyLinear\",\n    \"Linear\",\n]\n\n\nclass Identity(Module):\n    r\"\"\"A placeholder identity operator that is argument-insensitive.\n\n    Args:\n        args: any argument (unused)\n        kwargs: any keyword argument (unused)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Examples::\n\n        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n        >>> input = torch.randn(128, 20)\n        >>> output = m(input)\n        >>> print(output.size())\n        torch.Size([128, 20])\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__()\n\n    def forward(self, input: Tensor) -> Tensor:\n        return input\n\n\nclass Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Shape:\n        - Input: :math:`(*, H_\\text{in})` where :math:`*` means any number of\n          dimensions including none and :math:`H_\\text{in} = \\text{in\\_features}`.\n        - Output: :math:`(*, H_\\text{out})` where all but the last dimension\n          are the same shape as the input and :math:`H_\\text{out} = \\text{out\\_features}`.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                :math:`k = \\frac{1}{\\text{in\\_features}}`\n\n    Examples::\n\n        >>> m = nn.Linear(20, 30)\n        >>> input = torch.randn(128, 20)\n        >>> output = m(input)\n        >>> print(output.size())\n        torch.Size([128, 30])\n    \"\"\"\n\n    __constants__ = [\"in_features\", \"out_features\"]\n    in_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(\n            torch.empty((out_features, in_features), **factory_kwargs)\n        )\n        if bias:\n            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n        # https://github.com/pytorch/pytorch/issues/57109\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n\n\n# This class exists solely to avoid triggering an obscure error when scripting\n# an improperly quantized attention layer. See this issue for details:\n# https://github.com/pytorch/pytorch/issues/58969\n# TODO: fail fast on quantization API usage error, then remove this class\n# and replace uses of it with plain Linear\nclass NonDynamicallyQuantizableLinear(Linear):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        super().__init__(\n            in_features, out_features, bias=bias, device=device, dtype=dtype\n        )\n\n\nclass Bilinear(Module):\n    r\"\"\"Applies a bilinear transformation to the incoming data: :math:`y = x_1^T A x_2 + b`.\n\n    Args:\n        in1_features: size of each first input sample\n        in2_features: size of each second input sample\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Shape:\n        - Input1: :math:`(*, H_\\text{in1})` where :math:`H_\\text{in1}=\\text{in1\\_features}` and\n          :math:`*` means any number of additional dimensions including none. All but the last dimension\n          of the inputs should be the same.\n        - Input2: :math:`(*, H_\\text{in2})` where :math:`H_\\text{in2}=\\text{in2\\_features}`.\n        - Output: :math:`(*, H_\\text{out})` where :math:`H_\\text{out}=\\text{out\\_features}`\n          and all but the last dimension are the same shape as the input.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})`.\n            The values are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in1\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n                :math:`k = \\frac{1}{\\text{in1\\_features}}`\n\n    Examples::\n\n        >>> m = nn.Bilinear(20, 30, 40)\n        >>> input1 = torch.randn(128, 20)\n        >>> input2 = torch.randn(128, 30)\n        >>> output = m(input1, input2)\n        >>> print(output.size())\n        torch.Size([128, 40])\n    \"\"\"\n\n    __constants__ = [\"in1_features\", \"in2_features\", \"out_features\"]\n    in1_features: int\n    in2_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(\n        self,\n        in1_features: int,\n        in2_features: int,\n        out_features: int,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.in1_features = in1_features\n        self.in2_features = in2_features\n        self.out_features = out_features\n        self.weight = Parameter(\n            torch.empty((out_features, in1_features, in2_features), **factory_kwargs)\n        )\n\n        if bias:\n            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        bound = 1 / math.sqrt(self.weight.size(1))\n        init.uniform_(self.weight, -bound, bound)\n        if self.bias is not None:\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:\n        return F.bilinear(input1, input2, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return (\n            f\"in1_features={self.in1_features}, in2_features={self.in2_features}, \"\n            f\"out_features={self.out_features}, bias={self.bias is not None}\"\n        )\n\n\nclass LazyLinear(LazyModuleMixin, Linear):\n    r\"\"\"A :class:`torch.nn.Linear` module where `in_features` is inferred.\n\n    In this module, the `weight` and `bias` are of :class:`torch.nn.UninitializedParameter`\n    class. They will be initialized after the first call to ``forward`` is done and the\n    module will become a regular :class:`torch.nn.Linear` module. The ``in_features`` argument\n    of the :class:`Linear` is inferred from the ``input.shape[-1]``.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                :math:`k = \\frac{1}{\\text{in\\_features}}`\n\n\n    \"\"\"\n\n    cls_to_become = Linear  # type: ignore[assignment]\n    weight: UninitializedParameter\n    bias: UninitializedParameter  # type: ignore[assignment]\n\n    def __init__(\n        self, out_features: int, bias: bool = True, device=None, dtype=None\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        # bias is hardcoded to False to avoid creating tensor\n        # that will soon be overwritten.\n        super().__init__(0, 0, False)\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_features = out_features\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def reset_parameters(self) -> None:\n        if not self.has_uninitialized_params() and self.in_features != 0:\n            super().reset_parameters()\n\n    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n        if self.has_uninitialized_params():\n            with torch.no_grad():\n                self.in_features = input.shape[-1]\n                self.weight.materialize((self.out_features, self.in_features))\n                if self.bias is not None:\n                    self.bias.materialize((self.out_features,))\n                self.reset_parameters()\n\n\n# TODO: PartialLinear - maybe in sparse?\n", 293], "C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py": ["# mypy: allow-untyped-defs\nimport warnings\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.init import constant_, xavier_normal_, xavier_uniform_\nfrom torch.nn.parameter import Parameter\n\nfrom .linear import NonDynamicallyQuantizableLinear\nfrom .module import Module\n\n\n__all__ = [\n    \"Threshold\",\n    \"ReLU\",\n    \"RReLU\",\n    \"Hardtanh\",\n    \"ReLU6\",\n    \"Sigmoid\",\n    \"Hardsigmoid\",\n    \"Tanh\",\n    \"SiLU\",\n    \"Mish\",\n    \"Hardswish\",\n    \"ELU\",\n    \"CELU\",\n    \"SELU\",\n    \"GLU\",\n    \"GELU\",\n    \"Hardshrink\",\n    \"LeakyReLU\",\n    \"LogSigmoid\",\n    \"Softplus\",\n    \"Softshrink\",\n    \"MultiheadAttention\",\n    \"PReLU\",\n    \"Softsign\",\n    \"Tanhshrink\",\n    \"Softmin\",\n    \"Softmax\",\n    \"Softmax2d\",\n    \"LogSoftmax\",\n]\n\n\nclass Threshold(Module):\n    r\"\"\"Thresholds each element of the input Tensor.\n\n    Threshold is defined as:\n\n    .. math::\n        y =\n        \\begin{cases}\n        x, &\\text{ if } x > \\text{threshold} \\\\\n        \\text{value}, &\\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        threshold: The value to threshold at\n        value: The value to replace with\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Examples::\n\n        >>> m = nn.Threshold(0.1, 20)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"threshold\", \"value\", \"inplace\"]\n\n    threshold: float\n    value: float\n    inplace: bool\n\n    def __init__(self, threshold: float, value: float, inplace: bool = False) -> None:\n        super().__init__()\n        self.threshold = threshold\n        self.value = value\n        self.inplace = inplace\n        # TODO: check in THNN (if inplace == True, then assert value <= threshold)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.threshold(input, self.threshold, self.value, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"threshold={self.threshold}, value={self.value}{inplace_str}\"\n\n\nclass ReLU(Module):\n    r\"\"\"Applies the rectified linear unit function element-wise.\n\n    :math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU.png\n\n    Examples::\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n\n      An implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2).unsqueeze(0)\n        >>> output = torch.cat((m(input), m(-input)))\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.relu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass RReLU(Module):\n    r\"\"\"Applies the randomized leaky rectified linear unit function, element-wise.\n\n    Method described in the paper:\n    `Empirical Evaluation of Rectified Activations in Convolutional Network <https://arxiv.org/abs/1505.00853>`_.\n\n    The function is defined as:\n\n    .. math::\n        \\text{RReLU}(x) =\n        \\begin{cases}\n            x & \\text{if } x \\geq 0 \\\\\n            ax & \\text{ otherwise }\n        \\end{cases}\n\n    where :math:`a` is randomly sampled from uniform distribution\n    :math:`\\mathcal{U}(\\text{lower}, \\text{upper})` during training while during\n    evaluation :math:`a` is fixed with :math:`a = \\frac{\\text{lower} + \\text{upper}}{2}`.\n\n    Args:\n        lower: lower bound of the uniform distribution. Default: :math:`\\frac{1}{8}`\n        upper: upper bound of the uniform distribution. Default: :math:`\\frac{1}{3}`\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/RReLU.png\n\n    Examples::\n\n        >>> m = nn.RReLU(0.1, 0.3)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    \"\"\"\n\n    __constants__ = [\"lower\", \"upper\", \"inplace\"]\n\n    lower: float\n    upper: float\n    inplace: bool\n\n    def __init__(\n        self, lower: float = 1.0 / 8, upper: float = 1.0 / 3, inplace: bool = False\n    ):\n        super().__init__()\n        self.lower = lower\n        self.upper = upper\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.rrelu(input, self.lower, self.upper, self.training, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"lower={self.lower}, upper={self.upper}{inplace_str}\"\n\n\nclass Hardtanh(Module):\n    r\"\"\"Applies the HardTanh function element-wise.\n\n    HardTanh is defined as:\n\n    .. math::\n        \\text{HardTanh}(x) = \\begin{cases}\n            \\text{max\\_val} & \\text{ if } x > \\text{ max\\_val } \\\\\n            \\text{min\\_val} & \\text{ if } x < \\text{ min\\_val } \\\\\n            x & \\text{ otherwise } \\\\\n        \\end{cases}\n\n    Args:\n        min_val: minimum value of the linear region range. Default: -1\n        max_val: maximum value of the linear region range. Default: 1\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Keyword arguments :attr:`min_value` and :attr:`max_value`\n    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardtanh.png\n\n    Examples::\n\n        >>> m = nn.Hardtanh(-2, 2)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"min_val\", \"max_val\", \"inplace\"]\n\n    min_val: float\n    max_val: float\n    inplace: bool\n\n    def __init__(\n        self,\n        min_val: float = -1.0,\n        max_val: float = 1.0,\n        inplace: bool = False,\n        min_value: Optional[float] = None,\n        max_value: Optional[float] = None,\n    ) -> None:\n        super().__init__()\n        if min_value is not None:\n            warnings.warn(\n                \"keyword argument `min_value` is deprecated and rename to `min_val`\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            min_val = min_value\n        if max_value is not None:\n            warnings.warn(\n                \"keyword argument `max_value` is deprecated and rename to `max_val`\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            max_val = max_value\n\n        self.min_val = min_val\n        self.max_val = max_val\n        self.inplace = inplace\n        assert self.max_val > self.min_val\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardtanh(input, self.min_val, self.max_val, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"min_val={self.min_val}, max_val={self.max_val}{inplace_str}\"\n\n\nclass ReLU6(Hardtanh):\n    r\"\"\"Applies the ReLU6 function element-wise.\n\n    .. math::\n        \\text{ReLU6}(x) = \\min(\\max(0,x), 6)\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU6.png\n\n    Examples::\n\n        >>> m = nn.ReLU6()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def __init__(self, inplace: bool = False):\n        super().__init__(0.0, 6.0, inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Sigmoid(Module):\n    r\"\"\"Applies the Sigmoid function element-wise.\n\n    .. math::\n        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Sigmoid.png\n\n    Examples::\n\n        >>> m = nn.Sigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return torch.sigmoid(input)\n\n\nclass Hardsigmoid(Module):\n    r\"\"\"Applies the Hardsigmoid function element-wise.\n\n    Hardsigmoid is defined as:\n\n    .. math::\n        \\text{Hardsigmoid}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            1 & \\text{if~} x \\ge +3, \\\\\n            x / 6 + 1 / 2 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardsigmoid.png\n\n    Examples::\n\n        >>> m = nn.Hardsigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardsigmoid(input, self.inplace)\n\n\nclass Tanh(Module):\n    r\"\"\"Applies the Hyperbolic Tangent (Tanh) function element-wise.\n\n    Tanh is defined as:\n\n    .. math::\n        \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanh.png\n\n    Examples::\n\n        >>> m = nn.Tanh()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return torch.tanh(input)\n\n\nclass SiLU(Module):\n    r\"\"\"Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n    The SiLU function is also known as the swish function.\n\n    .. math::\n        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n    .. note::\n        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n        where the SiLU was experimented with later.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SiLU.png\n\n    Examples::\n\n        >>> m = nn.SiLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.silu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Mish(Module):\n    r\"\"\"Applies the Mish function, element-wise.\n\n    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    .. math::\n        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n    .. note::\n        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Mish.png\n\n    Examples::\n\n        >>> m = nn.Mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.mish(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass Hardswish(Module):\n    r\"\"\"Applies the Hardswish function, element-wise.\n\n    Method described in the paper: `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_.\n\n    Hardswish is defined as:\n\n    .. math::\n        \\text{Hardswish}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            x & \\text{if~} x \\ge +3, \\\\\n            x \\cdot (x + 3) /6 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardswish.png\n\n    Examples::\n\n        >>> m = nn.Hardswish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardswish(input, self.inplace)\n\n\nclass ELU(Module):\n    r\"\"\"Applies the Exponential Linear Unit (ELU) function, element-wise.\n\n    Method described in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear\n    Units (ELUs) <https://arxiv.org/abs/1511.07289>`__.\n\n    ELU is defined as:\n\n    .. math::\n        \\text{ELU}(x) = \\begin{cases}\n        x, & \\text{ if } x > 0\\\\\n        \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n        \\end{cases}\n\n    Args:\n        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ELU.png\n\n    Examples::\n\n        >>> m = nn.ELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"alpha\", \"inplace\"]\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1.0, inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.elu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"alpha={self.alpha}{inplace_str}\"\n\n\nclass CELU(Module):\n    r\"\"\"Applies the CELU function element-wise.\n\n    .. math::\n        \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\n\n    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .\n\n    Args:\n        alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/CELU.png\n\n    Examples::\n\n        >>> m = nn.CELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _`Continuously Differentiable Exponential Linear Units`:\n        https://arxiv.org/abs/1704.07483\n    \"\"\"\n\n    __constants__ = [\"alpha\", \"inplace\"]\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1.0, inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.celu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"alpha={self.alpha}{inplace_str}\"\n\n\nclass SELU(Module):\n    r\"\"\"Applies the SELU function element-wise.\n\n    .. math::\n        \\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\n\n    with :math:`\\alpha = 1.6732632423543772848170429916717` and\n    :math:`\\text{scale} = 1.0507009873554804934193349852946`.\n\n    .. warning::\n        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,\n        ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``\n        in order to get `Self-Normalizing Neural Networks`_.\n        See :func:`torch.nn.init.calculate_gain` for more information.\n\n    More details can be found in the paper `Self-Normalizing Neural Networks`_ .\n\n    Args:\n        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SELU.png\n\n    Examples::\n\n        >>> m = nn.SELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515\n    \"\"\"\n\n    __constants__ = [\"inplace\"]\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.selu(input, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \"inplace=True\" if self.inplace else \"\"\n        return inplace_str\n\n\nclass GLU(Module):\n    r\"\"\"Applies the gated linear unit function.\n\n    :math:`{GLU}(a, b)= a \\otimes \\sigma(b)` where :math:`a` is the first half\n    of the input matrices and :math:`b` is the second half.\n\n    Args:\n        dim (int): the dimension on which to split the input. Default: -1\n\n    Shape:\n        - Input: :math:`(\\ast_1, N, \\ast_2)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(\\ast_1, M, \\ast_2)` where :math:`M=N/2`\n\n    Examples::\n\n        >>> m = nn.GLU()\n        >>> input = torch.randn(4, 2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: int\n\n    def __init__(self, dim: int = -1) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.glu(input, self.dim)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}\"\n\n\nclass GELU(Module):\n    r\"\"\"Applies the Gaussian Error Linear Units function.\n\n    .. math:: \\text{GELU}(x) = x * \\Phi(x)\n\n    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\n    When the approximate argument is 'tanh', Gelu is estimated with:\n\n    .. math:: \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\n    Args:\n        approximate (str, optional): the gelu approximation algorithm to use:\n            ``'none'`` | ``'tanh'``. Default: ``'none'``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/GELU.png\n\n    Examples::\n\n        >>> m = nn.GELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"approximate\"]\n    approximate: str\n\n    def __init__(self, approximate: str = \"none\") -> None:\n        super().__init__()\n        self.approximate = approximate\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.gelu(input, approximate=self.approximate)\n\n    def extra_repr(self) -> str:\n        return f\"approximate={repr(self.approximate)}\"\n\n\nclass Hardshrink(Module):\n    r\"\"\"Applies the Hard Shrinkage (Hardshrink) function element-wise.\n\n    Hardshrink is defined as:\n\n    .. math::\n        \\text{HardShrink}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x > \\lambda \\\\\n        x, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` value for the Hardshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardshrink.png\n\n    Examples::\n\n        >>> m = nn.Hardshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"lambd\"]\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        return f\"{self.lambd}\"\n\n\nclass LeakyReLU(Module):\n    r\"\"\"Applies the LeakyReLU function element-wise.\n\n    .. math::\n        \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)\n\n\n    or\n\n    .. math::\n        \\text{LeakyReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\geq 0 \\\\\n        \\text{negative\\_slope} \\times x, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        negative_slope: Controls the angle of the negative slope (which is used for\n          negative input values). Default: 1e-2\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    .. image:: ../scripts/activation_images/LeakyReLU.png\n\n    Examples::\n\n        >>> m = nn.LeakyReLU(0.1)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"inplace\", \"negative_slope\"]\n    inplace: bool\n    negative_slope: float\n\n    def __init__(self, negative_slope: float = 1e-2, inplace: bool = False) -> None:\n        super().__init__()\n        self.negative_slope = negative_slope\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.leaky_relu(input, self.negative_slope, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = \", inplace=True\" if self.inplace else \"\"\n        return f\"negative_slope={self.negative_slope}{inplace_str}\"\n\n\nclass LogSigmoid(Module):\n    r\"\"\"Applies the Logsigmoid function element-wise.\n\n    .. math::\n        \\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/LogSigmoid.png\n\n    Examples::\n\n        >>> m = nn.LogSigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.logsigmoid(input)\n\n\nclass Softplus(Module):\n    r\"\"\"Applies the Softplus function element-wise.\n\n    .. math::\n        \\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))\n\n    SoftPlus is a smooth approximation to the ReLU function and can be used\n    to constrain the output of a machine to always be positive.\n\n    For numerical stability the implementation reverts to the linear function\n    when :math:`input \\times \\beta > threshold`.\n\n    Args:\n        beta: the :math:`\\beta` value for the Softplus formulation. Default: 1\n        threshold: values above this revert to a linear function. Default: 20\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softplus.png\n\n    Examples::\n\n        >>> m = nn.Softplus()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"beta\", \"threshold\"]\n    beta: float\n    threshold: float\n\n    def __init__(self, beta: float = 1.0, threshold: float = 20.0) -> None:\n        super().__init__()\n        self.beta = beta\n        self.threshold = threshold\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softplus(input, self.beta, self.threshold)\n\n    def extra_repr(self) -> str:\n        return f\"beta={self.beta}, threshold={self.threshold}\"\n\n\nclass Softshrink(Module):\n    r\"\"\"Applies the soft shrinkage function element-wise.\n\n    .. math::\n        \\text{SoftShrinkage}(x) =\n        \\begin{cases}\n        x - \\lambda, & \\text{ if } x > \\lambda \\\\\n        x + \\lambda, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softshrink.png\n\n    Examples::\n\n        >>> m = nn.Softshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"lambd\"]\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        return str(self.lambd)\n\n\ndef _check_arg_device(x: Optional[torch.Tensor]) -> bool:\n    if x is not None:\n        return x.device.type in [\n            \"cpu\",\n            \"cuda\",\n            torch.utils.backend_registration._privateuse1_backend_name,\n        ]\n    return True\n\n\ndef _arg_requires_grad(x: Optional[torch.Tensor]) -> bool:\n    if x is not None:\n        return x.requires_grad\n    return False\n\n\ndef _is_make_fx_tracing():\n    if not torch.jit.is_scripting():\n        torch_dispatch_mode_stack = (\n            torch.utils._python_dispatch._get_current_dispatch_mode_stack()\n        )\n        return any(\n            type(x) == torch.fx.experimental.proxy_tensor.ProxyTorchDispatchMode\n            for x in torch_dispatch_mode_stack\n        )\n    else:\n        return False\n\n\nclass MultiheadAttention(Module):\n    r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\n\n    .. note::\n        See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n        for an in depth discussion of the performant building blocks PyTorch offers for building your own\n        transformer layers.\n\n    Method described in the paper:\n    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n\n    Multi-Head Attention is defined as:\n\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n\n    where :math:`\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n\n    ``nn.MultiheadAttention`` will use the optimized implementations of\n    ``scaled_dot_product_attention()`` when possible.\n\n    In addition to support for the new ``scaled_dot_product_attention()``\n    function, for speeding up Inference, MHA will use\n    fastpath inference with support for Nested Tensors, iff:\n\n    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n    - inputs are batched (3D) with ``batch_first==True``\n    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n    - training is disabled (using ``.eval()``)\n    - ``add_bias_kv`` is ``False``\n    - ``add_zero_attn`` is ``False``\n    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n      nor ``attn_mask`` is passed\n    - autocast is disabled\n\n    If the optimized inference fastpath implementation is in use, a\n    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n    ``query``/``key``/``value`` to represent padding more efficiently than using a\n    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n    will be returned, and an additional speedup proportional to the fraction of the input\n    that is padding can be expected.\n\n    Args:\n        embed_dim: Total dimension of the model.\n        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n            Default: ``False``.\n        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n\n    Examples::\n\n        >>> # xdoctest: +SKIP\n        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n         https://arxiv.org/abs/2205.14135\n\n    \"\"\"\n\n    __constants__ = [\"batch_first\"]\n    bias_k: Optional[torch.Tensor]\n    bias_v: Optional[torch.Tensor]\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        kdim=None,\n        vdim=None,\n        batch_first=False,\n        device=None,\n        dtype=None,\n    ) -> None:\n        if embed_dim <= 0 or num_heads <= 0:\n            raise ValueError(\n                f\"embed_dim and num_heads must be greater than 0,\"\n                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n            )\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.batch_first = batch_first\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n\n        if not self._qkv_same_embed_dim:\n            self.q_proj_weight = Parameter(\n                torch.empty((embed_dim, embed_dim), **factory_kwargs)\n            )\n            self.k_proj_weight = Parameter(\n                torch.empty((embed_dim, self.kdim), **factory_kwargs)\n            )\n            self.v_proj_weight = Parameter(\n                torch.empty((embed_dim, self.vdim), **factory_kwargs)\n            )\n            self.register_parameter(\"in_proj_weight\", None)\n        else:\n            self.in_proj_weight = Parameter(\n                torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)\n            )\n            self.register_parameter(\"q_proj_weight\", None)\n            self.register_parameter(\"k_proj_weight\", None)\n            self.register_parameter(\"v_proj_weight\", None)\n\n        if bias:\n            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n        else:\n            self.register_parameter(\"in_proj_bias\", None)\n        self.out_proj = NonDynamicallyQuantizableLinear(\n            embed_dim, embed_dim, bias=bias, **factory_kwargs\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self._qkv_same_embed_dim:\n            xavier_uniform_(self.in_proj_weight)\n        else:\n            xavier_uniform_(self.q_proj_weight)\n            xavier_uniform_(self.k_proj_weight)\n            xavier_uniform_(self.v_proj_weight)\n\n        if self.in_proj_bias is not None:\n            constant_(self.in_proj_bias, 0.0)\n            constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n        if \"_qkv_same_embed_dim\" not in state:\n            state[\"_qkv_same_embed_dim\"] = True\n\n        super().__setstate__(state)\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        key_padding_mask: Optional[Tensor] = None,\n        need_weights: bool = True,\n        attn_mask: Optional[Tensor] = None,\n        average_attn_weights: bool = True,\n        is_causal: bool = False,\n    ) -> tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"Compute attention outputs using query, key, and value embeddings.\n\n            Supports optional parameters for padding, masks and attention weights.\n\n        Args:\n            query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n                or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n                :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n                Queries are compared against key-value pairs to produce the output.\n                See \"Attention Is All You Need\" for more details.\n            key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n                or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n                :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n                See \"Attention Is All You Need\" for more details.\n            value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n                ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n                sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n                See \"Attention Is All You Need\" for more details.\n            key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n                to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n                Binary and float masks are supported.\n                For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n                the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n            need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n                Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n                and achieve the best performance for MHA.\n                Default: ``True``.\n            attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n                :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n                :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n                broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n                Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n                corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n                the attention weight.\n                If both attn_mask and key_padding_mask are supplied, their types should match.\n            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n                heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n                effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n            is_causal: If specified, applies a causal mask as attention mask.\n                Default: ``False``.\n                Warning:\n                ``is_causal`` provides a hint that ``attn_mask`` is the\n                causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n\n        Outputs:\n            - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n              :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n              where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n              embedding dimension ``embed_dim``.\n            - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n              returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n              head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n\n            .. note::\n                `batch_first` argument is ignored for unbatched inputs.\n        \"\"\"  # noqa: B950\n        why_not_fast_path = \"\"\n        if (\n            (attn_mask is not None and torch.is_floating_point(attn_mask))\n            or (key_padding_mask is not None)\n            and torch.is_floating_point(key_padding_mask)\n        ):\n            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n\n        is_batched = query.dim() == 3\n\n        key_padding_mask = F._canonical_mask(\n            mask=key_padding_mask,\n            mask_name=\"key_padding_mask\",\n            other_type=F._none_or_dtype(attn_mask),\n            other_name=\"attn_mask\",\n            target_type=query.dtype,\n        )\n\n        attn_mask = F._canonical_mask(\n            mask=attn_mask,\n            mask_name=\"attn_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=query.dtype,\n            check_other=False,\n        )\n\n        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n\n        if not is_fastpath_enabled:\n            why_not_fast_path = \"torch.backends.mha.get_fastpath_enabled() was not True\"\n        elif not is_batched:\n            why_not_fast_path = (\n                f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n            )\n        elif query is not key or key is not value:\n            # When lifting this restriction, don't forget to either\n            # enforce that the dtypes all match or test cases where\n            # they don't!\n            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n        elif self.in_proj_weight is None:\n            why_not_fast_path = \"in_proj_weight was None\"\n        elif query.dtype != self.in_proj_weight.dtype:\n            # this case will fail anyway, but at least they'll get a useful error message.\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n        elif self.training:\n            why_not_fast_path = \"training is enabled\"\n        elif (self.num_heads % 2) != 0:\n            why_not_fast_path = \"self.num_heads is not even\"\n        elif not self.batch_first:\n            why_not_fast_path = \"batch_first was not True\"\n        elif self.bias_k is not None:\n            why_not_fast_path = \"self.bias_k was not None\"\n        elif self.bias_v is not None:\n            why_not_fast_path = \"self.bias_v was not None\"\n        elif self.add_zero_attn:\n            why_not_fast_path = \"add_zero_attn was enabled\"\n        elif not self._qkv_same_embed_dim:\n            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n        elif query.is_nested and (\n            key_padding_mask is not None or attn_mask is not None\n        ):\n            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n                                 is not supported with NestedTensor input\"\n        elif torch.is_autocast_enabled():\n            why_not_fast_path = \"autocast is enabled\"\n\n        if not why_not_fast_path:\n            tensor_args = (\n                query,\n                key,\n                value,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.out_proj.weight,\n                self.out_proj.bias,\n            )\n            # We have to use list comprehensions below because TorchScript does not support\n            # generator expressions.\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_fast_path = \"some Tensor argument has_torch_function\"\n            elif _is_make_fx_tracing():\n                why_not_fast_path = \"we are running make_fx tracing\"\n            elif not all(_check_arg_device(x) for x in tensor_args):\n                why_not_fast_path = (\n                    \"some Tensor argument's device is neither one of \"\n                    f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\"\n                )\n            elif torch.is_grad_enabled() and any(\n                _arg_requires_grad(x) for x in tensor_args\n            ):\n                why_not_fast_path = (\n                    \"grad is enabled and at least one of query or the \"\n                    \"input/output projection weights or biases requires_grad\"\n                )\n            if not why_not_fast_path:\n                merged_mask, mask_type = self.merge_masks(\n                    attn_mask, key_padding_mask, query\n                )\n\n                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n                    return torch._native_multi_head_attention(\n                        query,\n                        key,\n                        value,\n                        self.embed_dim,\n                        self.num_heads,\n                        self.in_proj_weight,\n                        self.in_proj_bias,\n                        self.out_proj.weight,\n                        self.out_proj.bias,\n                        merged_mask,\n                        need_weights,\n                        average_attn_weights,\n                        mask_type,\n                    )\n\n        any_nested = query.is_nested or key.is_nested or value.is_nested\n        assert not any_nested, (\n            \"MultiheadAttention does not support NestedTensor outside of its fast path. \"\n            + f\"The fast path was not hit because {why_not_fast_path}\"\n        )\n\n        if self.batch_first and is_batched:\n            # make sure that the transpose op does not affect the \"is\" property\n            if key is value:\n                if query is key:\n                    query = key = value = query.transpose(1, 0)\n                else:\n                    query, key = (x.transpose(1, 0) for x in (query, key))\n                    value = key\n            else:\n                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n\n        if not self._qkv_same_embed_dim:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask,\n                need_weights=need_weights,\n                attn_mask=attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj_weight,\n                k_proj_weight=self.k_proj_weight,\n                v_proj_weight=self.v_proj_weight,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal,\n            )\n        else:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask,\n                need_weights=need_weights,\n                attn_mask=attn_mask,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal,\n            )\n        if self.batch_first and is_batched:\n            return attn_output.transpose(1, 0), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def merge_masks(\n        self,\n        attn_mask: Optional[Tensor],\n        key_padding_mask: Optional[Tensor],\n        query: Tensor,\n    ) -> tuple[Optional[Tensor], Optional[int]]:\n        r\"\"\"Determine mask type and combine masks if necessary.\n\n        If only one mask is provided, that mask\n        and the corresponding mask type will be returned. If both masks are provided, they will be both\n        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\n        and mask type 2 will be returned\n        Args:\n            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\n            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\n            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\n        Returns:\n            merged_mask: merged mask\n            mask_type: merged mask type (0, 1, or 2)\n        \"\"\"\n        mask_type: Optional[int] = None\n        merged_mask: Optional[Tensor] = None\n\n        if key_padding_mask is not None:\n            mask_type = 1\n            merged_mask = key_padding_mask\n\n        if attn_mask is not None:\n            # In this branch query can't be a nested tensor, so it has a shape\n            batch_size, seq_len, _ = query.shape\n            mask_type = 2\n\n            # Always expands attn_mask to 4D\n            if attn_mask.dim() == 3:\n                attn_mask_expanded = attn_mask.view(batch_size, -1, seq_len, seq_len)\n            else:  # attn_mask.dim() == 2:\n                attn_mask_expanded = attn_mask.view(1, 1, seq_len, seq_len).expand(\n                    batch_size, self.num_heads, -1, -1\n                )\n            merged_mask = attn_mask_expanded\n\n            if key_padding_mask is not None:\n                key_padding_mask_expanded = key_padding_mask.view(\n                    batch_size, 1, 1, seq_len\n                ).expand(-1, self.num_heads, -1, -1)\n                merged_mask = attn_mask_expanded + key_padding_mask_expanded\n\n        # no attn_mask and no key_padding_mask, returns None, None\n        return merged_mask, mask_type\n\n\nclass PReLU(Module):\n    r\"\"\"Applies the element-wise PReLU function.\n\n    .. math::\n        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\n    or\n\n    .. math::\n        \\text{PReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\ge 0 \\\\\n        ax, & \\text{ otherwise }\n        \\end{cases}\n\n    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\n    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\n    a separate :math:`a` is used for each input channel.\n\n\n    .. note::\n        weight decay should not be used when learning :math:`a` for good performance.\n\n    .. note::\n        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n        no channel dim and the number of channels = 1.\n\n    Args:\n        num_parameters (int): number of :math:`a` to learn.\n            Although it takes an int as input, there is only two values are legitimate:\n            1, or the number of channels at input. Default: 1\n        init (float): the initial value of :math:`a`. Default: 0.25\n\n    Shape:\n        - Input: :math:`( *)` where `*` means, any number of additional\n          dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Attributes:\n        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n\n    .. image:: ../scripts/activation_images/PReLU.png\n\n    Examples::\n\n        >>> m = nn.PReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"num_parameters\"]\n    num_parameters: int\n\n    def __init__(\n        self, num_parameters: int = 1, init: float = 0.25, device=None, dtype=None\n    ) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        self.num_parameters = num_parameters\n        super().__init__()\n        self.init = init\n        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.constant_(self.weight, self.init)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.prelu(input, self.weight)\n\n    def extra_repr(self) -> str:\n        return f\"num_parameters={self.num_parameters}\"\n\n\nclass Softsign(Module):\n    r\"\"\"Applies the element-wise Softsign function.\n\n    .. math::\n        \\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softsign.png\n\n    Examples::\n\n        >>> m = nn.Softsign()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softsign(input)\n\n\nclass Tanhshrink(Module):\n    r\"\"\"Applies the element-wise Tanhshrink function.\n\n    .. math::\n        \\text{Tanhshrink}(x) = x - \\tanh(x)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanhshrink.png\n\n    Examples::\n\n        >>> m = nn.Tanhshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.tanhshrink(input)\n\n\nclass Softmin(Module):\n    r\"\"\"Applies the Softmin function to an n-dimensional input Tensor.\n\n    Rescales them so that the elements of the n-dimensional output Tensor\n    lie in the range `[0, 1]` and sum to 1.\n\n    Softmin is defined as:\n\n    .. math::\n        \\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which Softmin will be computed (so every slice\n            along dim will sum to 1).\n\n    Returns:\n        a Tensor of the same dimension and shape as the input, with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmin(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softmin(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self):\n        return f\"dim={self.dim}\"\n\n\nclass Softmax(Module):\n    r\"\"\"Applies the Softmax function to an n-dimensional input Tensor.\n\n    Rescales them so that the elements of the n-dimensional output Tensor\n    lie in the range [0,1] and sum to 1.\n\n    Softmax is defined as:\n\n    .. math::\n        \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\n    When the input Tensor is a sparse tensor then the unspecified\n    values are treated as ``-inf``.\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Args:\n        dim (int): A dimension along which Softmax will be computed (so every slice\n            along dim will sum to 1).\n\n    .. note::\n        This module doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use `LogSoftmax` instead (it's faster and has better numerical properties).\n\n    Examples::\n\n        >>> m = nn.Softmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}\"\n\n\nclass Softmax2d(Module):\n    r\"\"\"Applies SoftMax over features to each spatial location.\n\n    When given an image of ``Channels x Height x Width``, it will\n    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`\n\n    Shape:\n        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.\n        - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmax2d()\n        >>> # you softmax over the 2nd dimension\n        >>> input = torch.randn(2, 3, 12, 13)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        if input.dim() not in (3, 4):\n            raise ValueError(\n                f\"Softmax2d: expected input to be 3D or 4D, got {input.dim()}D instead\"\n            )\n        return F.softmax(input, -3, _stacklevel=5)\n\n\nclass LogSoftmax(Module):\n    r\"\"\"Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional input Tensor.\n\n    The LogSoftmax formulation can be simplified as:\n\n    .. math::\n        \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which LogSoftmax will be computed.\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [-inf, 0)\n\n    Examples::\n\n        >>> m = nn.LogSoftmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = [\"dim\"]\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, \"dim\"):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.log_softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self):\n        return f\"dim={self.dim}\"\n", 1751], "C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\functional.py": ["\"\"\"Functional interface.\"\"\"\n\nimport importlib\nimport math\nimport warnings\nfrom typing import Callable, Optional, TYPE_CHECKING, Union\n\nimport torch\nfrom torch import _VF, sym_int as _sym_int, Tensor\nfrom torch._C import _add_docstr, _infer_size\nfrom torch._jit_internal import (\n    _overload,\n    boolean_dispatch,\n    BroadcastingList1,\n    BroadcastingList2,\n    BroadcastingList3,\n)\nfrom torch._torch_docs import reproducibility_notes, sparse_support_notes, tf32_notes\nfrom torch.nn import _reduction as _Reduction, grad  # noqa: F401\nfrom torch.nn.modules.utils import _list_with_default, _pair, _single, _triple\nfrom torch.overrides import (\n    handle_torch_function,\n    has_torch_function,\n    has_torch_function_unary,\n    has_torch_function_variadic,\n)\n\n\nif TYPE_CHECKING:\n    from torch.types import _dtype as DType\nelse:\n    # The JIT doesn't understand Union, nor torch.dtype here\n    DType = int\n\ntry:\n    import numpy as np\nexcept ModuleNotFoundError:\n    np = None\n\n\nconv1d = _add_docstr(\n    torch.conv1d,\n    r\"\"\"\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a one-element tuple `(padW,)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(33, 16, 30)\n    >>> filters = torch.randn(20, 16, 5)\n    >>> F.conv1d(inputs, filters)\n\"\"\",\n)\n\nconv2d = _add_docstr(\n    torch.conv2d,\n    r\"\"\"\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, both :math:`\\text{in\\_channels}` and :math:`\\text{out\\_channels}`\n      should be divisible by the number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8, 4, 3, 3)\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> F.conv2d(inputs, filters, padding=1)\n\"\"\",\n)  # noqa: E501\n\nconv3d = _add_docstr(\n    torch.conv3d,\n    r\"\"\"\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n\"\"\",\n)  # noqa: E501\n\nconv_transpose1d = _add_docstr(\n    torch.conv_transpose1d,\n    r\"\"\"\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called \"deconvolution\".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n\"\"\",\n)\n\nconv_transpose2d = _add_docstr(\n    torch.conv_transpose2d,\n    r\"\"\"\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called \"deconvolution\".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n\"\"\",\n)  # noqa: E501\n\nconv_transpose3d = _add_docstr(\n    torch.conv_transpose3d,\n    r\"\"\"\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called \"deconvolution\"\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n\"\"\",\n)  # noqa: E501\n\nconv_tbc = _add_docstr(\n    torch.conv_tbc,\n    r\"\"\"\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n\"\"\",\n)\n\n\n# Pooling\navg_pool1d = _add_docstr(\n    torch.avg_pool1d,\n    r\"\"\"\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n\"\"\",\n)\n\n\navg_pool2d = _add_docstr(\n    torch._C._nn.avg_pool2d,\n    r\"\"\"\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\n\"\"\",\n)\n\navg_pool3d = _add_docstr(\n    torch._C._nn.avg_pool3d,\n    r\"\"\"\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kT, kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise\n        size of the pooling region will be used. Default: None\n\"\"\",\n)\n\n\ndef fractional_max_pool2d_with_indices(\n    input: Tensor,\n    kernel_size: BroadcastingList2[int],\n    output_size: Optional[BroadcastingList2[int]] = None,\n    output_ratio: Optional[BroadcastingList2[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n\n    Applies 2D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k`)\n                     or a tuple `(kH, kW)`\n        output_size: the target output size of the image of the form :math:`oH \\times oW`.\n                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \\times oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.\n\n    Examples::\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> # pool of square window of size=3, and target output size 13x12\n        >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n        >>> # pool of square window and target output size being half of input image size\n        >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n\n    .. _Fractional MaxPooling:\n        http://arxiv.org/abs/1412.6071\n    \"\"\"\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool2d_with_indices,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    if output_size is None and output_ratio is None:\n        raise ValueError(\n            \"fractional_max_pool2d requires specifying either an output_size or an output_ratio\"\n        )\n    if output_size is None:\n        assert output_ratio is not None\n        if len(output_ratio) > 2:\n            raise ValueError(\n                \"fractional_max_pool2d requires output_ratio to either be a single Int or tuple of Ints.\"\n            )\n        _output_ratio = _pair(output_ratio)\n        output_size = [\n            int(input.size(-2) * _output_ratio[0]),\n            int(input.size(-1) * _output_ratio[1]),\n        ]\n\n    if _random_samples is None:\n        n_batch = 1 if input.dim() == 3 else input.size(0)\n        _random_samples = torch.rand(\n            n_batch, input.size(-3), 2, dtype=input.dtype, device=input.device\n        )\n    return torch._C._nn.fractional_max_pool2d(\n        input, kernel_size, output_size, _random_samples\n    )\n\n\ndef _fractional_max_pool2d(\n    input: Tensor,\n    kernel_size: BroadcastingList2[int],\n    output_size: Optional[BroadcastingList2[int]] = None,\n    output_ratio: Optional[BroadcastingList2[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None,\n) -> Tensor:\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool2d,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    return fractional_max_pool2d_with_indices(\n        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n    )[0]\n\n\nfractional_max_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=4,\n    default=False,\n    if_true=fractional_max_pool2d_with_indices,\n    if_false=_fractional_max_pool2d,\n    module_name=__name__,\n    func_name=\"fractional_max_pool2d\",\n)\n\n\ndef fractional_max_pool3d_with_indices(\n    input: Tensor,\n    kernel_size: BroadcastingList3[int],\n    output_size: Optional[BroadcastingList3[int]] = None,\n    output_ratio: Optional[BroadcastingList3[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n\n    Applies 3D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k \\times k`)\n                     or a tuple `(kT, kH, kW)`\n        output_size: the target output size of the form :math:`oT \\times oH \\times oW`.\n                     Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output\n                     :math:`oH \\times oH \\times oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.\n\n    Shape:\n        - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n\n    Examples::\n        >>> input = torch.randn(20, 16, 50, 32, 16)\n        >>> # pool of cubic window of size=3, and target output size 13x12x11\n        >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n        >>> # pool of cubic window and target output size being half of input size\n        >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n\n    .. _Fractional MaxPooling:\n        http://arxiv.org/abs/1412.6071\n    \"\"\"\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool3d_with_indices,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    if output_size is None and output_ratio is None:\n        raise ValueError(\n            \"fractional_max_pool3d requires specifying either an output_size or an output_ratio\"\n        )\n    if output_size is None:\n        assert output_ratio is not None\n        _output_ratio = _triple(output_ratio)\n        output_size = [\n            int(input.size(-3) * _output_ratio[0]),\n            int(input.size(-2) * _output_ratio[1]),\n            int(input.size(-1) * _output_ratio[2]),\n        ]\n\n    if _random_samples is None:\n        n_batch = 1 if input.dim() == 4 else input.size(0)\n        _random_samples = torch.rand(\n            n_batch, input.size(-4), 3, dtype=input.dtype, device=input.device\n        )\n    return torch._C._nn.fractional_max_pool3d(\n        input, kernel_size, output_size, _random_samples\n    )\n\n\ndef _fractional_max_pool3d(\n    input: Tensor,\n    kernel_size: BroadcastingList3[int],\n    output_size: Optional[BroadcastingList3[int]] = None,\n    output_ratio: Optional[BroadcastingList3[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None,\n) -> Tensor:\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool3d,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    return fractional_max_pool3d_with_indices(\n        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n    )[0]\n\n\nfractional_max_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=4,\n    default=False,\n    if_true=fractional_max_pool3d_with_indices,\n    if_false=_fractional_max_pool3d,\n    module_name=__name__,\n    func_name=\"fractional_max_pool3d\",\n)\n\n\ndef max_pool1d_with_indices(\n    input: Tensor,\n    kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    dilation: BroadcastingList1[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 1D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool1d` for details.\n\n    Args:\n        input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`, minibatch dim optional.\n        kernel_size: the size of the window. Can be a single number or a\n            tuple `(kW,)`\n        stride: the stride of the window. Can be a single number or a tuple\n            `(sW,)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool1d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool1d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch.max_pool1d_with_indices(\n        input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n\n\ndef _max_pool1d(\n    input: Tensor,\n    kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    dilation: BroadcastingList1[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool1d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool1d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool1d_with_indices,\n    if_false=_max_pool1d,\n    module_name=__name__,\n    func_name=\"max_pool1d\",\n)\n\n\ndef max_pool2d_with_indices(\n    input: Tensor,\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    dilation: BroadcastingList2[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 2D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool2d` for details.\n\n    Args:\n        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n        kernel_size: size of the pooling region. Can be a single number or a\n            tuple `(kH, kW)`\n        stride: stride of the pooling operation. Can be a single number or a\n            tuple `(sH, sW)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool2d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool2d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch._C._nn.max_pool2d_with_indices(\n        input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n\n\ndef _max_pool2d(\n    input: Tensor,\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    dilation: BroadcastingList2[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool2d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool2d_with_indices,\n    if_false=_max_pool2d,\n    module_name=__name__,\n    func_name=\"max_pool2d\",\n)\n\n\ndef max_pool3d_with_indices(\n    input: Tensor,\n    kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    dilation: BroadcastingList3[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 3D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool3d` for details.\n\n    Args:\n        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)`, minibatch dim optional.\n        kernel_size: size of the pooling region. Can be a single number or a\n                     tuple `(kT, kH, kW)`\n        stride: stride of the pooling operation. Can be a single number or a\n                tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool3d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool3d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch._C._nn.max_pool3d_with_indices(\n        input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n\n\ndef _max_pool3d(\n    input: Tensor,\n    kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    dilation: BroadcastingList3[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool3d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(list[int], [])\n    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool3d_with_indices,\n    if_false=_max_pool3d,\n    module_name=__name__,\n    func_name=\"max_pool3d\",\n)\n\n\ndef _unpool_output_size(\n    input: Tensor,\n    kernel_size: list[int],\n    stride: list[int],\n    padding: list[int],\n    output_size: Optional[list[int]],\n) -> list[int]:\n    input_size = input.size()\n    default_size = torch.jit.annotate(list[int], [])\n    for d in range(len(kernel_size)):\n        default_size.append(\n            (input_size[-len(kernel_size) + d] - 1) * stride[d]\n            + kernel_size[d]\n            - 2 * padding[d]\n        )\n    if output_size is None:\n        ret = default_size\n    else:\n        if len(output_size) == len(kernel_size) + 2:\n            output_size = output_size[2:]\n        if len(output_size) != len(kernel_size):\n            raise ValueError(\n                \"output_size should be a sequence containing \"\n                f\"{len(kernel_size)} or {len(kernel_size) + 2} elements, but it has a length of '{len(output_size)}'\"\n            )\n        for d in range(len(kernel_size)):\n            min_size = default_size[d] - stride[d]\n            max_size = default_size[d] + stride[d]\n            if not (min_size < output_size[d] < max_size):\n                raise ValueError(\n                    f'invalid output_size \"{output_size}\" (dim {d} must be between {min_size} and {max_size})'\n                )\n\n        ret = output_size\n    return ret\n\n\ndef max_unpool1d(\n    input: Tensor,\n    indices: Tensor,\n    kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    output_size: Optional[BroadcastingList1[int]] = None,\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool1d`.\n\n    See :class:`~torch.nn.MaxUnpool1d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool1d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _single(kernel_size)\n    if stride is not None:\n        _stride = _single(stride)\n    else:\n        _stride = kernel_size\n    padding = _single(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    if isinstance(output_size, list):\n        output_size = output_size + [1]\n    else:\n        output_size = output_size + (1,)\n    return torch._C._nn.max_unpool2d(\n        input.unsqueeze(-1), indices.unsqueeze(-1), output_size\n    ).squeeze(-1)\n\n\ndef max_unpool2d(\n    input: Tensor,\n    indices: Tensor,\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    output_size: Optional[BroadcastingList2[int]] = None,\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool2d`.\n\n    See :class:`~torch.nn.MaxUnpool2d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool2d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _pair(kernel_size)\n    if stride is not None:\n        _stride = _pair(stride)\n    else:\n        _stride = kernel_size\n    padding = _pair(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    return torch._C._nn.max_unpool2d(input, indices, output_size)\n\n\ndef max_unpool3d(\n    input: Tensor,\n    indices: Tensor,\n    kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    output_size: Optional[BroadcastingList3[int]] = None,\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool3d`.\n\n    See :class:`~torch.nn.MaxUnpool3d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool3d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _triple(kernel_size)\n    if stride is not None:\n        _stride = _triple(stride)\n    else:\n        _stride = kernel_size\n    padding = _triple(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)\n\n\ndef lp_pool3d(\n    input: Tensor,\n    norm_type: Union[int, float],\n    kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    ceil_mode: bool = False,\n) -> Tensor:\n    r\"\"\"\n    Apply a 3D power-average pooling over an input signal composed of several input planes.\n\n    If the sum of all inputs to the power of `p` is\n    zero, the gradient is set to zero as well.\n\n    See :class:`~torch.nn.LPPool3d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            lp_pool3d,\n            (input,),\n            input,\n            norm_type,\n            kernel_size,\n            stride=stride,\n            ceil_mode=ceil_mode,\n        )\n    kd, kw, kh = _triple(kernel_size)\n    if stride is not None:\n        out = avg_pool3d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n    else:\n        out = avg_pool3d(\n            input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode\n        )\n\n    return (\n        (torch.sign(out) * relu(torch.abs(out))).mul(kd * kw * kh).pow(1.0 / norm_type)\n    )\n\n\ndef lp_pool2d(\n    input: Tensor,\n    norm_type: Union[int, float],\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    ceil_mode: bool = False,\n) -> Tensor:\n    r\"\"\"\n    Apply a 2D power-average pooling over an input signal composed of several input planes.\n\n    If the sum of all inputs to the power of `p` is\n    zero, the gradient is set to zero as well.\n\n    See :class:`~torch.nn.LPPool2d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            lp_pool2d,\n            (input,),\n            input,\n            norm_type,\n            kernel_size,\n            stride=stride,\n            ceil_mode=ceil_mode,\n        )\n    kw, kh = _pair(kernel_size)\n    if stride is not None:\n        out = avg_pool2d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n    else:\n        out = avg_pool2d(\n            input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode\n        )\n\n    return (torch.sign(out) * relu(torch.abs(out))).mul(kw * kh).pow(1.0 / norm_type)\n\n\ndef lp_pool1d(\n    input: Tensor,\n    norm_type: Union[int, float],\n    kernel_size: int,\n    stride: Optional[BroadcastingList1[int]] = None,\n    ceil_mode: bool = False,\n) -> Tensor:\n    r\"\"\"Apply a 1D power-average pooling over an input signal composed of several input planes.\n\n    If the sum of all inputs to the power of `p` is\n    zero, the gradient is set to zero as well.\n\n    See :class:`~torch.nn.LPPool1d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            lp_pool1d,\n            (input,),\n            input,\n            norm_type,\n            kernel_size,\n            stride=stride,\n            ceil_mode=ceil_mode,\n        )\n    if stride is not None:\n        out = avg_pool1d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n    else:\n        out = avg_pool1d(\n            input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode\n        )\n\n    return (\n        (torch.sign(out) * relu(torch.abs(out))).mul(kernel_size).pow(1.0 / norm_type)\n    )\n\n\ndef adaptive_max_pool1d_with_indices(\n    input: Tensor,\n    output_size: BroadcastingList1[int],\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    adaptive_max_pool1d(input, output_size, return_indices=False)\n\n    Applies a 1D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool1d_with_indices,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    return torch.adaptive_max_pool1d(input, output_size)\n\n\ndef _adaptive_max_pool1d(\n    input: Tensor,\n    output_size: BroadcastingList1[int],\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool1d,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    return adaptive_max_pool1d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool1d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool1d_with_indices,\n    if_false=_adaptive_max_pool1d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool1d\",\n)\n\n\ndef adaptive_max_pool2d_with_indices(\n    input: Tensor,\n    output_size: BroadcastingList2[int],\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"adaptive_max_pool2d(input, output_size, return_indices=False)\n\n    Applies a 2D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            double-integer tuple)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool2d_with_indices,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_max_pool2d(input, output_size)\n\n\ndef _adaptive_max_pool2d(\n    input: Tensor,\n    output_size: BroadcastingList2[int],\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool2d,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    return adaptive_max_pool2d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool2d_with_indices,\n    if_false=_adaptive_max_pool2d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool2d\",\n)\n\n\ndef adaptive_max_pool3d_with_indices(\n    input: Tensor,\n    output_size: BroadcastingList3[int],\n    return_indices: bool = False,\n) -> tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    adaptive_max_pool3d(input, output_size, return_indices=False)\n\n    Applies a 3D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            triple-integer tuple)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool3d_with_indices,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_max_pool3d(input, output_size)\n\n\ndef _adaptive_max_pool3d(\n    input: Tensor,\n    output_size: BroadcastingList3[int],\n    return_indices: bool = False,\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool3d,\n            (input,),\n            input,\n            output_size,\n            return_indices=return_indices,\n        )\n    return adaptive_max_pool3d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool3d_with_indices,\n    if_false=_adaptive_max_pool3d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool3d\",\n)\n\n\nadaptive_avg_pool1d = _add_docstr(\n    torch.adaptive_avg_pool1d,\n    r\"\"\"\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n\"\"\",\n)\n\n\ndef adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor:\n    r\"\"\"Apply a 2D adaptive average pooling over an input signal composed of several input planes.\n\n    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            double-integer tuple)\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)\n    _output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)\n\n\ndef adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:\n    r\"\"\"Apply a 3D adaptive average pooling over an input signal composed of several input planes.\n\n    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            triple-integer tuple)\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)\n    _output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)\n\n\n# Activation functions\ndef dropout(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = True,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\n\n    Uses samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout` for details.\n\n    Args:\n        p: probability of an element to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            dropout, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return (\n        _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n    )\n\n\ndef alpha_dropout(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = False,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Apply alpha dropout to the input.\n\n    See :class:`~torch.nn.AlphaDropout` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            alpha_dropout, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return (\n        _VF.alpha_dropout_(input, p, training)\n        if inplace\n        else _VF.alpha_dropout(input, p, training)\n    )\n\n\ndef dropout1d(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = True,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 1D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 1D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout1d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            dropout1d, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (2, 3):\n        raise RuntimeError(\n            f\"dropout1d: Expected 2D or 3D input, but received a {inp_dim}D input. \"\n            \"Note that dropout1d exists to provide channel-wise dropout on inputs with 1 \"\n            \"spatial dimension, a channel dimension, and an optional batch dimension \"\n            \"(i.e. 2D or 3D inputs).\"\n        )\n\n    is_batched = inp_dim == 3\n    if not is_batched:\n        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n\n    result = (\n        _VF.feature_dropout_(input, p, training)\n        if inplace\n        else _VF.feature_dropout(input, p, training)\n    )\n\n    if not is_batched:\n        result = result.squeeze_(0) if inplace else result.squeeze(0)\n\n    return result\n\n\ndef dropout2d(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = True,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 2D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 2D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout2d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            dropout2d, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (3, 4):\n        warn_msg = (\n            f\"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated \"\n            \"and will result in an error in a future release. To retain the behavior \"\n            \"and silence this warning, please use dropout instead. Note that dropout2d \"\n            \"exists to provide channel-wise dropout on inputs with 2 spatial dimensions, \"\n            \"a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\"\n        )\n        warnings.warn(warn_msg)\n\n    # TODO: Properly support no-batch-dim inputs. For now, these are NOT supported; passing\n    # a 3D input will perform dropout1d behavior instead. This was done historically and the\n    # behavior is maintained here for now.\n    # See https://github.com/pytorch/pytorch/issues/77081\n    if inp_dim == 3:\n        warnings.warn(\n            \"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n            \"1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C \"\n            \"is the channel dim. This behavior will change in a future release to interpret the \"\n            \"input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D \"\n            \"channel-wise dropout behavior, please switch to using dropout1d instead.\"\n        )\n\n    result = (\n        _VF.feature_dropout_(input, p, training)\n        if inplace\n        else _VF.feature_dropout(input, p, training)\n    )\n\n    return result\n\n\ndef dropout3d(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = True,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 3D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 3D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout3d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            dropout3d, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (4, 5):\n        warn_msg = (\n            f\"dropout3d: Received a {inp_dim}-D input to dropout3d, which is deprecated \"\n            \"and will result in an error in a future release. To retain the behavior \"\n            \"and silence this warning, please use dropout instead. Note that dropout3d \"\n            \"exists to provide channel-wise dropout on inputs with 3 spatial dimensions, \"\n            \"a channel dimension, and an optional batch dimension (i.e. 4D or 5D inputs).\"\n        )\n        warnings.warn(warn_msg)\n\n    is_batched = inp_dim == 5\n    if not is_batched:\n        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n\n    result = (\n        _VF.feature_dropout_(input, p, training)\n        if inplace\n        else _VF.feature_dropout(input, p, training)\n    )\n\n    if not is_batched:\n        result = result.squeeze_(0) if inplace else result.squeeze(0)\n    return result\n\n\ndef feature_alpha_dropout(\n    input: Tensor,\n    p: float = 0.5,\n    training: bool = False,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Randomly masks out entire channels (a channel is a feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the batch input\n    is a tensor :math:`\\text{input}[i, j]` of the input tensor. Instead of\n    setting activations to zero, as in regular Dropout, the activations are set\n    to the negative saturation value of the SELU activation function.\n\n    Each element will be masked independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n    The elements to be masked are randomized on every forward call, and scaled\n    and shifted to maintain zero mean and unit variance.\n\n    See :class:`~torch.nn.FeatureAlphaDropout` for details.\n\n    Args:\n        p: dropout probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            feature_alpha_dropout,\n            (input,),\n            input,\n            p=p,\n            training=training,\n            inplace=inplace,\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return (\n        _VF.feature_alpha_dropout_(input, p, training)\n        if inplace\n        else _VF.feature_alpha_dropout(input, p, training)\n    )\n\n\ndef _threshold(\n    input: Tensor,\n    threshold: float,\n    value: float,\n    inplace: bool = False,\n) -> Tensor:\n    r\"\"\"Apply a threshold to each element of the input Tensor.\n\n    See :class:`~torch.nn.Threshold` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            _threshold, (input,), input, threshold, value, inplace=inplace\n        )\n    if inplace:\n        result = _VF.threshold_(input, threshold, value)\n    else:\n        result = _VF.threshold(input, threshold, value)\n    return result\n\n\n# We define this function as _threshold because it takes an argument\n# named threshold, which clobbers the recursive reference to the\n# function needed for __torch_function__ support\nthreshold = _threshold\n\nthreshold_ = _add_docstr(\n    _VF.threshold_,\n    r\"\"\"\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n\"\"\",\n)\n\n\ndef relu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"relu(input, inplace=False) -> Tensor\n\n    Applies the rectified linear unit function element-wise. See\n    :class:`~torch.nn.ReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(relu, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch.relu_(input)\n    else:\n        result = torch.relu(input)\n    return result\n\n\nrelu_ = _add_docstr(\n    torch.relu_,\n    r\"\"\"\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n\"\"\",\n)\n\n\ndef glu(input: Tensor, dim: int = -1) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    glu(input, dim=-1) -> Tensor\n\n    The gated linear unit. Computes:\n\n    .. math ::\n        \\text{GLU}(a, b) = a \\otimes \\sigma(b)\n\n    where `input` is split in half along `dim` to form `a` and `b`, :math:`\\sigma`\n    is the sigmoid function and :math:`\\otimes` is the element-wise product between matrices.\n\n    See `Language Modeling with Gated Convolutional Networks <https://arxiv.org/abs/1612.08083>`_.\n\n    Args:\n        input (Tensor): input tensor\n        dim (int): dimension on which to split the input. Default: -1\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(glu, (input,), input, dim=dim)\n    if input.dim() == 0:\n        raise RuntimeError(\n            \"glu does not support scalars because halving size must be even\"\n        )\n    return torch._C._nn.glu(input, dim)\n\n\ndef hardtanh(\n    input: Tensor,\n    min_val: float = -1.0,\n    max_val: float = 1.0,\n    inplace: bool = False,\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor\n\n    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more\n    details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            hardtanh, (input,), input, min_val=min_val, max_val=max_val, inplace=inplace\n        )\n    if min_val > max_val:\n        raise ValueError(\"min_val cannot be greater than max_val\")\n    if inplace:\n        result = torch._C._nn.hardtanh_(input, min_val, max_val)\n    else:\n        result = torch._C._nn.hardtanh(input, min_val, max_val)\n    return result\n\n\nhardtanh_ = _add_docstr(\n    torch._C._nn.hardtanh_,\n    r\"\"\"\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n\"\"\",\n)\n\n\ndef relu6(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"relu6(input, inplace=False) -> Tensor\n\n    Applies the element-wise function :math:`\\text{ReLU6}(x) = \\min(\\max(0,x), 6)`.\n\n    See :class:`~torch.nn.ReLU6` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(relu6, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.relu6_(input)\n    else:\n        result = torch._C._nn.relu6(input)\n    return result\n\n\ndef elu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Exponential Linear Unit (ELU) function element-wise.\n\n    See :class:`~torch.nn.ELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(elu, (input,), input, alpha=alpha, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.elu_(input, alpha)\n    else:\n        result = torch._C._nn.elu(input, alpha)\n    return result\n\n\nelu_ = _add_docstr(\n    torch._C._nn.elu_,\n    r\"\"\"\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n\"\"\",\n)\n\n\ndef selu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"selu(input, inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))`,\n    with :math:`\\alpha=1.6732632423543772848170429916717` and\n    :math:`scale=1.0507009873554804934193349852946`.\n\n    See :class:`~torch.nn.SELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(selu, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch.selu_(input)\n    else:\n        result = torch.selu(input)\n    return result\n\n\nselu_ = _add_docstr(\n    torch.selu_,\n    r\"\"\"\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n\"\"\",\n)\n\n\ndef celu(\n    input: Tensor,\n    alpha: float = 1.0,\n    inplace: bool = False,\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"celu(input, alpha=1., inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))`.\n\n    See :class:`~torch.nn.CELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            celu, (input,), input, alpha=alpha, inplace=inplace\n        )\n    if inplace:\n        result = torch.celu_(input, alpha)\n    else:\n        result = torch.celu(input, alpha)\n    return result\n\n\ncelu_ = _add_docstr(\n    torch.celu_,\n    r\"\"\"\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n\"\"\",\n)\n\n\ndef leaky_relu(\n    input: Tensor,\n    negative_slope: float = 0.01,\n    inplace: bool = False,\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)`\n\n    See :class:`~torch.nn.LeakyReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            leaky_relu, (input,), input, negative_slope=negative_slope, inplace=inplace\n        )\n    if inplace:\n        result = torch._C._nn.leaky_relu_(input, negative_slope)\n    else:\n        result = torch._C._nn.leaky_relu(input, negative_slope)\n    return result\n\n\nleaky_relu_ = _add_docstr(\n    torch._C._nn.leaky_relu_,\n    r\"\"\"\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n\"\"\",\n)\n\n\nprelu = _add_docstr(\n    torch.prelu,\n    r\"\"\"prelu(input, weight) -> Tensor\n\nApplies element-wise the function\n:math:`\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)` where weight is a\nlearnable parameter.\n\n.. note::\n    `weight` is expected to be a scalar or 1-D tensor. If `weight` is 1-D,\n    its size must match the number of input channels, determined by\n    `input.size(1)` when `input.dim() >= 2`, otherwise 1.\n    In the 1-D case, note that when `input` has dim > 2, `weight` can be expanded\n    to the shape of `input` in a way that is not possible using normal\n    :ref:`broadcasting semantics<broadcasting-semantics>`.\n\nSee :class:`~torch.nn.PReLU` for more details.\n\"\"\",\n)\n\n\ndef rrelu(\n    input: Tensor,\n    lower: float = 1.0 / 8,\n    upper: float = 1.0 / 3,\n    training: bool = False,\n    inplace: bool = False,\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor\n\n    Randomized leaky ReLU.\n\n    See :class:`~torch.nn.RReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            rrelu,\n            (input,),\n            input,\n            lower=lower,\n            upper=upper,\n            training=training,\n            inplace=inplace,\n        )\n    if inplace:\n        result = torch.rrelu_(input, lower, upper, training)\n    else:\n        result = torch.rrelu(input, lower, upper, training)\n    return result\n\n\nrrelu_ = _add_docstr(\n    torch.rrelu_,\n    r\"\"\"\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n\"\"\",\n)\n\nlogsigmoid = _add_docstr(\n    torch._C._nn.log_sigmoid,\n    r\"\"\"\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n\"\"\",\n)\n\ngelu = _add_docstr(\n    torch._C._nn.gelu,\n    r\"\"\"\ngelu(input, approximate = 'none') -> Tensor\n\nWhen the approximate argument is 'none', it applies element-wise the function\n:math:`\\text{GELU}(x) = x * \\Phi(x)`\n\nwhere :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\nWhen the approximate argument is 'tanh', Gelu is estimated with\n\n.. math::\n    \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\nSee `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.\n\"\"\",\n)\n\nhardshrink = _add_docstr(\n    torch.hardshrink,\n    r\"\"\"\nhardshrink(input, lambd=0.5) -> Tensor\n\nApplies the hard shrinkage function element-wise\n\nSee :class:`~torch.nn.Hardshrink` for more details.\n\"\"\",\n)\n\n\ndef tanhshrink(input):  # noqa: D400,D402\n    r\"\"\"tanhshrink(input) -> Tensor\n\n    Applies element-wise, :math:`\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)`\n\n    See :class:`~torch.nn.Tanhshrink` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(tanhshrink, (input,), input)\n    return input - input.tanh()\n\n\ndef softsign(input):  # noqa: D400,D402\n    r\"\"\"softsign(input) -> Tensor\n\n    Applies element-wise, the function :math:`\\text{SoftSign}(x) = \\frac{x}{1 + |x|}`\n\n    See :class:`~torch.nn.Softsign` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(softsign, (input,), input)\n    return input / (input.abs() + 1)\n\n\nsoftplus = _add_docstr(\n    torch._C._nn.softplus,\n    r\"\"\"\nsoftplus(input, beta=1, threshold=20) -> Tensor\n\nApplies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n\nFor numerical stability the implementation reverts to the linear function\nwhen :math:`input \\times \\beta > threshold`.\n\nSee :class:`~torch.nn.Softplus` for more details.\n\"\"\",\n)\n\n\ndef _get_softmax_dim(name: str, ndim: int, stacklevel: int) -> int:\n    warnings.warn(\n        f\"Implicit dimension choice for {name} has been deprecated. \"\n        \"Change the call to include dim=X as an argument.\",\n        stacklevel=stacklevel,\n    )\n    if ndim == 0 or ndim == 1 or ndim == 3:\n        ret = 0\n    else:\n        ret = 1\n    return ret\n\n\ndef softmin(\n    input: Tensor,\n    dim: Optional[int] = None,\n    _stacklevel: int = 3,\n    dtype: Optional[DType] = None,\n) -> Tensor:\n    r\"\"\"Apply a softmin function.\n\n    Note that :math:`\\text{Softmin}(x) = \\text{Softmax}(-x)`. See softmax definition for mathematical formula.\n\n    See :class:`~torch.nn.Softmin` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which softmin will be computed (so every slice\n            along dim will sum to 1).\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is casted to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            softmin, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype\n        )\n    if dim is None:\n        dim = _get_softmax_dim(\"softmin\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = (-input).softmax(dim)\n    else:\n        ret = (-input).softmax(dim, dtype=dtype)\n    return ret\n\n\ndef softmax(\n    input: Tensor,\n    dim: Optional[int] = None,\n    _stacklevel: int = 3,\n    dtype: Optional[DType] = None,\n) -> Tensor:\n    r\"\"\"Apply a softmax function.\n\n    Softmax is defined as:\n\n    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n\n    It is applied to all slices along dim, and will re-scale them so that the elements\n    lie in the range `[0, 1]` and sum to 1.\n\n    See :class:`~torch.nn.Softmax` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which softmax will be computed.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is casted to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n\n    .. note::\n        This function doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use log_softmax instead (it's faster and has better numerical properties).\n\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype\n        )\n    if dim is None:\n        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = input.softmax(dim)\n    else:\n        ret = input.softmax(dim, dtype=dtype)\n    return ret\n\n\ndef gumbel_softmax(\n    logits: Tensor,\n    tau: float = 1,\n    hard: bool = False,\n    eps: float = 1e-10,\n    dim: int = -1,\n) -> Tensor:\n    r\"\"\"\n    Sample from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretize.\n\n    Args:\n      logits: `[..., num_features]` unnormalized log probabilities\n      tau: non-negative scalar temperature\n      hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n            but will be differentiated as if it is the soft sample in autograd\n      dim (int): A dimension along which softmax will be computed. Default: -1.\n\n    Returns:\n      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n      If ``hard=True``, the returned samples will be one-hot, otherwise they will\n      be probability distributions that sum to 1 across `dim`.\n\n    .. note::\n      This function is here for legacy reasons, may be removed from nn.Functional in the future.\n\n    .. note::\n      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`\n\n      It achieves two things:\n      - makes the output value exactly one-hot\n      (since we add then subtract y_soft value)\n      - makes the gradient equal to y_soft gradient\n      (since we strip all other gradients)\n\n    Examples::\n        >>> logits = torch.randn(20, 32)\n        >>> # Sample soft categorical using reparametrization trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n        >>> # Sample hard categorical using \"Straight-through\" trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n\n    .. _Link 1:\n        https://arxiv.org/abs/1611.00712\n    .. _Link 2:\n        https://arxiv.org/abs/1611.01144\n    \"\"\"\n    if has_torch_function_unary(logits):\n        return handle_torch_function(\n            gumbel_softmax, (logits,), logits, tau=tau, hard=hard, eps=eps, dim=dim\n        )\n    if eps != 1e-10:\n        warnings.warn(\"`eps` parameter is deprecated and has no effect.\")\n\n    gumbels = (\n        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)\n        .exponential_()\n        .log()\n    )  # ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)\n\n    if hard:\n        # Straight through.\n        index = y_soft.max(dim, keepdim=True)[1]\n        y_hard = torch.zeros_like(\n            logits, memory_format=torch.legacy_contiguous_format\n        ).scatter_(dim, index, 1.0)\n        ret = y_hard - y_soft.detach() + y_soft\n    else:\n        # Reparametrization trick.\n        ret = y_soft\n    return ret\n\n\ndef log_softmax(\n    input: Tensor,\n    dim: Optional[int] = None,\n    _stacklevel: int = 3,\n    dtype: Optional[DType] = None,\n) -> Tensor:\n    r\"\"\"Apply a softmax followed by a logarithm.\n\n    While mathematically equivalent to log(softmax(x)), doing these two\n    operations separately is slower and numerically unstable. This function\n    uses an alternative formulation to compute the output and gradient correctly.\n\n    See :class:`~torch.nn.LogSoftmax` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which log_softmax will be computed.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is cast to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            log_softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype\n        )\n    if dim is None:\n        dim = _get_softmax_dim(\"log_softmax\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = input.log_softmax(dim)\n    else:\n        ret = input.log_softmax(dim, dtype=dtype)\n    return ret\n\n\nsoftshrink = _add_docstr(\n    torch._C._nn.softshrink,\n    r\"\"\"\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n\"\"\",\n)\n\n\ndef tanh(input):  # noqa: D400,D402\n    r\"\"\"tanh(input) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}`\n\n    See :class:`~torch.nn.Tanh` for more details.\n    \"\"\"\n    return input.tanh()\n\n\ndef sigmoid(input):  # noqa: D400,D402\n    r\"\"\"sigmoid(input) -> Tensor\n\n    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n\n    See :class:`~torch.nn.Sigmoid` for more details.\n    \"\"\"\n    return input.sigmoid()\n\n\ndef hardsigmoid(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Hardsigmoid function element-wise.\n\n    .. math::\n        \\text{Hardsigmoid}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            1 & \\text{if~} x \\ge +3, \\\\\n            x / 6 + 1 / 2 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n\n    See :class:`~torch.nn.Hardsigmoid` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.hardsigmoid_(input)\n    return torch._C._nn.hardsigmoid(input)\n\n\nlinear = _add_docstr(\n    torch._C._nn.linear,\n    r\"\"\"\nlinear(input, weight, bias=None) -> Tensor\n\nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>`\n\n{sparse_beta_warning}\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nShape:\n\n    - Input: :math:`(*, in\\_features)` where `*` means any number of\n      additional dimensions, including none\n    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n    - Bias: :math:`(out\\_features)` or :math:`()`\n    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n\"\"\".format(\n        **sparse_support_notes\n    ),\n)\n\n\nbilinear = _add_docstr(\n    torch.bilinear,\n    r\"\"\"\nbilinear(input1, input2, weight, bias=None) -> Tensor\n\nApplies a bilinear transformation to the incoming data:\n:math:`y = x_1^T A x_2 + b`\n\nShape:\n\n    - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}`\n      and :math:`*` means any number of additional dimensions.\n      All but the last dimension of the inputs should be the same.\n    - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`\n    - weight: :math:`(\\text{out\\_features}, \\text{in1\\_features},\n      \\text{in2\\_features})`\n    - bias: :math:`(\\text{out\\_features})`\n    - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n      and all but the last dimension are the same shape as the input.\n\"\"\",\n)\n\n\ndef silu(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n    The SiLU function is also known as the swish function.\n\n    .. math::\n        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n    .. note::\n        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n        where the SiLU was experimented with later.\n\n    See :class:`~torch.nn.SiLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(silu, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.silu_(input)\n    return torch._C._nn.silu(input)\n\n\ndef mish(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Mish function, element-wise.\n\n    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    .. math::\n        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n    .. note::\n        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\n    See :class:`~torch.nn.Mish` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(mish, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.mish_(input)\n    return torch._C._nn.mish(input)\n\n\ndef hardswish(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply hardswish function, element-wise.\n\n    Follows implementation as described in the paper:\n    `Searching for MobileNetV3`_.\n\n    .. math::\n        \\text{Hardswish}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            x & \\text{if~} x \\ge +3, \\\\\n            x \\cdot (x + 3) /6 & \\text{otherwise}\n        \\end{cases}\n\n    See :class:`~torch.nn.Hardswish` for more details.\n\n    .. _`Searching for MobileNetV3`:\n        https://arxiv.org/abs/1905.02244\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(hardswish, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.hardswish_(input)\n    return torch._C._nn.hardswish(input)\n\n\ndef _no_grad_embedding_renorm_(\n    weight: Tensor,\n    input: Tensor,\n    max_norm: float,\n    norm_type: float,\n) -> tuple[Tensor, Tensor]:\n    torch.embedding_renorm_(weight.detach(), input, max_norm, norm_type)\n\n\ndef embedding(\n    input: Tensor,\n    weight: Tensor,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n) -> Tensor:\n    r\"\"\"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n    This module is often used to retrieve word embeddings using indices.\n    The input to the module is a list of indices, and the embedding matrix,\n    and the output is the corresponding word embeddings.\n\n    See :class:`torch.nn.Embedding` for more details.\n\n    .. note::\n        Note that the analytical gradients of this function with respect to\n        entries in :attr:`weight` at the row specified by :attr:`padding_idx`\n        are expected to differ from the numerical ones.\n\n    .. note::\n        Note that `:class:`torch.nn.Embedding` differs from this function in\n        that it initializes the row of :attr:`weight` specified by\n        :attr:`padding_idx` to all zeros on construction.\n\n    Args:\n        input (LongTensor): Tensor containing indices into the embedding matrix\n        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n            and number of columns equal to the embedding size\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                     i.e. it remains as a fixed \"pad\".\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n                                    Note: this will modify :attr:`weight` in-place.\n        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n\n    Shape:\n        - Input: LongTensor of arbitrary shape containing the indices to extract\n        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,\n          where V = maximum index + 1 and embedding_dim = the embedding size\n        - Output: `(*, embedding_dim)`, where `*` is the input shape\n\n    Examples::\n\n        >>> # a batch of 2 samples of 4 indices each\n        >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n        >>> # an embedding matrix containing 10 tensors of size 3\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> F.embedding(input, embedding_matrix)\n        tensor([[[ 0.8490,  0.9625,  0.6753],\n                 [ 0.9666,  0.7761,  0.6108],\n                 [ 0.6246,  0.9751,  0.3618],\n                 [ 0.4161,  0.2419,  0.7383]],\n\n                [[ 0.6246,  0.9751,  0.3618],\n                 [ 0.0237,  0.7794,  0.0528],\n                 [ 0.9666,  0.7761,  0.6108],\n                 [ 0.3385,  0.8612,  0.1867]]])\n\n        >>> # example with padding_idx\n        >>> weights = torch.rand(10, 3)\n        >>> weights[0, :].zero_()\n        >>> embedding_matrix = weights\n        >>> input = torch.tensor([[0, 2, 0, 5]])\n        >>> F.embedding(input, embedding_matrix, padding_idx=0)\n        tensor([[[ 0.0000,  0.0000,  0.0000],\n                 [ 0.5609,  0.5384,  0.8720],\n                 [ 0.0000,  0.0000,  0.0000],\n                 [ 0.6262,  0.2438,  0.7471]]])\n    \"\"\"\n    if has_torch_function_variadic(input, weight):\n        return handle_torch_function(\n            embedding,\n            (input, weight),\n            input,\n            weight,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse,\n        )\n    if padding_idx is not None:\n        if padding_idx > 0:\n            assert padding_idx < weight.size(\n                0\n            ), \"Padding_idx must be within num_embeddings\"\n        elif padding_idx < 0:\n            assert padding_idx >= -weight.size(\n                0\n            ), \"Padding_idx must be within num_embeddings\"\n            padding_idx = weight.size(0) + padding_idx\n    else:\n        padding_idx = -1\n    if max_norm is not None:\n        # Note [embedding_renorm contiguous]\n        # `embedding_renorm_` will call .contiguous() on input anyways, so we\n        # call it here and take advantage of the improved locality in the\n        # `embedding` call below too.\n        input = input.contiguous()\n        # Note [embedding_renorm set_grad_enabled]\n        # XXX: equivalent to\n        # with torch.no_grad():\n        #   torch.embedding_renorm_\n        # remove once script supports set_grad_enabled\n        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n\n\ndef embedding_bag(\n    input: Tensor,\n    weight: Tensor,\n    offsets: Optional[Tensor] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2,\n    scale_grad_by_freq: bool = False,\n    mode: str = \"mean\",\n    sparse: bool = False,\n    per_sample_weights: Optional[Tensor] = None,\n    include_last_offset: bool = False,\n    padding_idx: Optional[int] = None,\n) -> Tensor:\n    r\"\"\"Compute sums, means or maxes of `bags` of embeddings.\n\n    Calculation is done without instantiating the intermediate embeddings.\n    See :class:`torch.nn.EmbeddingBag` for more details.\n\n    Note:\n        {backward_reproducibility_note}\n\n    Args:\n        input (LongTensor): Tensor containing bags of indices into the embedding matrix\n        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n            and number of columns equal to the embedding size\n        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines\n                             the starting index position of each bag (sequence) in :attr:`input`.\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n                                    Note: this will modify :attr:`weight` in-place.\n        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.\n                                     Default ``2``.\n        scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n                                                Note: this option is not supported when ``mode=\"max\"``.\n        mode (str, optional): ``\"sum\"``, ``\"mean\"`` or ``\"max\"``. Specifies the way to reduce the bag.\n                                 Default: ``\"mean\"``\n        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n                                 Note: this option is not supported when ``mode=\"max\"``.\n        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\n            to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`\n            must have exactly the same shape as input and is treated as having the same\n            :attr:`offsets`, if those are not None.\n\n        include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.\n            The last element is the size of the input, or the ending index position of the last bag (sequence).\n\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the\n                                     gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated\n                                     during training, i.e. it remains as a fixed \"pad\". Note that the embedding\n                                     vector at :attr:`padding_idx` is excluded from the reduction.\n\n    Shape:\n        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)\n\n          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)\n            each of fixed length ``N``, and this will return ``B`` values aggregated in a way\n            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.\n\n          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of\n            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing\n            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`\n            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.\n            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n\n        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`\n\n        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.\n\n        - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`\n\n    Examples::\n\n        >>> # an Embedding module containing 10 tensors of size 3\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> # a batch of 2 samples of 4 indices each\n        >>> input = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\n        >>> offsets = torch.tensor([0, 4])\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> F.embedding_bag(input, embedding_matrix, offsets)\n        tensor([[ 0.3397,  0.3552,  0.5545],\n                [ 0.5893,  0.4386,  0.5882]])\n\n        >>> # example with padding_idx\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])\n        >>> offsets = torch.tensor([0, 4])\n        >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')\n        tensor([[ 0.0000,  0.0000,  0.0000],\n                [-0.7082,  3.2145, -2.6251]])\n    \"\"\"\n    if has_torch_function_variadic(input, weight, offsets, per_sample_weights):\n        return handle_torch_function(\n            embedding_bag,\n            (input, weight, offsets, per_sample_weights),\n            input,\n            weight,\n            offsets=offsets,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            mode=mode,\n            sparse=sparse,\n            per_sample_weights=per_sample_weights,\n            include_last_offset=include_last_offset,\n            padding_idx=padding_idx,\n        )\n    # Check for backward compatibility.\n    # Used to be embedding_bag(weight, input, ...)\n    # Now is     embedding_bag(input, weight, ...)\n    if weight.dtype == torch.long and input.is_floating_point():\n        warnings.warn(\n            \"Argument order of nn.functional.embedding_bag was changed. \"\n            \"Usage `embedding_bag(weight, input, ...)` is deprecated, \"\n            \"and should now be `embedding_bag(input, weight, ...)`.\"\n        )\n        weight, input = input, weight\n\n    if per_sample_weights is not None and input.size() != per_sample_weights.size():\n        raise ValueError(\n            f\"embedding_bag: If per_sample_weights ({per_sample_weights.shape}) is not None, \"\n            f\"then it must have the same shape as the input ({input.shape})\"\n        )\n\n    if not weight.dim() == 2:\n        raise ValueError(\n            f\"weight has to be a 2D Tensor, but got Tensor of dimension {weight.dim()}\"\n        )\n\n    if not torch.jit.is_scripting() and input.dim() == 2 and input.is_nested:\n        include_last_offset = True\n        offsets = input.offsets()\n        input = input.values().reshape(-1)\n        if per_sample_weights is not None:\n            if not per_sample_weights.is_nested:\n                raise ValueError(\n                    \"If input is nested, then per_sample_weights must be nested if specified\"\n                )\n            per_sample_weights = per_sample_weights.values().reshape(-1)\n    elif input.dim() == 2:\n        if offsets is not None:\n            type_str = \"<unknown>\"\n            # TODO: Remove this once script supports type() calls\n            if not torch.jit.is_scripting():\n                type_str = str(type(offsets))\n            raise ValueError(\n                \"if input is 2D, then offsets has to be None\"\n                \", as input is treated is a mini-batch of\"\n                \" fixed length sequences. However, found \"\n                f\"offsets of type {type_str}\"\n            )\n        offsets = torch.arange(\n            0, input.numel(), input.size(1), dtype=input.dtype, device=input.device\n        )\n\n        input = input.reshape(-1)\n        if per_sample_weights is not None:\n            per_sample_weights = per_sample_weights.reshape(-1)\n    elif input.dim() == 1:\n        if offsets is None:\n            raise ValueError(\"offsets has to be a 1D Tensor but got None\")\n        if offsets.dim() != 1:\n            raise ValueError(\"offsets has to be a 1D Tensor\")\n    else:\n        raise ValueError(\n            f\"input has to be 1D or 2D Tensor, but got Tensor of dimension {input.dim()}\"\n        )\n    if mode == \"sum\":\n        mode_enum = 0\n    elif mode == \"mean\":\n        mode_enum = 1\n    elif mode == \"max\":\n        mode_enum = 2\n\n        if scale_grad_by_freq:\n            raise ValueError(\n                \"max mode does not support scaling the gradient by the frequency\"\n            )\n\n        if sparse:\n            raise ValueError(\"max mode does not support sparse weights\")\n\n    else:\n        raise ValueError(\"mode has to be one of sum, mean or max\")\n\n    if max_norm is not None:\n        # XXX: equivalent to\n        # with torch.no_grad():\n        #   torch.nembedding_renorm_\n        # remove once script supports set_grad_enabled\n        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n\n    if per_sample_weights is not None and mode != \"sum\":\n        raise NotImplementedError(\n            \"embedding_bag: per_sample_weights was not None. \"\n            \"per_sample_weights is only supported for mode='sum' \"\n            f\"(got mode='{mode}'). Please open a feature request on GitHub.\"\n        )\n\n    ret, _, _, _ = torch.embedding_bag(\n        weight,\n        input,\n        offsets,\n        scale_grad_by_freq,\n        mode_enum,\n        sparse,\n        per_sample_weights,\n        include_last_offset,\n        padding_idx,\n    )\n    return ret\n\n\nif embedding_bag.__doc__:\n    embedding_bag.__doc__ = embedding_bag.__doc__.format(**reproducibility_notes)\n\n\ndef _verify_batch_size(size: list[int]) -> None:\n    # XXX: JIT script does not support the reduce from functools, and mul op is a\n    # builtin, which cannot be used as a value to a func yet, so rewrite this size\n    # check to a simple equivalent for loop\n    #\n    # TODO: make use of reduce like below when JIT is ready with the missing features:\n    # from operator import mul\n    # from functools import reduce\n    #\n    #   if reduce(mul, size[2:], size[0]) == 1\n    size_prods = size[0]\n    for i in range(len(size) - 2):\n        size_prods *= size[i + 2]\n    if size_prods == 1:\n        raise ValueError(\n            f\"Expected more than 1 value per channel when training, got input size {size}\"\n        )\n\n\ndef batch_norm(\n    input: Tensor,\n    running_mean: Optional[Tensor],\n    running_var: Optional[Tensor],\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    training: bool = False,\n    momentum: float = 0.1,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Batch Normalization for each channel across a batch of data.\n\n    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\n    :class:`~torch.nn.BatchNorm3d` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n        return handle_torch_function(\n            batch_norm,\n            (input, running_mean, running_var, weight, bias),\n            input,\n            running_mean,\n            running_var,\n            weight=weight,\n            bias=bias,\n            training=training,\n            momentum=momentum,\n            eps=eps,\n        )\n    if training:\n        _verify_batch_size(input.size())\n\n    return torch.batch_norm(\n        input,\n        weight,\n        bias,\n        running_mean,\n        running_var,\n        training,\n        momentum,\n        eps,\n        torch.backends.cudnn.enabled,\n    )\n\n\ndef _verify_spatial_size(size: list[int]) -> None:\n    # Verify that there is > 1 spatial element for instance norm calculation.\n    size_prods = 1\n    for i in range(2, len(size)):\n        size_prods *= size[i]\n    if size_prods == 1:\n        raise ValueError(\n            f\"Expected more than 1 spatial element when training, got input size {size}\"\n        )\n\n\ndef instance_norm(\n    input: Tensor,\n    running_mean: Optional[Tensor] = None,\n    running_var: Optional[Tensor] = None,\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    use_input_stats: bool = True,\n    momentum: float = 0.1,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Instance Normalization independently for each channel in every data sample within a batch.\n\n    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,\n    :class:`~torch.nn.InstanceNorm3d` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n        return handle_torch_function(\n            instance_norm,\n            (input, running_mean, running_var, weight, bias),\n            input,\n            running_mean=running_mean,\n            running_var=running_var,\n            weight=weight,\n            bias=bias,\n            use_input_stats=use_input_stats,\n            momentum=momentum,\n            eps=eps,\n        )\n    if use_input_stats:\n        _verify_spatial_size(input.size())\n    return torch.instance_norm(\n        input,\n        weight,\n        bias,\n        running_mean,\n        running_var,\n        use_input_stats,\n        momentum,\n        eps,\n        torch.backends.cudnn.enabled,\n    )\n\n\ndef layer_norm(\n    input: Tensor,\n    normalized_shape: list[int],\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Layer Normalization for last certain number of dimensions.\n\n    See :class:`~torch.nn.LayerNorm` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(\n            layer_norm,\n            (input, weight, bias),\n            input,\n            normalized_shape,\n            weight=weight,\n            bias=bias,\n            eps=eps,\n        )\n    return torch.layer_norm(\n        input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled\n    )\n\n\ndef rms_norm(\n    input: Tensor,\n    normalized_shape: list[int],\n    weight: Optional[Tensor] = None,\n    eps: Optional[float] = None,\n) -> Tensor:\n    r\"\"\"Apply Root Mean Square Layer Normalization.\n\n    See :class:`~torch.nn.RMSNorm` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, weight):\n        return handle_torch_function(\n            rms_norm, (input, weight), input, normalized_shape, weight=weight, eps=eps\n        )\n    return torch.rms_norm(input, normalized_shape, weight, eps)\n\n\ndef group_norm(\n    input: Tensor,\n    num_groups: int,\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Group Normalization for last certain number of dimensions.\n\n    See :class:`~torch.nn.GroupNorm` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(\n            group_norm,\n            (\n                input,\n                weight,\n                bias,\n            ),\n            input,\n            num_groups,\n            weight=weight,\n            bias=bias,\n            eps=eps,\n        )\n    if input.dim() < 2:\n        raise RuntimeError(\n            f\"Expected at least 2 dimensions for input tensor but received {input.dim()}\"\n        )\n    _verify_batch_size(\n        [input.size(0) * input.size(1) // num_groups, num_groups]\n        + list(input.size()[2:])\n    )\n    return torch.group_norm(\n        input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled\n    )\n\n\ndef local_response_norm(\n    input: Tensor,\n    size: int,\n    alpha: float = 1e-4,\n    beta: float = 0.75,\n    k: float = 1.0,\n) -> Tensor:\n    r\"\"\"Apply local response normalization over an input signal.\n\n    The input signal is composed of several input planes, where channels occupy the second dimension.\n    Normalization is applied across channels.\n\n    See :class:`~torch.nn.LocalResponseNorm` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            local_response_norm, (input,), input, size, alpha=alpha, beta=beta, k=k\n        )\n    dim = input.dim()\n    if dim < 3:\n        raise ValueError(\n            f\"Expected 3D or higher dimensionality                          input (got {dim} dimensions)\"\n        )\n\n    if input.numel() == 0:\n        return input\n\n    div = input.mul(input)\n    if dim == 3:\n        div = div.unsqueeze(1)\n        div = pad(div, (0, 0, size // 2, (size - 1) // 2))\n        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)\n    else:\n        sizes = input.size()\n        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)\n        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))\n        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)\n        div = div.view(sizes)\n    div = div.mul(alpha).add(k).pow(beta)\n    return input / div\n\n\n# loss\n\n\ndef ctc_loss(\n    log_probs: Tensor,\n    targets: Tensor,\n    input_lengths: Tensor,\n    target_lengths: Tensor,\n    blank: int = 0,\n    reduction: str = \"mean\",\n    zero_infinity: bool = False,\n) -> Tensor:\n    r\"\"\"Apply the Connectionist Temporal Classification loss.\n\n    See :class:`~torch.nn.CTCLoss` for details.\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Note:\n        {backward_reproducibility_note}\n\n    Args:\n        log_probs: :math:`(T, N, C)` or :math:`(T, C)` where `C = number of characters in alphabet including blank`,\n            `T = input length`, and `N = batch size`.\n            The logarithmized probabilities of the outputs\n            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n        targets: :math:`(N, S)` or `(sum(target_lengths))`.\n            Targets cannot be blank. In the second form, the targets are assumed to be concatenated.\n        input_lengths: :math:`(N)` or :math:`()`.\n            Lengths of the inputs (must each be :math:`\\leq T`)\n        target_lengths: :math:`(N)` or :math:`()`.\n            Lengths of the targets\n        blank (int, optional):\n            Blank label. Default :math:`0`.\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the output losses will be divided by the target lengths and\n            then the mean over the batch is taken, ``'sum'``: the output will be\n            summed. Default: ``'mean'``\n        zero_infinity (bool, optional):\n            Whether to zero infinite losses and the associated gradients.\n            Default: ``False``\n            Infinite losses mainly occur when the inputs are too short\n            to be aligned to the targets.\n\n    Example::\n\n        >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n        >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n        >>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n        >>> target_lengths = torch.randint(10, 30, (16,), dtype=torch.long)\n        >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n        return handle_torch_function(\n            ctc_loss,\n            (log_probs, targets, input_lengths, target_lengths),\n            log_probs,\n            targets,\n            input_lengths,\n            target_lengths,\n            blank=blank,\n            reduction=reduction,\n            zero_infinity=zero_infinity,\n        )\n    return torch.ctc_loss(\n        log_probs,\n        targets,\n        input_lengths,\n        target_lengths,\n        blank,\n        _Reduction.get_enum(reduction),\n        zero_infinity,\n    )\n\n\nif ctc_loss.__doc__:\n    ctc_loss.__doc__ = ctc_loss.__doc__.format(**reproducibility_notes)\n\n\ndef nll_loss(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    ignore_index: int = -100,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Compute the negative log likelihood loss.\n\n    See :class:`~torch.nn.NLLLoss` for details.\n\n    Args:\n        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n            in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n        target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n            K-dimensional loss.\n        weight (Tensor, optional): a manual rescaling weight given to each\n            class. If given, has to be a Tensor of size `C`\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        ignore_index (int, optional): Specifies a target value that is ignored\n            and does not contribute to the input gradient. When :attr:`size_average` is\n            ``True``, the loss is averaged over non-ignored targets. Default: -100\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    Example::\n\n        >>> # input is of size N x C = 3 x 5\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> # each element in target has to have 0 <= value < C\n        >>> target = torch.tensor([1, 0, 4])\n        >>> output = F.nll_loss(F.log_softmax(input, dim=1), target)\n        >>> output.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            nll_loss,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            ignore_index=ignore_index,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    return torch._C._nn.nll_loss_nd(\n        input, target, weight, _Reduction.get_enum(reduction), ignore_index\n    )\n\n\ndef poisson_nll_loss(\n    input: Tensor,\n    target: Tensor,\n    log_input: bool = True,\n    full: bool = False,\n    size_average: Optional[bool] = None,\n    eps: float = 1e-8,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Poisson negative log likelihood loss.\n\n    See :class:`~torch.nn.PoissonNLLLoss` for details.\n\n    Args:\n        input: expectation of underlying Poisson distribution.\n        target: random sample :math:`target \\sim \\text{Poisson}(input)`.\n        log_input: if ``True`` the loss is computed as\n            :math:`\\exp(\\text{input}) - \\text{target} * \\text{input}`, if ``False`` then loss is\n            :math:`\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})`. Default: ``True``\n        full: whether to compute full loss, i. e. to add the Stirling\n            approximation term. Default: ``False``\n            :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`.\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n            :attr:`log_input`\\ =\\ ``False``. Default: 1e-8\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            poisson_nll_loss,\n            (input, target),\n            input,\n            target,\n            log_input=log_input,\n            full=full,\n            size_average=size_average,\n            eps=eps,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    if reduction != \"none\" and reduction != \"mean\" and reduction != \"sum\":\n        ret = input\n        raise ValueError(reduction + \" is not a valid value for reduction\")\n\n    ret = torch.poisson_nll_loss(\n        input, target, log_input, full, eps, _Reduction.get_enum(reduction)\n    )\n    return ret\n\n\ndef gaussian_nll_loss(\n    input: Tensor,\n    target: Tensor,\n    var: Union[Tensor, float],\n    full: bool = False,\n    eps: float = 1e-6,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Gaussian negative log likelihood loss.\n\n    See :class:`~torch.nn.GaussianNLLLoss` for details.\n\n    Args:\n        input: expectation of the Gaussian distribution.\n        target: sample from the Gaussian distribution.\n        var: tensor of positive variance(s), one for each of the expectations\n            in the input (heteroscedastic), or a single one (homoscedastic),\n            or a positive scalar value to be used for all expectations.\n        full (bool, optional): include the constant term in the loss calculation. Default: ``False``.\n        eps (float, optional): value added to var, for stability. Default: 1e-6.\n        reduction (str, optional): specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the output is the average of all batch member losses,\n            ``'sum'``: the output is the sum of all batch member losses.\n            Default: ``'mean'``.\n    \"\"\"\n    if has_torch_function_variadic(input, target, var):\n        return handle_torch_function(\n            gaussian_nll_loss,\n            (input, target, var),\n            input,\n            target,\n            var,\n            full=full,\n            eps=eps,\n            reduction=reduction,\n        )\n\n    # Entries of var must be non-negative\n    if isinstance(var, float):\n        if var < 0:\n            raise ValueError(\"var has negative entry/entries\")\n        var = var * torch.ones_like(input)\n    elif torch.any(var < 0):\n        raise ValueError(\"var has negative entry/entries\")\n\n    # Check var size\n    # If var.size == input.size, the case is heteroscedastic and no further checks are needed.\n    # Otherwise:\n    if var.size() != input.size():\n        # If var is one dimension short of input, but the sizes match otherwise, then this is a homoscedastic case.\n        # e.g. input.size = (10, 2, 3), var.size = (10, 2)\n        # -> unsqueeze var so that var.shape = (10, 2, 1)\n        # this is done so that broadcasting can happen in the loss calculation\n        if input.size()[:-1] == var.size():\n            var = torch.unsqueeze(var, -1)\n\n        # This checks if the sizes match up to the final dimension, and the final dimension of var is of size 1.\n        # This is also a homoscedastic case.\n        # e.g. input.size = (10, 2, 3), var.size = (10, 2, 1)\n        elif (\n            input.size()[:-1] == var.size()[:-1] and var.size(-1) == 1\n        ):  # Heteroscedastic case\n            pass\n\n        # If none of the above pass, then the size of var is incorrect.\n        else:\n            raise ValueError(\"var is of incorrect size\")\n\n    # Check validity of reduction mode\n    if reduction != \"none\" and reduction != \"mean\" and reduction != \"sum\":\n        raise ValueError(reduction + \" is not valid\")\n\n    # Clamp for stability\n    var = var.clone()\n    with torch.no_grad():\n        var.clamp_(min=eps)\n\n    # Calculate the loss\n    loss = 0.5 * (torch.log(var) + (input - target) ** 2 / var)\n    if full:\n        loss += 0.5 * math.log(2 * math.pi)\n\n    if reduction == \"mean\":\n        return loss.mean()\n    elif reduction == \"sum\":\n        return loss.sum()\n    else:\n        return loss\n\n\ndef kl_div(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    log_target: bool = False,\n) -> Tensor:\n    r\"\"\"Compute the KL Divergence loss.\n\n    Refer - The `Kullback-Leibler divergence Loss\n    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`__\n\n    See :class:`~torch.nn.KLDivLoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape in log-probabilities.\n        target: Tensor of the same shape as input. See :attr:`log_target` for\n            the target's interpretation.\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.\n            ``'none'``: no reduction will be applied\n            ``'batchmean'``: the sum of the output will be divided by the batchsize\n            ``'sum'``: the output will be summed\n            ``'mean'``: the output will be divided by the number of elements in the output\n            Default: ``'mean'``\n        log_target (bool): A flag indicating whether ``target`` is passed in the log space.\n            It is recommended to pass certain distributions (like ``softmax``)\n            in the log space to avoid numerical issues caused by explicit ``log``.\n            Default: ``False``\n\n    .. note::\n        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,\n        and in the meantime, specifying either of those two args will override :attr:`reduction`.\n\n    .. warning::\n        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use\n        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            kl_div,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            log_target=log_target,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        if reduction == \"mean\":\n            warnings.warn(\n                \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n                \"'batchmean' divides only by the batch size, and aligns with the KL div math definition.\"\n                \"'mean' will be changed to behave the same as 'batchmean' in the next major release.\"\n            )\n\n        # special case for batchmean\n        if reduction == \"batchmean\":\n            reduction_enum = _Reduction.get_enum(\"sum\")\n        else:\n            reduction_enum = _Reduction.get_enum(reduction)\n\n    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)\n\n    if reduction == \"batchmean\" and input.dim() != 0:\n        reduced = reduced / input.size()[0]\n\n    return reduced\n\n\ndef cross_entropy(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    ignore_index: int = -100,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    label_smoothing: float = 0.0,\n) -> Tensor:\n    r\"\"\"Compute the cross entropy loss between input logits and target.\n\n    See :class:`~torch.nn.CrossEntropyLoss` for details.\n\n    Args:\n        input (Tensor) : Predicted unnormalized logits;\n            see Shape section below for supported shapes.\n        target (Tensor) : Ground truth class indices or class probabilities;\n            see Shape section below for supported shapes.\n        weight (Tensor, optional): a manual rescaling weight given to each\n            class. If given, has to be a Tensor of size `C`\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        ignore_index (int, optional): Specifies a target value that is ignored\n            and does not contribute to the input gradient. When :attr:`size_average` is\n            ``True``, the loss is averaged over non-ignored targets. Note that\n            :attr:`ignore_index` is only applicable when the target contains class indices.\n            Default: -100\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n            become a mixture of the original ground truth and a uniform distribution as described in\n            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\n    Shape:\n        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n          in the case of `K`-dimensional loss.\n        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n\n        where:\n\n        .. math::\n            \\begin{aligned}\n                C ={} & \\text{number of classes} \\\\\n                N ={} & \\text{batch size} \\\\\n            \\end{aligned}\n\n    Examples::\n\n        >>> # Example of target with class indices\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> target = torch.randint(5, (3,), dtype=torch.int64)\n        >>> loss = F.cross_entropy(input, target)\n        >>> loss.backward()\n        >>>\n        >>> # Example of target with class probabilities\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> target = torch.randn(3, 5).softmax(dim=1)\n        >>> loss = F.cross_entropy(input, target)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            cross_entropy,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            ignore_index=ignore_index,\n            reduce=reduce,\n            reduction=reduction,\n            label_smoothing=label_smoothing,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    return torch._C._nn.cross_entropy_loss(\n        input,\n        target,\n        weight,\n        _Reduction.get_enum(reduction),\n        ignore_index,\n        label_smoothing,\n    )\n\n\ndef binary_cross_entropy(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Measure Binary Cross Entropy between the target and input probabilities.\n\n    See :class:`~torch.nn.BCELoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape as probabilities.\n        target: Tensor of the same shape as input with values between 0 and 1.\n        weight (Tensor, optional): a manual rescaling weight\n                if provided it's repeated to match input tensor shape\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    Examples::\n\n        >>> input = torch.randn(3, 2, requires_grad=True)\n        >>> target = torch.rand(3, 2, requires_grad=False)\n        >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            binary_cross_entropy,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if target.size() != input.size():\n        raise ValueError(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}) is deprecated. \"\n            \"Please ensure they have the same size.\"\n        )\n\n    if weight is not None:\n        new_size = _infer_size(target.size(), weight.size())\n        weight = weight.expand(new_size)\n\n    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n\n\ndef binary_cross_entropy_with_logits(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    pos_weight: Optional[Tensor] = None,\n) -> Tensor:\n    r\"\"\"Calculate Binary Cross Entropy between target and input logits.\n\n    See :class:`~torch.nn.BCEWithLogitsLoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n        target: Tensor of the same shape as input with values between 0 and 1\n        weight (Tensor, optional): a manual rescaling weight\n            if provided it's repeated to match input tensor shape\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n        pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n            Must be a tensor with equal size along the class dimension to the number of classes.\n            Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n            operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n            size [B, C, H, W] will apply different pos_weights to each element of the batch or\n            [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n            along all spatial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n            Default: ``None``\n\n    Examples::\n\n         >>> input = torch.randn(3, requires_grad=True)\n         >>> target = torch.empty(3).random_(2)\n         >>> loss = F.binary_cross_entropy_with_logits(input, target)\n         >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight, pos_weight):\n        return handle_torch_function(\n            binary_cross_entropy_with_logits,\n            (input, target, weight, pos_weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            pos_weight=pos_weight,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n\n    if not (target.size() == input.size()):\n        raise ValueError(\n            f\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\n        )\n\n    return torch.binary_cross_entropy_with_logits(\n        input, target, weight, pos_weight, reduction_enum\n    )\n\n\ndef smooth_l1_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    beta: float = 1.0,\n) -> Tensor:\n    r\"\"\"Compute the Smooth L1 loss.\n\n    Function uses a squared term if the absolute\n    element-wise error falls below beta and an L1 term otherwise.\n\n    See :class:`~torch.nn.SmoothL1Loss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            smooth_l1_loss,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            beta=beta,\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n\n    if beta == 0.0:\n        return torch._C._nn.l1_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(reduction)\n        )\n    else:\n        return torch._C._nn.smooth_l1_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(reduction), beta\n        )\n\n\ndef huber_loss(\n    input: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    delta: float = 1.0,\n    weight: Optional[Tensor] = None,\n) -> Tensor:\n    r\"\"\"huber_loss(input, target, reduction='mean', delta=1.0, weight=None) -> Tensor\n\n    Computes the Huber loss, with optional weighting.\n\n    Function uses a squared term if the absolute\n    element-wise error falls below delta and a delta-scaled L1 term otherwise.\n\n    When delta equals 1, this loss is equivalent to SmoothL1Loss.\n    In general, Huber loss differs from SmoothL1Loss by a factor of delta (AKA beta in Smooth L1).\n\n    Args:\n        input (Tensor): Predicted values.\n        target (Tensor): Ground truth values.\n        reduction (str, optional): Specifies the reduction to apply to the output:\n                                   'none' | 'mean' | 'sum'. 'mean': the mean of the output is taken.\n                                   'sum': the output will be summed. 'none': no reduction will be applied.\n                                   Default: 'mean'.\n        delta (float, optional): The threshold at which to change between delta-scaled L1 and L2 loss. Default: 1.0.\n        weight (Tensor, optional): Weights for each sample. Default: None.\n\n    Returns:\n        Tensor: Huber loss (optionally weighted).\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            huber_loss,\n            (input, target, weight),\n            input,\n            target,\n            reduction=reduction,\n            delta=delta,\n            weight=weight,\n        )\n\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n\n    if weight is None:\n        # Use the optimized C++ backend for standard Huber loss\n        return torch._C._nn.huber_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(reduction), delta\n        )\n    else:\n        if weight.size() != input.size():\n            raise ValueError(\"Weights and input must have the same size.\")\n\n        # Calculate the unweighted loss first\n        unweighted_loss = torch._C._nn.huber_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(\"none\"), delta\n        )\n\n        # Apply weight to the unweighted loss\n        weighted_loss = unweighted_loss * weight\n\n        if reduction == \"none\":\n            return weighted_loss\n        elif reduction == \"sum\":\n            return torch.sum(weighted_loss)\n        elif reduction == \"mean\":\n            return weighted_loss.mean()\n        else:\n            raise ValueError(\n                f\"Invalid reduction mode: {reduction}. Expected one of 'none', 'mean', 'sum'.\"\n            )\n\n\ndef l1_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    weight: Optional[Tensor] = None,\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    Function that takes the mean element-wise absolute value difference.\n\n    See :class:`~torch.nn.L1Loss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            l1_loss,\n            (input, target, weight),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n\n    if weight is not None:\n        if weight.size() != input.size():\n            raise ValueError(\"Weights and input must have the same size.\")\n\n        absolute_errors = torch.abs(expanded_input - expanded_target)\n        weighted_absolute_errors = absolute_errors * weight\n\n        if reduction == \"none\":\n            return weighted_absolute_errors\n        elif reduction == \"sum\":\n            return torch.sum(weighted_absolute_errors)\n        elif reduction == \"mean\":\n            return torch.sum(weighted_absolute_errors) / torch.sum(weight)\n        else:\n            raise ValueError(\n                f\"Invalid reduction mode: {reduction}. Expected one of 'none', 'mean', 'sum'.\"\n            )\n    else:\n        return torch._C._nn.l1_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(reduction)\n        )\n\n\ndef mse_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    weight: Optional[Tensor] = None,\n) -> Tensor:\n    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean', weight=None) -> Tensor\n\n    Measures the element-wise mean squared error, with optional weighting.\n\n    Args:\n        input (Tensor): Predicted values.\n        target (Tensor): Ground truth values.\n        size_average (bool, optional): Deprecated (use reduction).\n        reduce (bool, optional): Deprecated (use reduction).\n        reduction (str, optional): Specifies the reduction to apply to the output:\n                                   'none' | 'mean' | 'sum'. 'mean': the mean of the output is taken.\n                                   'sum': the output will be summed. 'none': no reduction will be applied.\n                                   Default: 'mean'.\n        weight (Tensor, optional): Weights for each sample. Default: None.\n\n    Returns:\n        Tensor: Mean Squared Error loss (optionally weighted).\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            mse_loss,\n            (input, target, weight),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            weight=weight,\n        )\n\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n\n    if weight is not None:\n        if weight.size() != input.size():\n            raise ValueError(\"Weights and input must have the same size.\")\n\n        # Perform weighted MSE loss manually\n        squared_errors = torch.pow(expanded_input - expanded_target, 2)\n        weighted_squared_errors = squared_errors * weight\n\n        if reduction == \"none\":\n            return weighted_squared_errors\n        elif reduction == \"sum\":\n            return torch.sum(weighted_squared_errors)\n        elif reduction == \"mean\":\n            return torch.sum(weighted_squared_errors) / torch.sum(weight)\n        else:\n            raise ValueError(\n                f\"Invalid reduction mode: {reduction}. Expected one of 'none', 'mean', 'sum'.\"\n            )\n    else:\n        return torch._C._nn.mse_loss(\n            expanded_input, expanded_target, _Reduction.get_enum(reduction)\n        )\n\n\ndef margin_ranking_loss(\n    input1: Tensor,\n    input2: Tensor,\n    target: Tensor,\n    margin: float = 0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MarginRankingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input1, input2, target):\n        return handle_torch_function(\n            margin_ranking_loss,\n            (input1, input2, target),\n            input1,\n            input2,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if input1.dim() != input2.dim() or input1.dim() != target.dim():\n        raise RuntimeError(\n            f\"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n            f\"input1: {input1.size()}, input2: {input2.size()}, target: {target.size()} \"\n        )\n    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n\n\ndef hinge_embedding_loss(\n    input: Tensor,\n    target: Tensor,\n    margin: float = 1.0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.HingeEmbeddingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            hinge_embedding_loss,\n            (input, target),\n            input,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch.hinge_embedding_loss(input, target, margin, reduction_enum)\n\n\ndef multilabel_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiLabelMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            multilabel_margin_loss,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)\n\n\ndef soft_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.SoftMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            soft_margin_loss,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)\n\n\ndef multilabel_soft_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            multilabel_soft_margin_loss,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    loss = -(target * logsigmoid(input) + (1 - target) * logsigmoid(-input))\n\n    if weight is not None:\n        loss = loss * weight\n\n    class_dim = input.dim() - 1\n    C = input.size(class_dim)\n    loss = loss.sum(dim=class_dim) / C  # only return N loss values\n\n    if reduction == \"none\":\n        ret = loss\n    elif reduction == \"mean\":\n        ret = loss.mean()\n    elif reduction == \"sum\":\n        ret = loss.sum()\n    else:\n        ret = input\n        raise ValueError(reduction + \" is not valid\")\n    return ret\n\n\ndef cosine_embedding_loss(\n    input1: Tensor,\n    input2: Tensor,\n    target: Tensor,\n    margin: float = 0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.CosineEmbeddingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input1, input2, target):\n        return handle_torch_function(\n            cosine_embedding_loss,\n            (input1, input2, target),\n            input1,\n            input2,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\n\n\ndef multi_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    p: int = 1,\n    margin: float = 1.0,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            multi_margin_loss,\n            (input, target, weight),\n            input,\n            target,\n            p=p,\n            margin=margin,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if p != 1 and p != 2:\n        raise ValueError(\"only p == 1 and p == 2 supported\")\n    if weight is not None:\n        if weight.dim() != 1:\n            raise ValueError(\"weight must be one-dimensional\")\n\n    return torch._C._nn.multi_margin_loss(\n        input, target, p, margin, weight, reduction_enum\n    )\n\n\npixel_shuffle = _add_docstr(\n    torch.pixel_shuffle,\n    r\"\"\"\npixel_shuffle(input, upscale_factor) -> Tensor\n\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is the :attr:`upscale_factor`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n\"\"\",\n)\n\npixel_unshuffle = _add_docstr(\n    torch.pixel_unshuffle,\n    r\"\"\"\npixel_unshuffle(input, downscale_factor) -> Tensor\n\nReverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)` to a tensor of shape\n:math:`(*, C \\times r^2, H, W)`, where r is the :attr:`downscale_factor`.\n\nSee :class:`~torch.nn.PixelUnshuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    downscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 1, 12, 12)\n    >>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 9, 4, 4])\n\"\"\",\n)\n\nchannel_shuffle = _add_docstr(\n    torch.channel_shuffle,\n    r\"\"\"\nchannel_shuffle(input, groups) -> Tensor\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n\"\"\",\n)\n\nnative_channel_shuffle = _add_docstr(\n    torch.native_channel_shuffle,\n    r\"\"\"\nnative_channel_shuffle(input, groups) -> Tensor\n\nNative kernel level implementation of the `channel_shuffle`.\nThis function might become private in future releases, use with caution.\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.native_channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n\"\"\",\n)\n\n\n@_overload\ndef upsample(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[float] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n) -> Tensor:  # noqa: B950\n    pass\n\n\n@_overload\ndef upsample(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[float] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n) -> Tensor:  # noqa: B950\n    pass\n\n\ndef upsample(  # noqa: F811\n    input,\n    size=None,\n    scale_factor=None,\n    mode=\"nearest\",\n    align_corners=None,\n):\n    r\"\"\"Upsample input.\n\n    Provided tensor is upsampled to either the given :attr:`size` or the given\n    :attr:`scale_factor`\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with ``nn.functional.interpolate(...)``.\n\n    Note:\n        {backward_reproducibility_note}\n\n    The algorithm used for upsampling is determined by :attr:`mode`.\n\n    Currently temporal, spatial and volumetric upsampling are supported, i.e.\n    expected inputs are 3-D, 4-D or 5-D in shape.\n\n    The input dimensions are interpreted in the form:\n    `mini-batch x channels x [optional depth] x [optional height] x width`.\n\n    The modes available for upsampling are: `nearest`, `linear` (3D-only),\n    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\n    Args:\n        input (Tensor): the input tensor\n        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n            output spatial size.\n        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.\n        mode (str): algorithm used for upsampling:\n            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n            ``'trilinear'``. Default: ``'nearest'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input and output as squares rather than points.\n            If set to ``True``, the input and output tensors are aligned by the\n            center points of their corner pixels, preserving the values at the corner pixels.\n            If set to ``False``, the input and output tensors are aligned by the corner\n            points of their corner pixels, and the interpolation uses edge value padding\n            for out-of-boundary values, making this operation *independent* of input size\n            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n            Default: ``False``\n\n    .. note::\n        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n        negative values or values greater than 255 for images.\n        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n        when displaying the image.\n\n    .. warning::\n        With ``align_corners = True``, the linearly interpolating modes\n        (`linear`, `bilinear`, and `trilinear`) don't proportionally align the\n        output and input pixels, and thus the output values can depend on the\n        input size. This was the default behavior for these modes up to version\n        0.3.1. Since then, the default behavior is ``align_corners = False``.\n        See :class:`~torch.nn.Upsample` for concrete examples on how this\n        affects the outputs.\n\n    \"\"\"\n    warnings.warn(\n        \"`nn.functional.upsample` is deprecated. \"\n        \"Use `nn.functional.interpolate` instead.\",\n        stacklevel=2,\n    )\n    return interpolate(input, size, scale_factor, mode, align_corners)\n\n\nif upsample.__doc__:\n    upsample.__doc__ = upsample.__doc__.format(**reproducibility_notes)\n\n\ndef _is_integer(x) -> bool:\n    r\"\"\"Type check the input number is an integer.\n\n    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n    \"\"\"\n    if isinstance(x, (int, torch.SymInt)):\n        return True\n    if np is not None and isinstance(x, np.integer):\n        return True\n    return isinstance(x, Tensor) and not x.is_floating_point()\n\n\n@_overload\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[list[float]] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:  # noqa: B950\n    pass\n\n\n@_overload\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[list[float]] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:  # noqa: B950\n    pass\n\n\n@_overload\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[float] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:  # noqa: B950\n    pass\n\n\n@_overload\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[float] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:\n    pass\n\n\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[list[float]] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:  # noqa: B950\n    r\"\"\"Down/up samples the input.\n\n    Tensor interpolated to either the given :attr:`size` or the given\n    :attr:`scale_factor`\n\n    The algorithm used for interpolation is determined by :attr:`mode`.\n\n    Currently temporal, spatial and volumetric sampling are supported, i.e.\n    expected inputs are 3-D, 4-D or 5-D in shape.\n\n    The input dimensions are interpreted in the form:\n    `mini-batch x channels x [optional depth] x [optional height] x width`.\n\n    The modes available for resizing are: `nearest`, `linear` (3D-only),\n    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact`\n\n    Args:\n        input (Tensor): the input tensor\n        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n            output spatial size.\n        scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n            its length has to match the number of spatial dimensions; `input.dim() - 2`.\n        mode (str): algorithm used for upsampling:\n            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n            ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input and output as squares rather than points.\n            If set to ``True``, the input and output tensors are aligned by the\n            center points of their corner pixels, preserving the values at the corner pixels.\n            If set to ``False``, the input and output tensors are aligned by the corner\n            points of their corner pixels, and the interpolation uses edge value padding\n            for out-of-boundary values, making this operation *independent* of input size\n            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n            Default: ``False``\n        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n            interpolation calculation. If `recompute_scale_factor` is ``True``, then\n            `scale_factor` must be passed in and `scale_factor` is used to compute the\n            output `size`. The computed output `size` will be used to infer new scales for\n            the interpolation. Note that when `scale_factor` is floating-point, it may differ\n            from the recomputed `scale_factor` due to rounding and precision issues.\n            If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will\n            be used directly for interpolation. Default: ``None``.\n        antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias\n            option together with ``align_corners=False``, interpolation result would match Pillow\n            result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.\n\n    .. note::\n        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n        negative values or values greater than 255 for images.\n        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n        when displaying the image.\n\n    .. note::\n        Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation\n        algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep\n        backward compatibility.\n        Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.\n\n    .. note::\n        The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation\n        when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``.\n        For more details, please refer to the discussion in\n        `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_.\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            interpolate,\n            (input,),\n            input,\n            size=size,\n            scale_factor=scale_factor,\n            mode=mode,\n            align_corners=align_corners,\n            recompute_scale_factor=recompute_scale_factor,\n            antialias=antialias,\n        )\n\n    if mode in (\"nearest\", \"area\", \"nearest-exact\"):\n        if align_corners is not None:\n            raise ValueError(\n                \"align_corners option can only be set with the \"\n                \"interpolating modes: linear | bilinear | bicubic | trilinear\"\n            )\n    else:\n        if align_corners is None:\n            align_corners = False\n\n    dim = input.dim() - 2  # Number of spatial dimensions.\n\n    # Process size and scale_factor.  Validate that exactly one is set.\n    # Validate its length if it is a list, or expand it if it is a scalar.\n    # After this block, exactly one of output_size and scale_factors will\n    # be non-None, and it will be a list (or tuple).\n    if size is not None and scale_factor is not None:\n        raise ValueError(\"only one of size or scale_factor should be defined\")\n    elif size is not None:\n        assert scale_factor is None\n        scale_factors = None\n        if isinstance(size, (list, tuple)):\n            if len(size) != dim:\n                raise ValueError(\n                    \"Input and output must have the same number of spatial dimensions, but got \"\n                    f\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\n                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n                    \"output size in (o1, o2, ...,oK) format.\"\n                )\n            if not torch.jit.is_scripting():\n                if not all(_is_integer(x) for x in size):\n                    raise TypeError(\n                        \"expected size to be one of int or Tuple[int] or Tuple[int, int] or \"\n                        f\"Tuple[int, int, int], but got size with types {[type(x) for x in size]}\"\n                    )\n            output_size = size\n        else:\n            output_size = [size for _ in range(dim)]\n    elif scale_factor is not None:\n        assert size is None\n        output_size = None\n        if isinstance(scale_factor, (list, tuple)):\n            if len(scale_factor) != dim:\n                raise ValueError(\n                    \"Input and scale_factor must have the same number of spatial dimensions, but \"\n                    f\"got input with spatial dimensions of {list(input.shape[2:])} and \"\n                    f\"scale_factor of shape {scale_factor}. \"\n                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n                    \"scale_factor in (s1, s2, ...,sK) format.\"\n                )\n            scale_factors = scale_factor\n        else:\n            scale_factors = [scale_factor for _ in range(dim)]\n    else:\n        raise ValueError(\"either size or scale_factor should be defined\")\n\n    if (\n        recompute_scale_factor is not None\n        and recompute_scale_factor\n        and size is not None\n    ):\n        raise ValueError(\n            \"recompute_scale_factor is not meaningful with an explicit size.\"\n        )\n\n    # \"area\" mode always requires an explicit size rather than scale factor.\n    # Re-use the recompute_scale_factor code path.\n    if mode == \"area\" and output_size is None:\n        recompute_scale_factor = True\n\n    if recompute_scale_factor is not None and recompute_scale_factor:\n        # We compute output_size here, then un-set scale_factors.\n        # The C++ code will recompute it based on the (integer) output size.\n        assert scale_factors is not None\n        if not torch.jit.is_scripting() and torch._C._get_tracing_state():\n            # make scale_factor a tensor in tracing so constant doesn't get baked in\n            output_size = [\n                (\n                    torch.floor(\n                        (\n                            input.size(i + 2).float()\n                            * torch.tensor(scale_factors[i], dtype=torch.float32)\n                        ).float()\n                    )\n                )\n                for i in range(dim)\n            ]\n        elif torch.jit.is_scripting():\n            output_size = [\n                int(math.floor(float(input.size(i + 2)) * scale_factors[i]))\n                for i in range(dim)\n            ]\n        else:\n            output_size = [\n                _sym_int(input.size(i + 2) * scale_factors[i]) for i in range(dim)\n            ]\n        scale_factors = None\n\n    if antialias and not (mode in (\"bilinear\", \"bicubic\") and input.ndim == 4):\n        raise ValueError(\n            \"Anti-alias option is restricted to bilinear and bicubic modes and requires a 4-D tensor as input\"\n        )\n\n    if input.dim() == 3 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)\n    if input.dim() == 4 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)\n    if input.dim() == 5 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors)\n\n    if input.dim() == 3 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact1d(input, output_size, scale_factors)\n    if input.dim() == 4 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact2d(input, output_size, scale_factors)\n    if input.dim() == 5 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact3d(input, output_size, scale_factors)\n\n    if input.dim() == 3 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool1d(input, output_size)\n    if input.dim() == 4 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool2d(input, output_size)\n    if input.dim() == 5 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool3d(input, output_size)\n\n    if input.dim() == 3 and mode == \"linear\":\n        assert align_corners is not None\n        return torch._C._nn.upsample_linear1d(\n            input, output_size, align_corners, scale_factors\n        )\n    if input.dim() == 4 and mode == \"bilinear\":\n        assert align_corners is not None\n        if antialias:\n            return torch._C._nn._upsample_bilinear2d_aa(\n                input, output_size, align_corners, scale_factors\n            )\n        # Two levels are necessary to prevent TorchScript from touching\n        # are_deterministic_algorithms_enabled.\n        if not torch.jit.is_scripting():\n            if torch.are_deterministic_algorithms_enabled() and (\n                input.is_cuda or input.is_xpu\n            ):\n                # Use slow decomp whose backward will be in terms of index_put\n                # importlib is required because the import cannot be top level\n                # (cycle) and cannot be nested (TS doesn't support)\n                return importlib.import_module(\n                    \"torch._decomp.decompositions\"\n                )._upsample_linear_vec(input, output_size, align_corners, scale_factors)\n        return torch._C._nn.upsample_bilinear2d(\n            input, output_size, align_corners, scale_factors\n        )\n    if input.dim() == 5 and mode == \"trilinear\":\n        assert align_corners is not None\n        return torch._C._nn.upsample_trilinear3d(\n            input, output_size, align_corners, scale_factors\n        )\n    if input.dim() == 4 and mode == \"bicubic\":\n        assert align_corners is not None\n        if antialias:\n            return torch._C._nn._upsample_bicubic2d_aa(\n                input, output_size, align_corners, scale_factors\n            )\n        return torch._C._nn.upsample_bicubic2d(\n            input, output_size, align_corners, scale_factors\n        )\n\n    if input.dim() == 3 and mode == \"bilinear\":\n        raise NotImplementedError(\"Got 3D input, but bilinear mode needs 4D input\")\n    if input.dim() == 3 and mode == \"trilinear\":\n        raise NotImplementedError(\"Got 3D input, but trilinear mode needs 5D input\")\n    if input.dim() == 4 and mode == \"linear\":\n        raise NotImplementedError(\"Got 4D input, but linear mode needs 3D input\")\n    if input.dim() == 4 and mode == \"trilinear\":\n        raise NotImplementedError(\"Got 4D input, but trilinear mode needs 5D input\")\n    if input.dim() == 5 and mode == \"linear\":\n        raise NotImplementedError(\"Got 5D input, but linear mode needs 3D input\")\n    if input.dim() == 5 and mode == \"bilinear\":\n        raise NotImplementedError(\"Got 5D input, but bilinear mode needs 4D input\")\n\n    raise NotImplementedError(\n        \"Input Error: Only 3D, 4D and 5D input Tensors supported\"\n        f\" (got {input.dim()}D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\"\n        f\" (got {mode})\"\n    )\n\n\nif interpolate.__doc__:\n    interpolate.__doc__ = interpolate.__doc__.format(**reproducibility_notes)\n\n\n@_overload\ndef upsample_nearest(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[float] = None,\n) -> Tensor:\n    pass\n\n\n@_overload\ndef upsample_nearest(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[float] = None,\n) -> Tensor:\n    pass\n\n\ndef upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811\n    r\"\"\"Upsamples the input, using nearest neighbours' pixel values.\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.\n\n    Currently spatial and volumetric upsampling are supported (i.e. expected\n    inputs are 4 or 5 dimensional).\n\n    Args:\n        input (Tensor): input\n        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia\n            size.\n        scale_factor (int): multiplier for spatial size. Has to be an integer.\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    # DeprecationWarning is ignored by default\n    warnings.warn(\n        \"`nn.functional.upsample_nearest` is deprecated. \"\n        \"Use `nn.functional.interpolate` instead.\",\n        stacklevel=2,\n    )\n    return interpolate(input, size, scale_factor, mode=\"nearest\")\n\n\nif upsample_nearest.__doc__:\n    upsample_nearest.__doc__ = upsample_nearest.__doc__.format(**reproducibility_notes)\n\n\n@_overload\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[float] = None,\n) -> Tensor:\n    pass\n\n\n@_overload\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[float] = None,\n) -> Tensor:\n    pass\n\n\n@_overload\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor,\n    size: Optional[int] = None,\n    scale_factor: Optional[list[float]] = None,\n) -> Tensor:\n    pass\n\n\n@_overload\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor,\n    size: Optional[list[int]] = None,\n    scale_factor: Optional[list[float]] = None,\n) -> Tensor:\n    pass\n\n\ndef upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811\n    r\"\"\"Upsamples the input, using bilinear upsampling.\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with\n        ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.\n\n    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\n    volumetric (5 dimensional) inputs.\n\n    Args:\n        input (Tensor): input\n        size (int or Tuple[int, int]): output spatial size.\n        scale_factor (int or Tuple[int, int]): multiplier for spatial size\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    # DeprecationWarning is ignored by default\n    warnings.warn(\n        \"`nn.functional.upsample_bilinear` is deprecated. \"\n        \"Use `nn.functional.interpolate` instead.\",\n        stacklevel=2,\n    )\n    return interpolate(input, size, scale_factor, mode=\"bilinear\", align_corners=True)\n\n\nif upsample_bilinear.__doc__:\n    upsample_bilinear.__doc__ = upsample_bilinear.__doc__.format(\n        **reproducibility_notes\n    )\n\nGRID_SAMPLE_INTERPOLATION_MODES = {\n    \"bilinear\": 0,\n    \"nearest\": 1,\n    \"bicubic\": 2,\n}\n\nGRID_SAMPLE_PADDING_MODES = {\n    \"zeros\": 0,\n    \"border\": 1,\n    \"reflection\": 2,\n}\n\n\ndef grid_sample(\n    input: Tensor,\n    grid: Tensor,\n    mode: str = \"bilinear\",\n    padding_mode: str = \"zeros\",\n    align_corners: Optional[bool] = None,\n) -> Tensor:\n    r\"\"\"Compute grid sample.\n\n    Given an :attr:`input` and a flow-field :attr:`grid`, computes the\n    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\n\n    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are\n    supported.\n\n    In the spatial (4-D) case, for :attr:`input` with shape\n    :math:`(N, C, H_\\text{in}, W_\\text{in})` and :attr:`grid` with shape\n    :math:`(N, H_\\text{out}, W_\\text{out}, 2)`, the output will have shape\n    :math:`(N, C, H_\\text{out}, W_\\text{out})`.\n\n    For each output location ``output[n, :, h, w]``, the size-2 vector\n    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,\n    which are used to interpolate the output value ``output[n, :, h, w]``.\n    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the\n    ``x``, ``y``, ``z`` pixel locations for interpolating\n    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or\n    ``bilinear`` interpolation method to sample the input pixels.\n\n    :attr:`grid` specifies the sampling pixel locations normalized by the\n    :attr:`input` spatial dimensions. Therefore, it should have most values in\n    the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the\n    left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the\n    right-bottom pixel of :attr:`input`.\n\n    If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding\n    outputs are handled as defined by :attr:`padding_mode`. Options are\n\n        * ``padding_mode=\"zeros\"``: use ``0`` for out-of-bound grid locations,\n        * ``padding_mode=\"border\"``: use border values for out-of-bound grid locations,\n        * ``padding_mode=\"reflection\"``: use values at locations reflected by\n          the border for out-of-bound grid locations. For location far away\n          from the border, it will keep being reflected until becoming in bound,\n          e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``\n          and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes\n          ``x'' = -0.5``.\n\n    Note:\n        This function is often used in conjunction with :func:`affine_grid`\n        to build `Spatial Transformer Networks`_ .\n\n    Note:\n        When using the CUDA backend, this operation may induce nondeterministic\n        behaviour in its backward pass that is not easily switched off.\n        Please see the notes on :doc:`/notes/randomness` for background.\n\n    Note:\n        NaN values in :attr:`grid` would be interpreted as ``-1``.\n\n    Args:\n        input (Tensor): input of shape :math:`(N, C, H_\\text{in}, W_\\text{in})` (4-D case)\n                        or :math:`(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})` (5-D case)\n        grid (Tensor): flow-field of shape :math:`(N, H_\\text{out}, W_\\text{out}, 2)` (4-D case)\n                       or :math:`(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)` (5-D case)\n        mode (str): interpolation mode to calculate output values\n            ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'``\n            Note: ``mode='bicubic'`` supports only 4-D input.\n            When ``mode='bilinear'`` and the input is 5-D, the interpolation mode\n            used internally will actually be trilinear. However, when the input is 4-D,\n            the interpolation mode will legitimately be bilinear.\n        padding_mode (str): padding mode for outside grid values\n            ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input  as squares rather than points.\n            If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring\n            to the center points of the input's corner pixels. If set to ``False``, they\n            are instead considered as referring to the corner points of the input's corner\n            pixels, making the sampling more resolution agnostic.\n            This option parallels the ``align_corners`` option in\n            :func:`interpolate`, and so whichever option is used here\n            should also be used there to resize the input image before grid sampling.\n            Default: ``False``\n\n    Returns:\n        output (Tensor): output Tensor\n\n    .. _`Spatial Transformer Networks`:\n        https://arxiv.org/abs/1506.02025\n\n    .. warning::\n        When ``align_corners = True``, the grid positions depend on the pixel\n        size relative to the input image size, and so the locations sampled by\n        :func:`grid_sample` will differ for the same input given at different\n        resolutions (that is, after being upsampled or downsampled).\n        The default behavior up to version 1.2.0 was ``align_corners = True``.\n        Since then, the default behavior has been changed to ``align_corners = False``,\n        in order to bring it in line with the default for :func:`interpolate`.\n\n    .. note::\n        ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\\alpha=-0.75`.\n        The constant :math:`\\alpha` might be different from packages to packages.\n        For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively.\n        This algorithm may \"overshoot\" the range of values it's interpolating.\n        For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255].\n        Clamp the results with :func:`torch.clamp` to ensure they are within the valid range.\n    .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation\n    .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51\n    .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\n    \"\"\"\n    if has_torch_function_variadic(input, grid):\n        return handle_torch_function(\n            grid_sample,\n            (input, grid),\n            input,\n            grid,\n            mode=mode,\n            padding_mode=padding_mode,\n            align_corners=align_corners,\n        )\n    if mode != \"bilinear\" and mode != \"nearest\" and mode != \"bicubic\":\n        raise ValueError(\n            f\"nn.functional.grid_sample(): expected mode to be 'bilinear', 'nearest' or 'bicubic', but got: '{mode}'\"\n        )\n    if (\n        padding_mode != \"zeros\"\n        and padding_mode != \"border\"\n        and padding_mode != \"reflection\"\n    ):\n        raise ValueError(\n            \"nn.functional.grid_sample(): expected padding_mode \"\n            \"to be 'zeros', 'border', or 'reflection', \"\n            f\"but got: '{padding_mode}'\"\n        )\n\n    if mode == \"bilinear\":\n        mode_enum = 0\n    elif mode == \"nearest\":\n        mode_enum = 1\n    else:  # mode == 'bicubic'\n        mode_enum = 2\n\n    if padding_mode == \"zeros\":\n        padding_mode_enum = 0\n    elif padding_mode == \"border\":\n        padding_mode_enum = 1\n    else:  # padding_mode == 'reflection'\n        padding_mode_enum = 2\n\n    if align_corners is None:\n        warnings.warn(\n            \"Default grid_sample and affine_grid behavior has changed \"\n            \"to align_corners=False since 1.3.0. Please specify \"\n            \"align_corners=True if the old behavior is desired. \"\n            \"See the documentation of grid_sample for details.\"\n        )\n        align_corners = False\n\n    return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)\n\n\ndef affine_grid(\n    theta: Tensor,\n    size: list[int],\n    align_corners: Optional[bool] = None,\n) -> Tensor:\n    r\"\"\"Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices :attr:`theta`.\n\n    .. note::\n        This function is often used in conjunction with :func:`grid_sample`\n        to build `Spatial Transformer Networks`_ .\n\n    Args:\n        theta (Tensor): input batch of affine matrices with shape\n            (:math:`N \\times 2 \\times 3`) for 2D or\n            (:math:`N \\times 3 \\times 4`) for 3D\n        size (torch.Size): the target output image size.\n            (:math:`N \\times C \\times H \\times W` for 2D or\n            :math:`N \\times C \\times D \\times H \\times W` for 3D)\n            Example: torch.Size((32, 3, 24, 24))\n        align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``\n            to refer to the centers of the corner pixels rather than the image corners.\n            Refer to :func:`grid_sample` for a more complete description.\n            A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`\n            with the same setting for this option.\n            Default: ``False``\n\n    Returns:\n        output (Tensor): output Tensor of size (:math:`N \\times H \\times W \\times 2`)\n\n    .. _`Spatial Transformer Networks`:\n        https://arxiv.org/abs/1506.02025\n\n    .. warning::\n        When ``align_corners = True``, the grid positions depend on the pixel\n        size relative to the input image size, and so the locations sampled by\n        :func:`grid_sample` will differ for the same input given at different\n        resolutions (that is, after being upsampled or downsampled).\n        The default behavior up to version 1.2.0 was ``align_corners = True``.\n        Since then, the default behavior has been changed to ``align_corners = False``,\n        in order to bring it in line with the default for :func:`interpolate`.\n    .. warning::\n        When ``align_corners = True``, 2D affine transforms on 1D data and\n        3D affine transforms on 2D data (that is, when one of the spatial\n        dimensions has unit size) are ill-defined, and not an intended use case.\n        This is not a problem when ``align_corners = False``.\n        Up to version 1.2.0, all grid points along a unit dimension were\n        considered arbitrarily to be at ``-1``.\n        From version 1.3.0, under ``align_corners = True`` all grid points\n        along a unit dimension are considered to be at ``0``\n        (the center of the input image).\n    \"\"\"\n    if has_torch_function_unary(theta):\n        return handle_torch_function(\n            affine_grid, (theta,), theta, size, align_corners=align_corners\n        )\n    if align_corners is None:\n        warnings.warn(\n            \"Default grid_sample and affine_grid behavior has changed \"\n            \"to align_corners=False since 1.3.0. Please specify \"\n            \"align_corners=True if the old behavior is desired. \"\n            \"See the documentation of grid_sample for details.\"\n        )\n        align_corners = False\n\n    # enforce floating point dtype on theta\n    if not theta.is_floating_point():\n        raise ValueError(\n            f\"Expected theta to have floating point type, but got {theta.dtype}\"\n        )\n    # check that shapes and sizes match\n    if len(size) == 4:\n        if theta.dim() != 3 or theta.shape[-2] != 2 or theta.shape[-1] != 3:\n            raise ValueError(\n                f\"Expected a batch of 2D affine matrices of shape Nx2x3 for size {size}. Got {theta.shape}.\"\n            )\n        spatial_size = size[-2:]  # spatial dimension sizes\n    elif len(size) == 5:\n        if theta.dim() != 3 or theta.shape[-2] != 3 or theta.shape[-1] != 4:\n            raise ValueError(\n                f\"Expected a batch of 3D affine matrices of shape Nx3x4 for size {size}. Got {theta.shape}.\"\n            )\n        spatial_size = size[-3:]  # spatial dimension sizes\n    else:\n        raise NotImplementedError(\n            \"affine_grid only supports 4D and 5D sizes, \"\n            \"for 2D and 3D affine transforms, respectively. \"\n            f\"Got size {size}.\"\n        )\n    # check for empty span\n    if align_corners and min(spatial_size) == 1:\n        warnings.warn(\n            \"Since version 1.3.0, affine_grid behavior has changed \"\n            \"for unit-size grids when align_corners=True. \"\n            \"This is not an intended use case of affine_grid. \"\n            \"See the documentation of affine_grid for details.\"\n        )\n    elif min(size) <= 0:\n        raise ValueError(f\"Expected non-zero, positive output size. Got {size}\")\n\n    return torch.affine_grid_generator(theta, size, align_corners)\n\n\ndef pad(\n    input: Tensor,\n    pad: list[int],\n    mode: str = \"constant\",\n    value: Optional[float] = None,\n) -> Tensor:\n    r\"\"\"\n    pad(input, pad, mode=\"constant\", value=None) -> Tensor\n\n    Pads tensor.\n\n    Padding size:\n        The padding size by which to pad some dimensions of :attr:`input`\n        are described starting from the last dimension and moving forward.\n        :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n        of ``input`` will be padded.\n        For example, to pad only the last dimension of the input tensor, then\n        :attr:`pad` has the form\n        :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n        to pad the last 2 dimensions of the input tensor, then use\n        :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n        :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n        to pad the last 3 dimensions, use\n        :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n        :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n        :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n\n    Padding mode:\n        See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,\n        :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`\n        for concrete examples on how each of the padding modes works. Constant\n        padding is implemented for arbitrary dimensions. Circular, replicate and\n        reflection padding are implemented for padding the last 3 dimensions of a\n        4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,\n        or the last dimension of a 2D or 3D input tensor.\n\n    Note:\n        When using the CUDA backend, this operation may induce nondeterministic\n        behaviour in its backward pass that is not easily switched off.\n        Please see the notes on :doc:`/notes/randomness` for background.\n\n    Args:\n        input (Tensor): N-dimensional tensor\n        pad (tuple): m-elements tuple, where\n            :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n        mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n            Default: ``'constant'``\n        value: fill value for ``'constant'`` padding. Default: ``0``\n\n    Examples::\n\n        >>> t4d = torch.empty(3, 3, 4, 2)\n        >>> p1d = (1, 1) # pad last dim by 1 on each side\n        >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n        >>> print(out.size())\n        torch.Size([3, 3, 4, 4])\n        >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n        >>> out = F.pad(t4d, p2d, \"constant\", 0)\n        >>> print(out.size())\n        torch.Size([3, 3, 8, 4])\n        >>> t4d = torch.empty(3, 3, 4, 2)\n        >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n        >>> out = F.pad(t4d, p3d, \"constant\", 0)\n        >>> print(out.size())\n        torch.Size([3, 9, 7, 3])\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value\n        )\n    if not torch.jit.is_scripting():\n        if torch.are_deterministic_algorithms_enabled() and (\n            input.is_cuda or input.is_xpu\n        ):\n            if mode == \"replicate\":\n                # Use slow decomp whose backward will be in terms of index_put.\n                # importlib is required because the import cannot be top level\n                # (cycle) and cannot be nested (TS doesn't support)\n                return importlib.import_module(\n                    \"torch._decomp.decompositions\"\n                )._replication_pad(input, pad)\n    return torch._C._nn.pad(input, pad, mode, value)\n\n\n# TODO: Fix via https://github.com/pytorch/pytorch/issues/75798\npad.__module__ = \"torch.nn.functional\"\n\n# distance\n\n\npairwise_distance = _add_docstr(\n    torch.pairwise_distance,\n    r\"\"\"\npairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n\nSee :class:`torch.nn.PairwiseDistance` for details\n\"\"\",\n)\n\n\npdist = _add_docstr(\n    torch.pdist,\n    r\"\"\"\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to ``scipy.spatial.distance.pdist(input,\n'minkowski', p=p)`` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to ``scipy.spatial.distance.pdist(input, 'hamming') * M``.\nWhen :math:`p = \\infty`, the closest scipy function is\n``scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())``.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n\"\"\",\n)\n\n\ncosine_similarity = _add_docstr(\n    torch.cosine_similarity,\n    r\"\"\"\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\nto a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\nsqueezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 fewer dimension.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2, \\epsilon) \\cdot \\max(\\Vert x_2 \\Vert _2, \\epsilon)}\n\nSupports :ref:`type promotion <type-promotion-doc>`.\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input.\n    dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n\"\"\",\n)\n\n\none_hot = _add_docstr(\n    torch._C._nn.one_hot,\n    r\"\"\"\none_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int, optional):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor. Default: -1\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])\n\"\"\",\n)\n\n\ndef triplet_margin_loss(\n    anchor: Tensor,\n    positive: Tensor,\n    negative: Tensor,\n    margin: float = 1.0,\n    p: float = 2,\n    eps: float = 1e-6,\n    swap: bool = False,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Compute the triplet loss between given input tensors and a margin greater than 0.\n\n    See :class:`~torch.nn.TripletMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(anchor, positive, negative):\n        return handle_torch_function(\n            triplet_margin_loss,\n            (anchor, positive, negative),\n            anchor,\n            positive,\n            negative,\n            margin=margin,\n            p=p,\n            eps=eps,\n            swap=swap,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if margin <= 0:\n        raise ValueError(f\"margin must be greater than 0, got {margin}\")\n    return torch.triplet_margin_loss(\n        anchor, positive, negative, margin, p, eps, swap, reduction_enum\n    )\n\n\ndef triplet_margin_with_distance_loss(\n    anchor: Tensor,\n    positive: Tensor,\n    negative: Tensor,\n    *,\n    distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n    margin: float = 1.0,\n    swap: bool = False,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Compute the triplet margin loss for input tensors using a custom distance function.\n\n    See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.\n    \"\"\"\n    if torch.jit.is_scripting():\n        raise NotImplementedError(\n            \"F.triplet_margin_with_distance_loss does not support JIT scripting: \"\n            \"functions requiring Callables cannot be scripted.\"\n        )\n\n    if has_torch_function_variadic(anchor, positive, negative):\n        return handle_torch_function(\n            triplet_margin_with_distance_loss,\n            (anchor, positive, negative),\n            anchor,\n            positive,\n            negative,\n            distance_function=distance_function,\n            margin=margin,\n            swap=swap,\n            reduction=reduction,\n        )\n\n    # Check validity of reduction mode\n    if reduction not in (\"mean\", \"sum\", \"none\"):\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    # Check validity of margin\n    if margin <= 0:\n        raise ValueError(f\"margin must be greater than 0, got {margin}\")\n\n    # Check dimensions\n    a_dim = anchor.ndim\n    p_dim = positive.ndim\n    n_dim = negative.ndim\n    if not (a_dim == p_dim and p_dim == n_dim):\n        raise RuntimeError(\n            f\"The anchor, positive, and negative tensors are expected to have \"\n            f\"the same number of dimensions, but got: anchor {a_dim}D, \"\n            f\"positive {p_dim}D, and negative {n_dim}D inputs\"\n        )\n\n    # Calculate loss\n    if distance_function is None:\n        distance_function = torch.pairwise_distance\n\n    dist_pos = distance_function(anchor, positive)\n    dist_neg = distance_function(anchor, negative)\n    # The distance swap is described in the paper \"Learning shallow\n    # convolutional feature descriptors with triplet losses\" by V. Balntas, E.\n    # Riba et al.  If True, and if the positive example is closer to the\n    # negative example than the anchor is, swaps the positive example and the\n    # anchor in the loss computation.\n    if swap:\n        dist_swap = distance_function(positive, negative)\n        dist_neg = torch.minimum(dist_neg, dist_swap)\n    loss = torch.clamp_min(margin + dist_pos - dist_neg, 0)\n\n    # Apply reduction\n    if reduction == \"sum\":\n        return torch.sum(loss)\n    elif reduction == \"mean\":\n        return torch.mean(loss)\n    else:  # reduction == \"none\"\n        return loss\n\n\ndef normalize(\n    input: Tensor,\n    p: float = 2.0,\n    dim: int = 1,\n    eps: float = 1e-12,\n    out: Optional[Tensor] = None,\n) -> Tensor:\n    r\"\"\"Perform :math:`L_p` normalization of inputs over specified dimension.\n\n    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each\n    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as\n\n    .. math::\n        v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n\n    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.\n\n    Args:\n        input: input tensor of any shape\n        p (float): the exponent value in the norm formulation. Default: 2\n        dim (int or tuple of ints): the dimension to reduce. Default: 1\n        eps (float): small value to avoid division by zero. Default: 1e-12\n        out (Tensor, optional): the output tensor. If :attr:`out` is used, this\n                                operation won't be differentiable.\n    \"\"\"\n    if has_torch_function_variadic(input, out):\n        return handle_torch_function(\n            normalize, (input, out), input, p=p, dim=dim, eps=eps, out=out\n        )\n    if out is None:\n        denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)\n        return input / denom\n    else:\n        denom = input.norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)\n        return torch.div(input, denom, out=out)\n\n\ndef assert_int_or_pair(arg: list[int], arg_name: str, message: str) -> None:\n    assert isinstance(arg, int) or len(arg) == 2, message.format(arg_name)\n\n\ndef unfold(\n    input: Tensor,\n    kernel_size: BroadcastingList2[int],\n    dilation: BroadcastingList2[int] = 1,\n    padding: BroadcastingList2[int] = 0,\n    stride: BroadcastingList2[int] = 1,\n) -> Tensor:\n    r\"\"\"Extract sliding local blocks from a batched input tensor.\n\n    .. warning::\n        Currently, only 4-D input tensors (batched image-like tensors) are\n        supported.\n\n    .. warning::\n\n        More than one element of the unfolded tensor may refer to a single\n        memory location. As a result, in-place operations (especially ones that\n        are vectorized) may result in incorrect behavior. If you need to write\n        to the tensor, please clone it first.\n\n\n    See :class:`torch.nn.Unfold` for details\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            unfold,\n            (input,),\n            input,\n            kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n        )\n    return torch._C._nn.im2col(\n        input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)\n    )\n\n\ndef fold(\n    input: Tensor,\n    output_size: BroadcastingList2[int],\n    kernel_size: BroadcastingList2[int],\n    dilation: BroadcastingList2[int] = 1,\n    padding: BroadcastingList2[int] = 0,\n    stride: BroadcastingList2[int] = 1,\n) -> Tensor:\n    r\"\"\"Combine an array of sliding local blocks into a large containing tensor.\n\n    .. warning::\n        Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.\n\n    See :class:`torch.nn.Fold` for details\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            fold,\n            (input,),\n            input,\n            output_size,\n            kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n        )\n    return torch._C._nn.col2im(\n        input,\n        _pair(output_size),\n        _pair(kernel_size),\n        _pair(dilation),\n        _pair(padding),\n        _pair(stride),\n    )\n\n\n#\n# multihead attention\n#\n\n\ndef _in_projection_packed(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w: Tensor,\n    b: Optional[Tensor] = None,\n) -> list[Tensor]:\n    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n\n    Output is a triple containing projection tensors for query, key and value.\n\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n        w: projection weights for q, k and v, packed into a single tensor. Weights\n            are packed along dimension 0, in q, k, v order.\n        b: optional projection biases for q, k and v, packed into a single tensor\n            in q, k, v order.\n\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        - w: :math:`(E * 3, E)` where E is the embedding dimension\n        - b: :math:`E * 3` where E is the embedding dimension\n\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            proj = linear(q, w, b)\n            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n            proj = (\n                proj.unflatten(-1, (3, E))\n                .unsqueeze(0)\n                .transpose(0, -2)\n                .squeeze(-2)\n                .contiguous()\n            )\n            return proj[0], proj[1], proj[2]\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            q_proj = linear(q, w_q, b_q)\n            kv_proj = linear(k, w_kv, b_kv)\n            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n            kv_proj = (\n                kv_proj.unflatten(-1, (2, E))\n                .unsqueeze(0)\n                .transpose(0, -2)\n                .squeeze(-2)\n                .contiguous()\n            )\n            return (q_proj, kv_proj[0], kv_proj[1])\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\n\ndef _in_projection(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w_q: Tensor,\n    w_k: Tensor,\n    w_v: Tensor,\n    b_q: Optional[Tensor] = None,\n    b_k: Optional[Tensor] = None,\n    b_v: Optional[Tensor] = None,\n) -> tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"Perform the in-projection step of the attention operation.\n\n    This is simply a triple of linear projections,\n    with shape constraints on the weights which\n    ensure embedding dimension uniformity in the projected outputs.\n    Output is a triple containing projection tensors for query, key and value.\n\n    Args:\n        q, k, v: query, key and value tensors to be projected.\n        w_q, w_k, w_v: weights for q, k and v, respectively.\n        b_q, b_k, b_v: optional biases for q, k and v, respectively.\n\n    Shape:\n        Inputs:\n        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any\n            number of leading dimensions.\n        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any\n            number of leading dimensions.\n        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any\n            number of leading dimensions.\n        - w_q: :math:`(Eq, Eq)`\n        - w_k: :math:`(Eq, Ek)`\n        - w_v: :math:`(Eq, Ev)`\n        - b_q: :math:`(Eq)`\n        - b_k: :math:`(Eq)`\n        - b_v: :math:`(Eq)`\n\n        Output: in output triple :math:`(q', k', v')`,\n         - q': :math:`[Qdims..., Eq]`\n         - k': :math:`[Kdims..., Eq]`\n         - v': :math:`[Vdims..., Eq]`\n\n    \"\"\"\n    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)\n    assert w_q.shape == (\n        Eq,\n        Eq,\n    ), f\"expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}\"\n    assert w_k.shape == (\n        Eq,\n        Ek,\n    ), f\"expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}\"\n    assert w_v.shape == (\n        Eq,\n        Ev,\n    ), f\"expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}\"\n    assert b_q is None or b_q.shape == (\n        Eq,\n    ), f\"expecting query bias shape of {(Eq,)}, but got {b_q.shape}\"\n    assert b_k is None or b_k.shape == (\n        Eq,\n    ), f\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\n    assert b_v is None or b_v.shape == (\n        Eq,\n    ), f\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\n\nscaled_dot_product_attention = _add_docstr(\n    torch._C._nn.scaled_dot_product_attention,\n    r\"\"\"scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n        is_causal=False, scale=None, enable_gqa=False) -> Tensor:\n\n    Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\n    and applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\n    specified as a keyword argument.\n\n    .. code-block:: python\n\n        # Efficient implementation equivalent to the following:\n        def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n                is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n            L, S = query.size(-2), key.size(-2)\n            scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n            attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n            if is_causal:\n                assert attn_mask is None\n                temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n                attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n                attn_bias.to(query.dtype)\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n                else:\n                    attn_bias = attn_mask + attn_bias\n\n            if enable_gqa:\n                key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n                value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\n            attn_weight = query @ key.transpose(-2, -1) * scale_factor\n            attn_weight += attn_bias\n            attn_weight = torch.softmax(attn_weight, dim=-1)\n            attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n            return attn_weight @ value\n\n    .. warning::\n        This function is beta and subject to change.\n\n    .. warning::\n        This function always applies dropout according to the specified ``dropout_p`` argument.\n        To disable dropout during evaluation, be sure to pass a value of ``0.0`` when the module\n        that makes the function call is not in training mode.\n\n        For example:\n\n        .. code-block:: python\n\n            class MyModel(nn.Module):\n                def __init__(self, p=0.5):\n                    super().__init__()\n                    self.p = p\n\n                def forward(self, ...):\n                    return F.scaled_dot_product_attention(...,\n                        dropout_p=(self.p if self.training else 0.0))\n\n    Note:\n\n        There are currently three supported implementations of scaled dot product attention:\n\n            - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n            - `Memory-Efficient Attention`_\n            - A PyTorch implementation defined in C++ matching the above formulation\n\n        The function may call optimized kernels for improved performance when using the CUDA backend.\n        For all other backends, the PyTorch implementation will be used.\n\n        All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n        most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n        is used, the following functions are provided for enabling and disabling implementations.\n        The context manager is the preferred mechanism:\n\n            - :func:`torch.nn.attention.sdpa_kernel`: A context manager used to enable or disable any of the implementations.\n            - :func:`torch.backends.cuda.enable_flash_sdp`: Globally enables or disables FlashAttention.\n            - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Globally enables or disables  Memory-Efficient Attention.\n            - :func:`torch.backends.cuda.enable_math_sdp`: Globally enables or disables  the PyTorch C++ implementation.\n\n        Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n        disable the PyTorch C++ implementation using :func:`torch.nn.attention.sdpa_kernel`.\n        In the event that a fused implementation is not available, a warning will be raised with the\n        reasons why the fused implementation cannot run.\n\n        Due to the nature of fusing floating point operations, the output of this function may be different\n        depending on what backend kernel is chosen.\n        The c++ implementation supports torch.float64 and can be used when higher precision is required.\n        For math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\n    For more information please see :doc:`/notes/numerical_accuracy`\n\n        Grouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\n        and math kernel on CUDA tensor, and does not support Nested tensor.\n        Constraints for GQA:\n\n            - number_of_heads_query % number_of_heads_key_value == 0 and,\n            - number_of_heads_key == number_of_heads_value\n\n    Note:\n\n        {cudnn_reproducibility_note}\n    \"\"\".format(\n        **reproducibility_notes\n    )\n    + r\"\"\"\n    Args:\n        query (Tensor): Query tensor; shape :math:`(N, ..., Hq, L, E)`.\n        key (Tensor): Key tensor; shape :math:`(N, ..., H, S, E)`.\n        value (Tensor): Value tensor; shape :math:`(N, ..., H, S, Ev)`.\n        attn_mask (optional Tensor): Attention mask; shape must be broadcastable to the shape of attention weights,\n            which is :math:`(N,..., L, S)`. Two types of masks are supported.\n            A boolean mask where a value of True indicates that the element *should* take part in attention.\n            A float mask of the same type as query, key, value that is added to the attention score.\n        dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n        is_causal (bool): If set to true, the attention masking is a lower triangular matrix when the mask is a\n            square matrix. The attention masking has the form of the upper left causal bias due to the alignment\n            (see :class:`torch.nn.attention.bias.CausalBias`) when the mask is a non-square matrix.\n            An error is thrown if both attn_mask and is_causal are set.\n        scale (optional float, keyword-only): Scaling factor applied prior to softmax. If None, the default value is set\n            to :math:`\\frac{1}{\\sqrt{E}}`.\n        enable_gqa (bool): If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\n\n    Returns:\n        output (Tensor): Attention output; shape :math:`(N, ..., Hq, L, Ev)`.\n\n    Shape legend:\n        - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n        - :math:`S: \\text{Source sequence length}`\n        - :math:`L: \\text{Target sequence length}`\n        - :math:`E: \\text{Embedding dimension of the query and key}`\n        - :math:`Ev: \\text{Embedding dimension of the value}`\n        - :math:`Hq: \\text{Number of heads of query}`\n        - :math:`H: \\text{Number of heads of key and value}`\n\n    Examples:\n\n        >>> # Optionally use the context manager to ensure one of the fused kernels is run\n        >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n        >>>     F.scaled_dot_product_attention(query,key,value)\n\n\n        >>> # Sample for GQA for llama3\n        >>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n        >>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n        >>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n\n\n    .. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n        https://arxiv.org/abs/2307.08691\n    .. _Memory-Efficient Attention:\n        https://github.com/facebookresearch/xformers\n    .. _Grouped-Query Attention:\n        https://arxiv.org/pdf/2305.13245\n    \"\"\",\n)\n\n\ndef _mha_shape_check(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    key_padding_mask: Optional[Tensor],\n    attn_mask: Optional[Tensor],\n    num_heads: int,\n):\n    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n    # and returns if the input is batched or not.\n    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n\n    # Shape check.\n    if query.dim() == 3:\n        # Batched Inputs\n        is_batched = True\n        assert key.dim() == 3 and value.dim() == 3, (\n            \"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n        )\n        if key_padding_mask is not None:\n            assert key_padding_mask.dim() == 2, (\n                \"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n            )\n        if attn_mask is not None:\n            assert attn_mask.dim() in (2, 3), (\n                \"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n                f\" but found {attn_mask.dim()}-D tensor instead\"\n            )\n    elif query.dim() == 2:\n        # Unbatched Inputs\n        is_batched = False\n        assert key.dim() == 2 and value.dim() == 2, (\n            \"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n        )\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.dim() == 1, (\n                \"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n            )\n\n        if attn_mask is not None:\n            assert attn_mask.dim() in (2, 3), (\n                \"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n                f\" but found {attn_mask.dim()}-D tensor instead\"\n            )\n            if attn_mask.dim() == 3:\n                expected_shape = (num_heads, query.shape[0], key.shape[0])\n                assert (\n                    attn_mask.shape == expected_shape\n                ), f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\"\n    else:\n        raise AssertionError(\n            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\"\n        )\n\n    return is_batched\n\n\ndef _canonical_mask(\n    mask: Optional[Tensor],\n    mask_name: str,\n    other_type: Optional[DType],\n    other_name: str,\n    target_type: DType,\n    check_other: bool = True,\n) -> Optional[Tensor]:\n    if mask is not None:\n        _mask_dtype = mask.dtype\n        _mask_is_float = torch.is_floating_point(mask)\n        if _mask_dtype != torch.bool and not _mask_is_float:\n            raise AssertionError(\n                f\"only bool and floating types of {mask_name} are supported\"\n            )\n        if check_other and other_type is not None:\n            if _mask_dtype != other_type:\n                warnings.warn(\n                    f\"Support for mismatched {mask_name} and {other_name} \"\n                    \"is deprecated. Use same type for both instead.\"\n                )\n        if not _mask_is_float:\n            mask = torch.zeros_like(mask, dtype=target_type).masked_fill_(\n                mask, float(\"-inf\")\n            )\n    return mask\n\n\ndef _none_or_dtype(input: Optional[Tensor]) -> Optional[DType]:\n    if input is None:\n        return None\n    elif isinstance(input, torch.Tensor):\n        return input.dtype\n    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n\n\ndef _check_key_padding_mask(\n    key_padding_mask: torch.Tensor, src_len: int, bsz: int\n) -> None:\n    torch._check_with(\n        AssertionError,\n        key_padding_mask.shape[0] == bsz,\n        lambda: f\"Expected key_padded_mask.shape[0] to be {bsz}, but got {key_padding_mask.shape[0]}\",\n    )\n    torch._check_with(\n        AssertionError,\n        key_padding_mask.shape[1] == src_len,\n        lambda: f\"Expected key_padded_mask.shape[1] to be {src_len}, but got {key_padding_mask.shape[1]}\",\n    )\n\n\ndef multi_head_attention_forward(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    embed_dim_to_check: int,\n    num_heads: int,\n    in_proj_weight: Optional[Tensor],\n    in_proj_bias: Optional[Tensor],\n    bias_k: Optional[Tensor],\n    bias_v: Optional[Tensor],\n    add_zero_attn: bool,\n    dropout_p: float,\n    out_proj_weight: Tensor,\n    out_proj_bias: Optional[Tensor],\n    training: bool = True,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = True,\n    attn_mask: Optional[Tensor] = None,\n    use_separate_proj_weight: bool = False,\n    q_proj_weight: Optional[Tensor] = None,\n    k_proj_weight: Optional[Tensor] = None,\n    v_proj_weight: Optional[Tensor] = None,\n    static_k: Optional[Tensor] = None,\n    static_v: Optional[Tensor] = None,\n    average_attn_weights: bool = True,\n    is_causal: bool = False,\n) -> tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"Forward method for MultiHeadAttention.\n\n    .. note::\n        See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n        for an in depth discussion of the performant building blocks PyTorch offers for building your own\n        transformer layers.\n\n    See :class:`torch.nn.MultiheadAttention` for details.\n\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        embed_dim_to_check: total dimension of the model.\n        num_heads: parallel attention heads.\n        in_proj_weight, in_proj_bias: input projection weight and bias.\n        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        dropout_p: probability of an element to be zeroed.\n        out_proj_weight, out_proj_bias: the output projection weight and bias.\n        training: apply dropout if is ``True``.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. This is an binary mask. When the value is True,\n            the corresponding value on the attention layer will be filled with -inf.\n        need_weights: output attn_output_weights.\n            Default: `True`\n            Note: `needs_weight` defaults to `True`, but should be set to `False`\n            For best performance when attention weights are not needed.\n            *Setting needs_weights to `True`\n            leads to a significant performance degradation.*\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        is_causal: If specified, applies a causal mask as attention mask, and ignores\n            attn_mask for computing scaled dot product attention.\n            Default: ``False``.\n            .. warning::\n                is_causal is provides a hint that the attn_mask is the\n                causal mask.Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n        use_separate_proj_weight: the function accept the proj. weights for query, key,\n            and value in different forms. If false, in_proj_weight will be used, which is\n            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n        static_k, static_v: static key and value used for attention operators.\n        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.\n            Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect\n            when ``need_weights=True.``. Default: True\n\n\n    Shape:\n        Inputs:\n        - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.\n          If a FloatTensor is provided, it will be directly added to the value.\n          If a BoolTensor is provided, the positions with the\n          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n          positions. If a BoolTensor is provided, positions with ``True``\n          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n          is provided, it will be added to the attention weight.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n\n        Outputs:\n        - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns\n          attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n          head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.\n    \"\"\"\n    tens_ops = (\n        query,\n        key,\n        value,\n        in_proj_weight,\n        in_proj_bias,\n        bias_k,\n        bias_v,\n        out_proj_weight,\n        out_proj_bias,\n    )\n    if has_torch_function(tens_ops):\n        return handle_torch_function(\n            multi_head_attention_forward,\n            tens_ops,\n            query,\n            key,\n            value,\n            embed_dim_to_check,\n            num_heads,\n            in_proj_weight,\n            in_proj_bias,\n            bias_k,\n            bias_v,\n            add_zero_attn,\n            dropout_p,\n            out_proj_weight,\n            out_proj_bias,\n            training=training,\n            key_padding_mask=key_padding_mask,\n            need_weights=need_weights,\n            attn_mask=attn_mask,\n            is_causal=is_causal,\n            use_separate_proj_weight=use_separate_proj_weight,\n            q_proj_weight=q_proj_weight,\n            k_proj_weight=k_proj_weight,\n            v_proj_weight=v_proj_weight,\n            static_k=static_k,\n            static_v=static_v,\n            average_attn_weights=average_attn_weights,\n        )\n\n    is_batched = _mha_shape_check(\n        query, key, value, key_padding_mask, attn_mask, num_heads\n    )\n\n    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n    # is batched, run the computation and before returning squeeze the\n    # batch dimension so that the output doesn't carry this temporary batch dimension.\n    if not is_batched:\n        # unsqueeze if the input is unbatched\n        query = query.unsqueeze(1)\n        key = key.unsqueeze(1)\n        value = value.unsqueeze(1)\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.unsqueeze(0)\n\n    # set up shape vars\n    tgt_len, bsz, embed_dim = query.shape\n    src_len, _, _ = key.shape\n\n    key_padding_mask = _canonical_mask(\n        mask=key_padding_mask,\n        mask_name=\"key_padding_mask\",\n        other_type=_none_or_dtype(attn_mask),\n        other_name=\"attn_mask\",\n        target_type=query.dtype,\n    )\n\n    if is_causal and attn_mask is None:\n        raise RuntimeError(\n            \"Need attn_mask if specifying the is_causal hint. \"\n            \"You may use the Transformer module method \"\n            \"`generate_square_subsequent_mask` to create this mask.\"\n        )\n\n    if is_causal and key_padding_mask is None and not need_weights:\n        # when we have a kpm or need weights, we need attn_mask\n        # Otherwise, we use the is_causal hint go as is_causal\n        # indicator to SDPA.\n        attn_mask = None\n    else:\n        attn_mask = _canonical_mask(\n            mask=attn_mask,\n            mask_name=\"attn_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=query.dtype,\n            check_other=False,\n        )\n\n        if key_padding_mask is not None:\n            # We have the attn_mask, and use that to merge kpm into it.\n            # Turn off use of is_causal hint, as the merged mask is no\n            # longer causal.\n            is_causal = False\n\n    assert (\n        embed_dim == embed_dim_to_check\n    ), f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n    if isinstance(embed_dim, torch.Tensor):\n        # embed_dim can be a tensor when JIT tracing\n        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n    else:\n        head_dim = embed_dim // num_heads\n    assert (\n        head_dim * num_heads == embed_dim\n    ), f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n    if use_separate_proj_weight:\n        # allow MHA to have different embedding dimensions when separate projection weights are used\n        assert (\n            key.shape[:2] == value.shape[:2]\n        ), f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n    else:\n        assert (\n            key.shape == value.shape\n        ), f\"key shape {key.shape} does not match value shape {value.shape}\"\n\n    #\n    # compute in-projection\n    #\n    if not use_separate_proj_weight:\n        assert (\n            in_proj_weight is not None\n        ), \"use_separate_proj_weight is False but in_proj_weight is None\"\n        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n    else:\n        assert (\n            q_proj_weight is not None\n        ), \"use_separate_proj_weight is True but q_proj_weight is None\"\n        assert (\n            k_proj_weight is not None\n        ), \"use_separate_proj_weight is True but k_proj_weight is None\"\n        assert (\n            v_proj_weight is not None\n        ), \"use_separate_proj_weight is True but v_proj_weight is None\"\n        if in_proj_bias is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = in_proj_bias.chunk(3)\n        q, k, v = _in_projection(\n            query,\n            key,\n            value,\n            q_proj_weight,\n            k_proj_weight,\n            v_proj_weight,\n            b_q,\n            b_k,\n            b_v,\n        )\n\n    # prep attention mask\n\n    if attn_mask is not None:\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_2d_size = (tgt_len, src_len)\n            if attn_mask.shape != correct_2d_size:\n                raise RuntimeError(\n                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n                )\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n            if attn_mask.shape != correct_3d_size:\n                raise RuntimeError(\n                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n                )\n        else:\n            raise RuntimeError(\n                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n            )\n\n    # add bias along batch dimension (currently second)\n    if bias_k is not None and bias_v is not None:\n        assert static_k is None, \"bias cannot be added to static key.\"\n        assert static_v is None, \"bias cannot be added to static value.\"\n        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = pad(key_padding_mask, (0, 1))\n    else:\n        assert bias_k is None\n        assert bias_v is None\n\n    #\n    # reshape q, k, v for multihead attention and make them batch first\n    #\n    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n    if static_k is None:\n        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert (\n            static_k.size(0) == bsz * num_heads\n        ), f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n        assert (\n            static_k.size(2) == head_dim\n        ), f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n        k = static_k\n    if static_v is None:\n        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert (\n            static_v.size(0) == bsz * num_heads\n        ), f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n        assert (\n            static_v.size(2) == head_dim\n        ), f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n        v = static_v\n\n    # add zero attention along batch dimension (now first)\n    if add_zero_attn:\n        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n        k = torch.cat(\n            [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1\n        )\n        v = torch.cat(\n            [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1\n        )\n        if attn_mask is not None:\n            attn_mask = pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = pad(key_padding_mask, (0, 1))\n\n    # update source sequence length after adjustments\n    src_len = k.size(1)\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n            _check_key_padding_mask(key_padding_mask, src_len, bsz)\n\n        key_padding_mask = (\n            key_padding_mask.view(bsz, 1, 1, src_len)\n            .expand(-1, num_heads, -1, -1)\n            .reshape(bsz * num_heads, 1, src_len)\n        )\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        else:\n            attn_mask = attn_mask + key_padding_mask\n\n    # adjust dropout probability\n    if not training:\n        dropout_p = 0.0\n\n    #\n    # (deep breath) calculate attention and out projection\n    #\n\n    if need_weights:\n        _B, _Nt, E = q.shape\n        q_scaled = q * math.sqrt(1.0 / float(E))\n\n        assert not (\n            is_causal and attn_mask is None\n        ), \"FIXME: is_causal not implemented for need_weights\"\n\n        if attn_mask is not None:\n            attn_output_weights = torch.baddbmm(\n                attn_mask, q_scaled, k.transpose(-2, -1)\n            )\n        else:\n            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n        attn_output_weights = softmax(attn_output_weights, dim=-1)\n        if dropout_p > 0.0:\n            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n\n        attn_output = torch.bmm(attn_output_weights, v)\n\n        attn_output = (\n            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n        )\n        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n\n        # optionally average attention weights over heads\n        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n        if average_attn_weights:\n            attn_output_weights = attn_output_weights.mean(dim=1)\n\n        if not is_batched:\n            # squeeze the output if input was unbatched\n            attn_output = attn_output.squeeze(1)\n            attn_output_weights = attn_output_weights.squeeze(0)\n        return attn_output, attn_output_weights\n    else:\n        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n        # in order to match the input for SDPA of (N, num_heads, L, S)\n        if attn_mask is not None:\n            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n                attn_mask = attn_mask.unsqueeze(0)\n            else:\n                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n\n        q = q.view(bsz, num_heads, tgt_len, head_dim)\n        k = k.view(bsz, num_heads, src_len, head_dim)\n        v = v.view(bsz, num_heads, src_len, head_dim)\n\n        attn_output = scaled_dot_product_attention(\n            q, k, v, attn_mask, dropout_p, is_causal\n        )\n        attn_output = (\n            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n        )\n\n        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n        if not is_batched:\n            # squeeze the output if input was unbatched\n            attn_output = attn_output.squeeze(1)\n        return attn_output, None\n", 6422]}, "functions": {"Sequential.__iter__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:230)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py", 230], "Module.__getattr__ (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1927], "Linear.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:124)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py", 124], "Module._call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1755], "Module._wrapped_call_impl (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1747], "Tanh.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:391)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py", 391], "softmax (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\functional.py:2103)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\functional.py", 2103], "Softmax.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1671)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py", 1671], "Sequential.forward (C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:238)": ["C:\\Python\\venv\\p311\\Lib\\site-packages\\torch\\nn\\modules\\container.py", 238]}}}
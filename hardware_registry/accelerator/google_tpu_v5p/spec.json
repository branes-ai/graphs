{
  "id": "google_tpu_v5p",
  "vendor": "Google",
  "model": "Google TPU v5p",
  "device_type": "tpu",
  "product_category": "datacenter",
  "ops_per_clock": {
    "fp64": 0,
    "fp32": 209545,
    "fp16": 419091,
    "fp8": 838182,
    "fp4": 0,
    "bf16": 419091,
    "tf32": 0,
    "int64": 0,
    "int32": 209545,
    "int16": 419091,
    "int8": 838182,
    "int4": 0
  },
  "ops_per_clock_notes": {
    "bf16": "459 TFLOPS / 1100 MHz = 417,273 BF16 ops/clock (estimated)",
    "fp8": "~2x BF16 throughput = 838,182 FP8 ops/clock",
    "int8": "~2x BF16 throughput = 838,182 INT8 ops/clock",
    "note": "TPU v5p has 4 SparseCores per chip. 459 TFLOPS BF16 confirmed by Google Cloud docs."
  },
  "theoretical_peaks": {
    "fp64": 0.0,
    "fp32": 229500.0,
    "fp16": 459000.0,
    "fp8": 918000.0,
    "fp4": 0.0,
    "bf16": 459000.0,
    "tf32": 0.0,
    "int64": 0.0,
    "int32": 229500.0,
    "int16": 459000.0,
    "int8": 918000.0,
    "int4": 0.0
  },
  "theoretical_peaks_notes": "459 TFLOPS BF16 confirmed by Google Cloud. FP8/INT8 ~2x BF16. 2x+ improvement over v4.",
  "peak_bandwidth_gbps": 2765.0,
  "architecture": "TPU v5p",
  "compute_units": 4,
  "memory_gb": 95,
  "tdp_watts": 300,
  "base_clock_mhz": 1100.0,
  "boost_clock_mhz": 1100.0,
  "platform": "custom",
  "notes": "Google TPU v5p (2023). Performance-optimized with FP8 and 4 SparseCores per chip. 4nm process. 95GB HBM2e at 2.76 TB/s. 3D torus with 4800 Gbps ICI. 2x+ faster than v4 for LLM training."
}
{
  "id": "qualcomm_cloud_ai_100",
  "vendor": "Qualcomm",
  "model": "Qualcomm Cloud AI 100",
  "device_type": "accelerator",
  "product_category": "datacenter",
  "ops_per_clock": {
    "fp64": 0,
    "fp32": 0,
    "fp16": 128000,
    "fp8": 0,
    "fp4": 0,
    "bf16": 0,
    "tf32": 0,
    "int64": 0,
    "int32": 0,
    "int16": 128000,
    "int8": 256000,
    "int4": 0
  },
  "ops_per_clock_notes": {
    "int8": "16 AI cores x 8K INT8 MACs/clock x 2 (MAC) = 256,000 INT8 ops/clock",
    "fp16": "16 AI cores x 4K FP16 MACs/clock x 2 (MAC) = 128,000 FP16 ops/clock",
    "note": "7th gen Qualcomm AI cores with scalar, vector, and tensor compute units. High on-chip SRAM (144 MB total)."
  },
  "theoretical_peaks": {
    "fp64": 0.0,
    "fp32": 0.0,
    "fp16": 200000.0,
    "fp8": 0.0,
    "fp4": 0.0,
    "bf16": 0.0,
    "tf32": 0.0,
    "int64": 0.0,
    "int32": 0.0,
    "int16": 200000.0,
    "int8": 400000.0,
    "int4": 0.0
  },
  "theoretical_peaks_notes": "400 TOPS INT8 and 200 TFLOPS FP16 peak @ 75W PCIe card. Sustained ~350-363 TOPS at ~70W. 5.24 TOPS/W efficiency.",
  "peak_bandwidth_gbps": 136.5,
  "architecture": "Cloud AI 100",
  "compute_units": 16,
  "memory_gb": 32,
  "tdp_watts": 75,
  "base_clock_mhz": 1000.0,
  "boost_clock_mhz": 1560.0,
  "platform": "x86_64",
  "notes": "Qualcomm datacenter AI inference accelerator. 16 AI cores on 7nm. 144 MB on-chip SRAM. PCIe Gen4 x8. LPDDR4x 136 GB/s. Half-height half-length card. For inference workloads: NLP, vision, recommenders."
}

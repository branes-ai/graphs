================================================================================
Hardware Calibration: 12th Gen Intel(R) Core(TM) i7-12700K
================================================================================

Running pre-flight checks...

======================================================================
PRE-FLIGHT CHECKS
======================================================================

✓ CPU Governor: performance
    Current:  performance
    Expected: performance
✓ CPU Frequency: 1718 MHz idle (intel_pstate will boost under load)
    Current:  1718 MHz (idle)
    Expected: Up to 4900 MHz under load
✓ Turbo Boost: Enabled
✓ System Load: 0.0% (idle)
    Current:  0.0%
    Expected: < 5%
✓ Thermal State: 28°C (cool)
    Current:  28°C
    Expected: < 80°C

RESULT: PASSED
  System is ready for calibration.======================================================================
System Information:
  CPU: x86_64
  Cores: 12 physical, 20 logical
  Memory: 31.1 GB
  Python: 3.11.14
  NumPy: 2.2.6
  PyTorch: 2.7.1+cu126

Target Device: CPU
Framework:     PYTORCH

Querying CPU clock frequencies...
  CPU Freq: 1109 MHz (23% of max 4900 MHz)
  Governor: performance
  Turbo:    Enabled

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.7.1+cu126
  Device: CPU

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...   135.0 GB/s (150.7%)    0.12 ms
  Size    16 MB...   138.6 GB/s (154.7%)    0.24 ms
  Size    32 MB...    62.5 GB/s  (69.8%)    1.07 ms
  Size    64 MB...    48.1 GB/s  (53.7%)    2.79 ms
  Size   128 MB...    45.8 GB/s  (51.2%)    5.86 ms
  Size   256 MB...    46.9 GB/s  (52.3%)   11.45 ms
  Size   512 MB...    46.0 GB/s  (51.4%)   23.32 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...   139.4 GB/s (155.6%)    0.12 ms  |  17.4 GFLOPS
  Size    16 MB...    96.9 GB/s (108.1%)    0.35 ms  |  12.1 GFLOPS
  Size    32 MB...    71.5 GB/s  (79.8%)    0.94 ms  |  8.9 GFLOPS
  Size    64 MB...    54.6 GB/s  (60.9%)    2.46 ms  |  6.8 GFLOPS
  Size   128 MB...    53.6 GB/s  (59.9%)    5.00 ms  |  6.7 GFLOPS
  Size   256 MB...    53.9 GB/s  (60.1%)    9.96 ms  |  6.7 GFLOPS
  Size   512 MB...    53.9 GB/s  (60.2%)   19.92 ms  |  6.7 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...   144.9 GB/s (161.7%)    0.17 ms  |  12.1 GFLOPS
  Size    16 MB...    83.8 GB/s  (93.5%)    0.60 ms  |  7.0 GFLOPS
  Size    32 MB...    63.2 GB/s  (70.5%)    1.59 ms  |  5.3 GFLOPS
  Size    64 MB...    56.4 GB/s  (62.9%)    3.57 ms  |  4.7 GFLOPS
  Size   128 MB...    56.6 GB/s  (63.2%)    7.11 ms  |  4.7 GFLOPS
  Size   256 MB...    56.6 GB/s  (63.1%)   14.23 ms  |  4.7 GFLOPS
  Size   512 MB...    56.8 GB/s  (63.4%)   28.37 ms  |  4.7 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    85.5 GB/s  (95.4%)    0.29 ms  |  14.2 GFLOPS
  Size    16 MB...    71.6 GB/s  (79.9%)    0.70 ms  |  11.9 GFLOPS
  Size    32 MB...    47.3 GB/s  (52.8%)    2.13 ms  |  7.9 GFLOPS
  Size    64 MB...    36.9 GB/s  (41.1%)    5.46 ms  |  6.1 GFLOPS
  Size   128 MB...    36.6 GB/s  (40.8%)   11.00 ms  |  6.1 GFLOPS
  Size   256 MB...    36.7 GB/s  (41.0%)   21.93 ms  |  6.1 GFLOPS
  Size   512 MB...    36.7 GB/s  (41.0%)   43.87 ms  |  6.1 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY              138.6 GB/s     0.24 ms      154.7%  a[i] = b[i]
SCALE             139.4 GB/s     0.12 ms      155.6%  a[i] = q * b[i]
ADD               144.9 GB/s     0.17 ms      161.7%  a[i] = b[i] + c[i]
TRIAD              85.5 GB/s     0.29 ms       95.4%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 36.6 GB/s
================================================================================
STREAM Score (minimum bandwidth): 36.6 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, fp16, fp8, fp4, bf16, tf32, int64, int32, int16, int8, int4

Framework: PyTorch 2.7.1+cu126
  Device: CPU

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64                                           1.5 GFLOPS    0.00ms
    fp32                                           1.8 GFLOPS    0.00ms
    tf32                                           2.4 GFLOPS    0.00ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    int64                                           1.9 GIOPS    0.00ms
    int32                                           1.8 GIOPS    0.00ms
    int16                                           1.9 GIOPS    0.00ms
    int8                                            1.9 GIOPS    0.00ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64                                           9.2 GFLOPS    0.00ms
    fp32                                           7.8 GFLOPS    0.00ms
    tf32                                          11.0 GFLOPS    0.00ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.0 GFLOPS    0.02ms
    int64                                           5.4 GIOPS    0.00ms
    int32                                           5.4 GIOPS    0.00ms
    int16                                           5.4 GIOPS    0.00ms
    int8                                            5.5 GIOPS    0.00ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                          40.5 GFLOPS    0.00ms
    fp32                                          49.6 GFLOPS    0.00ms
    tf32                                          55.1 GFLOPS    0.00ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    1.05ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.1 GFLOPS    0.17ms
    int64                                           5.9 GIOPS    0.03ms
    int32                                           6.6 GIOPS    0.03ms
    int16                                           6.6 GIOPS    0.03ms
    int8                                            6.6 GIOPS    0.03ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                          20.5 GFLOPS    0.10ms
    fp32                                          58.4 GFLOPS    0.03ms
    tf32                                          58.2 GFLOPS    0.03ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    9.93ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.1 GFLOPS    1.76ms
    int64                                           5.7 GIOPS    0.35ms
    int32                                           7.1 GIOPS    0.28ms
    int16                                           7.2 GIOPS    0.28ms
    int8                                            7.2 GIOPS    0.28ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           8.7 GFLOPS    2.31ms
    fp32                                          23.1 GFLOPS    0.87ms
    tf32                                          20.2 GFLOPS    0.99ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)   98.36ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.1 GFLOPS   17.56ms
    int64                                           3.5 GIOPS    5.78ms
    int32                                           5.5 GIOPS    3.66ms
    int16                                           6.8 GIOPS    2.94ms
    int8                                            7.0 GIOPS    2.85ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    fp32        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    tf32        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    fp16        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.00ms
    int64        ⚠ SLOW (   0.4 GIOPS < 1.0 GFLOPS threshold)    0.00ms
    int32        ⚠ SLOW (   0.4 GIOPS < 1.0 GFLOPS threshold)    0.00ms
    int16        ⚠ SLOW (   0.5 GIOPS < 1.0 GFLOPS threshold)    0.00ms
    int8         ⚠ SLOW (   0.5 GIOPS < 1.0 GFLOPS threshold)    0.00ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64                                           2.7 GFLOPS    0.01ms
    fp32                                           3.2 GFLOPS    0.01ms
    tf32                                           3.1 GFLOPS    0.01ms
    fp16                                           2.9 GFLOPS    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.6 GFLOPS    0.01ms
    int64                                           1.6 GIOPS    0.01ms
    int32                                           3.2 GIOPS    0.01ms
    int16                                           3.7 GIOPS    0.01ms
    int8                                            3.9 GIOPS    0.01ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           4.7 GFLOPS    0.04ms
    fp32                                           7.7 GFLOPS    0.03ms
    tf32                                           7.7 GFLOPS    0.03ms
    fp16                                           8.2 GFLOPS    0.02ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           5.0 GFLOPS    0.04ms
    int64                                           3.2 GIOPS    0.06ms
    int32                                           7.5 GIOPS    0.03ms
    int16                                          11.1 GIOPS    0.02ms
    int8                                           13.1 GIOPS    0.02ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           4.3 GFLOPS    0.46ms
    fp32                                          10.3 GFLOPS    0.19ms
    tf32                                          10.3 GFLOPS    0.19ms
    fp16                                          24.2 GFLOPS    0.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          16.2 GFLOPS    0.12ms
    int64                                           4.3 GIOPS    0.47ms
    int32                                          10.3 GIOPS    0.19ms
    int16                                          24.8 GIOPS    0.08ms
    int8                                           52.8 GIOPS    0.04ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           1.4 GFLOPS   14.63ms
    fp32                                           2.7 GFLOPS    7.43ms
    tf32                                           2.8 GFLOPS    7.22ms
    fp16                                           5.7 GFLOPS    3.50ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           5.2 GFLOPS    3.88ms
    int64                                           1.3 GIOPS   14.83ms
    int32                                           2.6 GIOPS    7.58ms
    int16                                           5.6 GIOPS    3.55ms
    int8                                           26.4 GIOPS    0.76ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp32        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    tf32        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    int64        ⚠ SLOW (   0.3 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int32        ⚠ SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int16        ⚠ SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int8         ⚠ SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           1.3 GFLOPS    0.01ms
    fp32        ⚠ SLOW (   1.0 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    tf32        ⚠ SLOW (   1.0 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp16        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.01ms
    int64                                           1.1 GIOPS    0.01ms
    int32        ⚠ SLOW (   0.8 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int16        ⚠ SLOW (   0.8 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int8         ⚠ SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.01ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           4.3 GFLOPS    0.01ms
    fp32                                           3.8 GFLOPS    0.01ms
    tf32                                           3.8 GFLOPS    0.01ms
    fp16                                           3.2 GFLOPS    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           3.2 GFLOPS    0.01ms
    int64                                           2.8 GIOPS    0.01ms
    int32                                           2.5 GIOPS    0.01ms
    int16                                           2.3 GIOPS    0.01ms
    int8                                            2.4 GIOPS    0.01ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                          14.0 GFLOPS    0.01ms
    fp32                                          13.0 GFLOPS    0.01ms
    tf32                                          13.0 GFLOPS    0.01ms
    fp16                                          11.4 GFLOPS    0.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          11.5 GFLOPS    0.01ms
    int64                                           4.9 GIOPS    0.03ms
    int32                                           4.7 GIOPS    0.03ms
    int16                                           4.3 GIOPS    0.03ms
    int8                                            4.9 GIOPS    0.03ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          41.9 GFLOPS    0.01ms
    fp32                                          43.0 GFLOPS    0.01ms
    tf32                                          43.8 GFLOPS    0.01ms
    fp16                                          33.1 GFLOPS    0.02ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          36.0 GFLOPS    0.01ms
    int64                                           5.6 GIOPS    0.09ms
    int32                                           6.1 GIOPS    0.09ms
    int16                                           5.2 GIOPS    0.10ms
    int8                                            6.3 GIOPS    0.08ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          45.4 GFLOPS    0.05ms
    fp32                                         113.0 GFLOPS    0.02ms
    tf32                                         113.9 GFLOPS    0.02ms
    fp16                                          66.2 GFLOPS    0.03ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          77.5 GFLOPS    0.03ms
    int64                                           6.7 GIOPS    0.31ms
    int32                                           7.2 GIOPS    0.29ms
    int16                                           7.0 GIOPS    0.30ms
    int8                                            7.4 GIOPS    0.28ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          28.5 GFLOPS    0.29ms
    fp32                                          74.7 GFLOPS    0.11ms
    tf32                                          74.9 GFLOPS    0.11ms
    fp16                                          88.2 GFLOPS    0.10ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         115.5 GFLOPS    0.07ms
    int64                                           4.8 GIOPS    1.75ms
    int32                                           7.3 GIOPS    1.16ms
    int16                                           7.3 GIOPS    1.15ms
    int8                                            7.6 GIOPS    1.10ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64                                           8.4 GFLOPS    0.01ms
    fp32                                           7.1 GFLOPS    0.01ms
    tf32                                           6.4 GFLOPS    0.01ms
    fp16                                           2.5 GFLOPS    0.03ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.8 GFLOPS    0.02ms
    int64                                           2.3 GIOPS    0.03ms
    int32                                           5.8 GIOPS    0.01ms
    int16                                           6.1 GIOPS    0.01ms
    int8                                            5.9 GIOPS    0.01ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                          28.7 GFLOPS    0.02ms
    fp32                                          13.9 GFLOPS    0.04ms
    tf32                                          13.1 GFLOPS    0.04ms
    fp16                                           3.6 GFLOPS    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.5 GFLOPS    0.12ms
    int64                                           3.0 GIOPS    0.17ms
    int32                                          18.7 GIOPS    0.03ms
    int16                                          27.2 GIOPS    0.02ms
    int8                                           28.9 GIOPS    0.02ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                          95.7 GFLOPS    0.04ms
    fp32                                         149.8 GFLOPS    0.03ms
    tf32                                         163.6 GFLOPS    0.03ms
    fp16                                           3.7 GFLOPS    1.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.8 GFLOPS    0.87ms
    int64                                           3.2 GIOPS    1.31ms
    int32                                          30.2 GIOPS    0.14ms
    int16                                          57.0 GIOPS    0.07ms
    int8                                           69.0 GIOPS    0.06ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                          58.6 GFLOPS    0.57ms
    fp32                                         307.5 GFLOPS    0.11ms
    tf32                                         308.7 GFLOPS    0.11ms
    fp16                                           3.3 GFLOPS   10.10ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.0 GFLOPS    8.46ms
    int64                                           2.4 GIOPS   13.94ms
    int32                                          19.8 GIOPS    1.69ms
    int16                                          53.8 GIOPS    0.62ms
    int8                                           84.9 GIOPS    0.40ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                         157.9 GFLOPS    1.70ms
    fp32                                         341.7 GFLOPS    0.79ms
    tf32                                         296.3 GFLOPS    0.91ms
    fp16                                           3.5 GFLOPS   75.63ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.2 GFLOPS   63.69ms
    int64                                           1.1 GIOPS  250.14ms
    int32                                          24.2 GIOPS   11.12ms
    int16                                          39.6 GIOPS    6.78ms
    int8                                           30.3 GIOPS    8.87ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                         208.8 GFLOPS   10.29ms
    fp32                                         428.0 GFLOPS    5.02ms
    tf32                                         441.2 GFLOPS    4.87ms
    fp16                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int64                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int32                                          17.5 GIOPS  122.95ms
    int16                                          35.0 GIOPS   61.37ms
    int8                                           86.2 GIOPS   24.90ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                         243.8 GFLOPS   70.48ms
    fp32                                         718.2 GFLOPS   23.92ms
    tf32                                         488.4 GFLOPS   35.17ms
    fp16                                              SKIPPED  (Skipped (poor performance at size 1K: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                              SKIPPED  (Skipped (poor performance at size 1K: Timeout (>5s)))
    int64                                             SKIPPED  (Skipped (poor performance at size 1K: Timeout (>5s)))
    int32                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int16                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int8                                           67.8 GIOPS  253.24ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

================================================================================
Hardware Calibration: 12th Gen Intel(R) Core(TM) i7-12700K
Date: 2025-11-28T09:39:49.214918
================================================================================

Framework: PYTORCH
Device:    CPU

Theoretical Specifications:
  Peak GFLOPS (FP32): 1280.0
  Peak Bandwidth:     89.6 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                    16      138.6 GB/s     0.24 ms      154.7%  a[i] = b[i]
  SCALE                    8      139.4 GB/s     0.12 ms      155.6%  a[i] = q * b[i]
  ADD                      8      144.9 GB/s     0.17 ms      161.7%  a[i] = b[i] + c[i]
  TRIAD                    8       85.5 GB/s     0.29 ms       95.4%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 36.6 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                4.7       10.3       10.3       24.2        N/A        N/A       16.2        4.3       10.3       24.8       52.8        N/A             int8
  DOT                40.5       58.4       58.2        0.2        N/A        N/A        1.1        5.9        7.1        7.2        7.2        N/A             fp32
  GEMV               45.4      113.0      113.9       88.2        N/A        N/A      115.5        6.7        7.3        7.3        7.6        N/A             bf16
  GEMM              243.8      718.2      488.4        3.7        N/A        N/A        4.8        3.2       30.2       57.0       86.2        N/A             fp32

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64             100K         4.7 GFLOPS     0.04ms     0.33        0.7%
  fp32               1M        10.3 GFLOPS     0.19ms     0.17        0.8%
  tf32               1M        10.3 GFLOPS     0.19ms     0.17        0.0%
  fp16               1M        24.2 GFLOPS     0.08ms     0.33        0.0%
  bf16               1M        16.2 GFLOPS     0.12ms     0.33        0.0%
  int64              1M          4.3 GIOPS     0.47ms     0.33        0.0%
  int32              1M         10.3 GIOPS     0.19ms     0.33        0.0%
  int16              1M         24.8 GIOPS     0.08ms     0.33        0.0%
  int8               1M         52.8 GIOPS     0.04ms     0.33        0.0%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64             100K        40.5 GFLOPS     0.00ms     0.50        6.3%
  fp32               1M        58.4 GFLOPS     0.03ms     0.25        4.6%
  tf32               1M        58.2 GFLOPS     0.03ms     0.25        0.0%
  fp16              10M         0.2 GFLOPS    98.36ms     0.50        0.0%
  bf16             100K         1.1 GFLOPS     0.17ms     0.50        0.0%
  int64            100K          5.9 GIOPS     0.03ms     0.50        0.0%
  int32              1M          7.1 GIOPS     0.28ms     0.50        0.0%
  int16              1M          7.2 GIOPS     0.28ms     0.50        0.0%
  int8               1M          7.2 GIOPS     0.28ms     0.50        0.0%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               1K        45.4 GFLOPS     0.05ms     1.00        7.1%
  fp32               1K       113.0 GFLOPS     0.02ms     0.50        8.8%
  tf32               1K       113.9 GFLOPS     0.02ms     0.50        0.0%
  fp16               2K        88.2 GFLOPS     0.10ms     1.00        0.0%
  bf16               2K       115.5 GFLOPS     0.07ms     1.00        0.0%
  int64              1K          6.7 GIOPS     0.31ms     1.00        0.0%
  int32              2K          7.3 GIOPS     1.16ms     1.00        0.0%
  int16              2K          7.3 GIOPS     1.15ms     1.00        0.0%
  int8               2K          7.6 GIOPS     1.10ms     1.00        0.0%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K       243.8 GFLOPS    70.48ms   170.67       38.1%
  fp32               2K       718.2 GFLOPS    23.92ms   341.33       56.1%
  tf32               2K       488.4 GFLOPS    35.17ms   341.33        0.0%
  fp16              128         3.7 GFLOPS     1.15ms    42.67        0.0%
  bf16              128         4.8 GFLOPS     0.87ms    42.67        0.0%
  int64             128          3.2 GIOPS     1.31ms    10.67        0.0%
  int32             128         30.2 GIOPS     0.14ms    21.33        0.0%
  int16             128         57.0 GIOPS     0.07ms    42.67        0.0%
  int8               1K         86.2 GIOPS    24.90ms   682.67        0.0%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


(.venv) lanner@lanner-desktop:~/dev/branes/clones/graphs$ cli/calibrate_hardware.py --framework pytorch --force

================================================================================
Auto-Detecting Hardware
================================================================================
CPU:      Cortex-A78AE
Vendor:   ARM
Cores:    8 cores, 8 threads
GPU #1:  Orin (nvgpu)
Board:    NVIDIA Jetson Orin NX Engineering Reference Developer Kit
SoC:      Tegra T234

Matched to registry: jetson_orin_nx_16gb_gpu
  Confidence: 95%
  Device: GPU


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1046 MHz idle (DVFS will boost under load)
    Current:  1046 MHz (idle)
    Expected: Up to 1498 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[X] System Load: 30.6% (high - will affect results)
    Current:  30.6%
    Expected: < 5%
    Fix: Close other applications or wait for background tasks
[OK] Thermal State: 45°C (cool)
    Current:  45°C
    Expected: < 80°C
[OK] GPU Power Mode: 25W (power-limited profile)

RESULT: FAILED
  Calibration aborted. Fix issues above or use --force to override.

  Fix commands:
    Close other applications or wait for background tasks
======================================================================

[!] WARNING: Proceeding with calibration despite failed pre-flight checks.
  Results will be flagged as non-representative of peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 15.3 GB
  Python: 3.10.12
  NumPy: 1.26.4
  PyTorch: 2.4.0a0+3bcc3cddb5.nv24.07

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 1114 MHz (74% of max 1498 MHz)
  Governor: schedutil

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock (idle): 306 MHz
  SM Clock (load): 408 MHz (100% of max 408 MHz)
  Power Mode: 25W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    18.0 GB/s  (17.6%)    0.93 ms
  Size    16 MB...    30.8 GB/s  (30.2%)    1.09 ms
  Size    32 MB...    35.2 GB/s  (34.5%)    1.91 ms
  Size    64 MB...    40.5 GB/s  (39.7%)    3.31 ms
  Size   128 MB...    42.2 GB/s  (41.4%)    6.36 ms
  Size   256 MB...    39.0 GB/s  (38.2%)   13.78 ms
  Size   512 MB...    36.6 GB/s  (35.9%)   29.31 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    18.1 GB/s  (17.8%)    0.93 ms  |  2.3 GFLOPS
  Size    16 MB...    24.1 GB/s  (23.7%)    1.39 ms  |  3.0 GFLOPS
  Size    32 MB...    35.7 GB/s  (35.0%)    1.88 ms  |  4.5 GFLOPS
  Size    64 MB...    33.9 GB/s  (33.3%)    3.96 ms  |  4.2 GFLOPS
  Size   128 MB...    37.6 GB/s  (36.9%)    7.14 ms  |  4.7 GFLOPS
  Size   256 MB...    38.1 GB/s  (37.3%)   14.11 ms  |  4.8 GFLOPS
  Size   512 MB...    41.5 GB/s  (40.7%)   25.86 ms  |  5.2 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    30.5 GB/s  (29.9%)    0.83 ms  |  2.5 GFLOPS
  Size    16 MB...    35.3 GB/s  (34.6%)    1.43 ms  |  2.9 GFLOPS
  Size    32 MB...    34.6 GB/s  (33.9%)    2.91 ms  |  2.9 GFLOPS
  Size    64 MB...    34.2 GB/s  (33.6%)    5.88 ms  |  2.9 GFLOPS
  Size   128 MB...    42.9 GB/s  (42.1%)    9.38 ms  |  3.6 GFLOPS
  Size   256 MB...    43.2 GB/s  (42.4%)   18.63 ms  |  3.6 GFLOPS
  Size   512 MB...    42.1 GB/s  (41.3%)   38.25 ms  |  3.5 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    22.1 GB/s  (21.7%)    1.14 ms  |  3.7 GFLOPS
  Size    16 MB...    20.6 GB/s  (20.2%)    2.44 ms  |  3.4 GFLOPS
  Size    32 MB...    21.4 GB/s  (20.9%)    4.71 ms  |  3.6 GFLOPS
  Size    64 MB...    23.9 GB/s  (23.5%)    8.41 ms  |  4.0 GFLOPS
  Size   128 MB...    26.0 GB/s  (25.5%)   15.48 ms  |  4.3 GFLOPS
  Size   256 MB...    24.8 GB/s  (24.3%)   32.50 ms  |  4.1 GFLOPS
  Size   512 MB...    25.1 GB/s  (24.6%)   64.17 ms  |  4.2 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               42.2 GB/s     6.36 ms       41.4%  a[i] = b[i]
SCALE              41.5 GB/s    25.86 ms       40.7%  a[i] = q * b[i]
ADD                43.2 GB/s    18.63 ms       42.4%  a[i] = b[i] + c[i]
TRIAD              26.0 GB/s    15.48 ms       25.5%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 18.0 GB/s
================================================================================
STREAM Score (minimum bandwidth): 18.0 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.68ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.10ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64      [!] SLOW (   1.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp32      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    tf32                                           2.2 GFLOPS    0.09ms
    fp16                                           2.0 GFLOPS    0.10ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.3 GFLOPS    0.09ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           1.8 GFLOPS    1.13ms
    fp32                                           6.2 GFLOPS    0.32ms
    tf32                                           5.1 GFLOPS    0.39ms
    fp16                                          10.6 GFLOPS    0.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           5.8 GFLOPS    0.34ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           3.8 GFLOPS    5.23ms
    fp32                                           8.2 GFLOPS    2.43ms
    tf32                                           9.4 GFLOPS    2.12ms
    fp16                                          18.5 GFLOPS    1.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          17.8 GFLOPS    1.12ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.87ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.60ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.32ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.23ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.23ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.23ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.44ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.86ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.39ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.20ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.17ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp32                                           1.2 GFLOPS    0.17ms
    tf32      [!] SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.91ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.48ms
    int64      [!] SLOW (   0.7 GIOPS < 1.0 GFLOPS threshold)    0.27ms
    int32      [!] SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.23ms
    int16      [!] SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int8       [!] SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           1.1 GFLOPS    1.84ms
    fp32                                           1.0 GFLOPS    1.97ms
    tf32                                           2.5 GFLOPS    0.80ms
    fp16                                           4.3 GFLOPS    0.47ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.2 GFLOPS    0.47ms
    int64                                           1.2 GIOPS    1.64ms
    int32                                           1.9 GIOPS    1.04ms
    int16                                           4.3 GIOPS    0.46ms
    int8                                            7.6 GIOPS    0.26ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           1.2 GFLOPS   17.17ms
    fp32                                           3.1 GFLOPS    6.45ms
    tf32                                           2.9 GFLOPS    6.81ms
    fp16                                           6.1 GFLOPS    3.31ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           6.1 GFLOPS    3.30ms
    int64                                           1.3 GIOPS   15.94ms
    int32                                           3.1 GIOPS    6.47ms
    int16                                           6.1 GIOPS    3.29ms
    int8                                           11.1 GIOPS    1.81ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.45ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.67ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.87ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.71ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.27ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.27ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.40ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.90ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.58ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.50ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.17ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.48ms
    fp16      [!] SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.50ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           1.5 GFLOPS    0.34ms
    fp32                                           1.9 GFLOPS    0.28ms
    tf32      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.58ms
    fp16      [!] SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    1.29ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.8 GFLOPS    0.29ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           1.5 GFLOPS    1.42ms
    fp32                                           6.8 GFLOPS    0.31ms
    tf32                                           7.5 GFLOPS    0.28ms
    fp16                                           7.2 GFLOPS    0.29ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           7.1 GFLOPS    0.30ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           4.6 GFLOPS    1.83ms
    fp32                                           6.2 GFLOPS    1.35ms
    tf32                                          14.0 GFLOPS    0.60ms
    fp16                                          22.7 GFLOPS    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          23.3 GFLOPS    0.36ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.32ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.65ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.46ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           1.7 GFLOPS    0.30ms
    fp32                                           1.7 GFLOPS    0.31ms
    tf32                                           1.8 GFLOPS    0.29ms
    fp16                                           1.6 GFLOPS    0.32ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.8 GFLOPS    0.29ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           6.1 GFLOPS    0.69ms
    fp32                                           9.0 GFLOPS    0.46ms
    tf32                                          14.0 GFLOPS    0.30ms
    fp16                                          12.9 GFLOPS    0.33ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          13.1 GFLOPS    0.32ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           7.6 GFLOPS    4.42ms
    fp32                                         106.7 GFLOPS    0.31ms
    tf32                                         111.0 GFLOPS    0.30ms
    fp16                                          76.1 GFLOPS    0.44ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         105.9 GFLOPS    0.32ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          10.7 GFLOPS   25.03ms
    fp32                                         226.4 GFLOPS    1.19ms
    tf32                                         431.6 GFLOPS    0.62ms
    fp16                                         737.6 GFLOPS    0.36ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         596.4 GFLOPS    0.45ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp32                                         370.4 GFLOPS    5.80ms
    tf32                                         760.0 GFLOPS    2.83ms
    fp16                                        1444.6 GFLOPS    1.49ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1196.3 GFLOPS    1.80ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                              SKIPPED  (Skipped (poor performance at size 1K: Timeout (>5s)))
    fp32                                         474.8 GFLOPS   36.18ms
    tf32                                        1253.0 GFLOPS   13.71ms
    fp16                                        1947.1 GFLOPS    8.82ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2962.5 GFLOPS    5.80ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: /home/lanner/dev/branes/clones/graphs/src/graphs/hardware/calibration/profiles/jetson_orin_nx_16gb_gpu_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
Date: 2026-01-27T16:46:35.104833
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 1882.0
  Peak Bandwidth:     102.0 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       42.2 GB/s     6.36 ms       41.4%  a[i] = b[i]
  SCALE                  512       41.5 GB/s    25.86 ms       40.7%  a[i] = q * b[i]
  ADD                    256       43.2 GB/s    18.63 ms       42.4%  a[i] = b[i] + c[i]
  TRIAD                  128       26.0 GB/s    15.48 ms       25.5%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 18.0 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                1.2        3.1        2.9        6.1        N/A        N/A        6.1        1.3        3.1        6.1       11.1        N/A             int8
  DOT                 3.8        8.2        9.4       18.5        N/A        N/A       17.8        N/A        N/A        N/A        N/A        N/A             fp16
  GEMV                4.6        6.8       14.0       22.7        N/A        N/A       23.3        N/A        N/A        N/A        N/A        N/A             bf16
  GEMM               10.7      474.8     1253.0     1947.1        N/A        N/A     2962.5        N/A        N/A        N/A        N/A        N/A             bf16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         1.2 GFLOPS    17.17ms     0.33        2.0%
  fp32              10M         3.1 GFLOPS     6.45ms     0.17        0.2%
  tf32              10M         2.9 GFLOPS     6.81ms     0.17        0.1%
  fp16              10M         6.1 GFLOPS     3.31ms     0.33        0.1%
  bf16              10M         6.1 GFLOPS     3.30ms     0.33        0.1%
  int64             10M          1.3 GIOPS    15.94ms     0.33        0.0%
  int32             10M          3.1 GIOPS     6.47ms     0.33        0.2%
  int16             10M          6.1 GIOPS     3.29ms     0.33        0.1%
  int8              10M         11.1 GIOPS     1.81ms     0.33        0.1%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         3.8 GFLOPS     5.23ms     0.50        6.5%
  fp32              10M         8.2 GFLOPS     2.43ms     0.25        0.4%
  tf32              10M         9.4 GFLOPS     2.12ms     0.25        0.3%
  fp16              10M        18.5 GFLOPS     1.08ms     0.50        0.2%
  bf16              10M        17.8 GFLOPS     1.12ms     0.50        0.2%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         4.6 GFLOPS     1.83ms     1.00        7.8%
  fp32               1K         6.8 GFLOPS     0.31ms     0.50        0.4%
  tf32               2K        14.0 GFLOPS     0.60ms     0.50        0.4%
  fp16               2K        22.7 GFLOPS     0.37ms     1.00        0.3%
  bf16               2K        23.3 GFLOPS     0.36ms     1.00        0.3%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              512        10.7 GFLOPS    25.03ms    42.67       18.2%
  fp32               2K       474.8 GFLOPS    36.18ms   341.33       25.2%
  tf32               2K      1253.0 GFLOPS    13.71ms   341.33       33.3%
  fp16               2K      1947.1 GFLOPS     8.82ms   682.67       25.9%
  bf16               2K      2962.5 GFLOPS     5.80ms   682.67       39.4%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


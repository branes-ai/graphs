
================================================================================
Using Hardware from Registry: jetson_orin_nx_16gb_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin NX 16GB (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 960 MHz idle (DVFS will boost under load)
    Current:  960 MHz (idle)
    Expected: Up to 1190 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[X] System Load: 53.4% (high - will affect results)
    Current:  53.4%
    Expected: < 5%
    Fix: Close other applications or wait for background tasks
[OK] Thermal State: 42°C (cool)
    Current:  42°C
    Expected: < 80°C
[OK] GPU Power Mode: 10W (power-limited profile)

RESULT: FAILED
  Calibration aborted. Fix issues above or use --force to override.

  Fix commands:
    Close other applications or wait for background tasks
======================================================================

[!] WARNING: Proceeding with calibration despite failed pre-flight checks.
  Results will be flagged as non-representative of peak performance.

System Information:
  CPU: aarch64
  Cores: 4 physical, 4 logical
  Memory: 15.3 GB
  Python: 3.10.12
  NumPy: 1.26.4
  PyTorch: 2.4.0a0+3bcc3cddb5.nv24.07

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 960 MHz (81% of max 1190 MHz)
  Governor: schedutil

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock (idle): 306 MHz
  SM Clock (load): 612 MHz (100% of max 612 MHz)
  Power Mode: 10W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    17.0 GB/s  (16.7%)    0.98 ms
  Size    16 MB...    30.2 GB/s  (29.7%)    1.11 ms
  Size    32 MB...    31.7 GB/s  (31.0%)    2.12 ms
  Size    64 MB...    32.5 GB/s  (31.9%)    4.13 ms
  Size   128 MB...    32.5 GB/s  (31.9%)    8.26 ms
  Size   256 MB...    30.5 GB/s  (29.9%)   17.62 ms
  Size   512 MB...    30.6 GB/s  (30.0%)   35.09 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    39.5 GB/s  (38.7%)    0.43 ms  |  4.9 GFLOPS
  Size    16 MB...    43.6 GB/s  (42.8%)    0.77 ms  |  5.5 GFLOPS
  Size    32 MB...    52.8 GB/s  (51.8%)    1.27 ms  |  6.6 GFLOPS
  Size    64 MB...    57.6 GB/s  (56.5%)    2.33 ms  |  7.2 GFLOPS
  Size   128 MB...    59.4 GB/s  (58.2%)    4.52 ms  |  7.4 GFLOPS
  Size   256 MB...    51.3 GB/s  (50.3%)   10.46 ms  |  6.4 GFLOPS
  Size   512 MB...    53.2 GB/s  (52.1%)   20.19 ms  |  6.6 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    41.9 GB/s  (41.1%)    0.60 ms  |  3.5 GFLOPS
  Size    16 MB...    50.8 GB/s  (49.8%)    0.99 ms  |  4.2 GFLOPS
  Size    32 MB...    58.2 GB/s  (57.1%)    1.73 ms  |  4.9 GFLOPS
  Size    64 MB...    60.0 GB/s  (58.8%)    3.36 ms  |  5.0 GFLOPS
  Size   128 MB...    58.8 GB/s  (57.7%)    6.84 ms  |  4.9 GFLOPS
  Size   256 MB...    60.5 GB/s  (59.3%)   13.31 ms  |  5.0 GFLOPS
  Size   512 MB...    59.7 GB/s  (58.6%)   26.96 ms  |  5.0 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    28.6 GB/s  (28.1%)    0.88 ms  |  4.8 GFLOPS
  Size    16 MB...    32.3 GB/s  (31.6%)    1.56 ms  |  5.4 GFLOPS
  Size    32 MB...    31.3 GB/s  (30.7%)    3.22 ms  |  5.2 GFLOPS
  Size    64 MB...    32.7 GB/s  (32.0%)    6.17 ms  |  5.4 GFLOPS
  Size   128 MB...    33.5 GB/s  (32.9%)   12.00 ms  |  5.6 GFLOPS
  Size   256 MB...    34.7 GB/s  (34.0%)   23.22 ms  |  5.8 GFLOPS
  Size   512 MB...    34.5 GB/s  (33.9%)   46.63 ms  |  5.8 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               32.5 GB/s     4.13 ms       31.9%  a[i] = b[i]
SCALE              59.4 GB/s     4.52 ms       58.2%  a[i] = q * b[i]
ADD                60.5 GB/s    13.31 ms       59.3%  a[i] = b[i] + c[i]
TRIAD              34.7 GB/s    23.22 ms       34.0%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 17.0 GB/s
================================================================================
STREAM Score (minimum bandwidth): 17.0 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.18ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.18ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.1 GFLOPS    0.19ms
    fp32                                           1.1 GFLOPS    0.19ms
    tf32                                           1.2 GFLOPS    0.17ms
    fp16                                           1.1 GFLOPS    0.18ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.1 GFLOPS    0.18ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           4.8 GFLOPS    0.42ms
    fp32                                           7.9 GFLOPS    0.25ms
    tf32                                           6.1 GFLOPS    0.33ms
    fp16                                           8.8 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.9 GFLOPS    0.23ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           5.7 GFLOPS    3.52ms
    fp32                                          10.9 GFLOPS    1.84ms
    tf32                                          13.0 GFLOPS    1.53ms
    fp16                                          15.3 GFLOPS    1.31ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          15.7 GFLOPS    1.27ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.20ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.20ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp32      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    tf32      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp16                                           1.0 GFLOPS    0.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   1.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    int64      [!] SLOW (   1.0 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int32      [!] SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int16      [!] SLOW (   1.0 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int8       [!] SLOW (   1.0 GIOPS < 1.0 GFLOPS threshold)    0.21ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           1.4 GFLOPS    1.47ms
    fp32                                           2.6 GFLOPS    0.76ms
    tf32                                           2.6 GFLOPS    0.76ms
    fp16                                           4.2 GFLOPS    0.48ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.3 GFLOPS    0.47ms
    int64                                           1.5 GIOPS    1.36ms
    int32                                           2.7 GIOPS    0.73ms
    int16                                           4.0 GIOPS    0.50ms
    int8                                            5.1 GIOPS    0.39ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           1.5 GFLOPS   13.16ms
    fp32                                           2.9 GFLOPS    6.89ms
    tf32                                           3.0 GFLOPS    6.66ms
    fp16                                           5.8 GFLOPS    3.42ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           5.7 GFLOPS    3.49ms
    int64                                           1.6 GIOPS   12.73ms
    int32                                           3.0 GIOPS    6.63ms
    int16                                           5.9 GIOPS    3.40ms
    int8                                            8.2 GIOPS    2.42ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.38ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.53ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.38ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.39ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.38ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.60ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.57ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.58ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.58ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.59ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.56ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.57ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.57ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.39ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.39ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           1.4 GFLOPS    0.38ms
    fp32                                           1.4 GFLOPS    0.37ms
    tf32                                           1.4 GFLOPS    0.38ms
    fp16                                           1.3 GFLOPS    0.40ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.3 GFLOPS    0.40ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           1.7 GFLOPS    1.25ms
    fp32                                           2.0 GFLOPS    1.04ms
    tf32                                           3.6 GFLOPS    0.58ms
    fp16                                           3.4 GFLOPS    0.61ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.9 GFLOPS    1.11ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           2.6 GFLOPS    3.24ms
    fp32                                           8.7 GFLOPS    0.97ms
    tf32                                           9.5 GFLOPS    0.88ms
    fp16                                          16.2 GFLOPS    0.52ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          16.2 GFLOPS    0.52ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.40ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.60ms
    tf32                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.39ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.85ms
    fp32      [!] SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.57ms
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.62ms
    int64                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           3.3 GFLOPS    1.28ms
    fp32                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           7.4 GFLOPS    0.57ms
    int64                                             SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           5.5 GFLOPS    6.10ms
    fp32                                              SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          88.7 GFLOPS    0.38ms
    int64                                             SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           8.2 GFLOPS   32.83ms
    fp32                                              SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         426.4 GFLOPS    0.63ms
    int64                                             SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp32                                              SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1022.7 GFLOPS    2.10ms
    int64                                             SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                              SKIPPED  (Skipped (poor performance at size 1K: Timeout (>5s)))
    fp32                                              SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    tf32                                              SKIPPED  (Skipped (poor performance at size 32: Timeout (>5s)))
    fp16                                              SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2170.3 GFLOPS    7.92ms
    int64                                             SKIPPED  (Skipped (poor performance at size 64: Timeout (>5s)))
    int32                                             SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    int16                                             SKIPPED  (Skipped (poor performance at size 128: Timeout (>5s)))
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: /home/lanner/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nx_16gb_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
Date: 2026-01-28T09:56:46.035468
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 1882.0
  Peak Bandwidth:     102.0 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                    64       32.5 GB/s     4.13 ms       31.9%  a[i] = b[i]
  SCALE                  128       59.4 GB/s     4.52 ms       58.2%  a[i] = q * b[i]
  ADD                    256       60.5 GB/s    13.31 ms       59.3%  a[i] = b[i] + c[i]
  TRIAD                  256       34.7 GB/s    23.22 ms       34.0%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 17.0 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                1.5        2.9        3.0        5.8        N/A        N/A        5.7        1.6        3.0        5.9        8.2        N/A             int8
  DOT                 5.7       10.9       13.0       15.3        N/A        N/A       15.7        N/A        N/A        N/A        N/A        N/A             bf16
  GEMV                2.6        8.7        9.5       16.2        N/A        N/A       16.2        N/A        N/A        N/A        N/A        N/A             fp16
  GEMM                8.2        0.9        N/A        0.2        N/A        N/A     2170.3        N/A        N/A        N/A        N/A        N/A             bf16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         1.5 GFLOPS    13.16ms     0.33        2.6%
  fp32              10M         2.9 GFLOPS     6.89ms     0.17        0.2%
  tf32              10M         3.0 GFLOPS     6.66ms     0.17        0.1%
  fp16              10M         5.8 GFLOPS     3.42ms     0.33        0.1%
  bf16              10M         5.7 GFLOPS     3.49ms     0.33        0.1%
  int64             10M          1.6 GIOPS    12.73ms     0.33        0.0%
  int32             10M          3.0 GIOPS     6.63ms     0.33        0.2%
  int16             10M          5.9 GIOPS     3.40ms     0.33        0.1%
  int8              10M          8.2 GIOPS     2.42ms     0.33        0.1%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         5.7 GFLOPS     3.52ms     0.50        9.6%
  fp32              10M        10.9 GFLOPS     1.84ms     0.25        0.6%
  tf32              10M        13.0 GFLOPS     1.53ms     0.25        0.3%
  fp16              10M        15.3 GFLOPS     1.31ms     0.50        0.2%
  bf16              10M        15.7 GFLOPS     1.27ms     0.50        0.2%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         2.6 GFLOPS     3.24ms     1.00        4.4%
  fp32               2K         8.7 GFLOPS     0.97ms     0.50        0.5%
  tf32               2K         9.5 GFLOPS     0.88ms     0.50        0.3%
  fp16               2K        16.2 GFLOPS     0.52ms     1.00        0.2%
  bf16               2K        16.2 GFLOPS     0.52ms     1.00        0.2%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              512         8.2 GFLOPS    32.83ms    42.67       13.9%
  fp32               64         0.9 GFLOPS     0.57ms    10.67        0.0%
  fp16               32         0.2 GFLOPS     0.37ms    10.67        0.0%
  bf16               2K      2170.3 GFLOPS     7.92ms   682.67       28.8%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: /home/lanner/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nx_16gb_gpu/calibrations/10W_612MHz_pytorch.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_nx_16gb_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_nx_16gb_gpu


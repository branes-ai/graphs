
================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 2202 MHz (100% of max)
    Current:  2202 MHz
    Expected: >= 1981 MHz (90%)
[-] Turbo Boost: Could not determine (not Intel pstate)
[OK] System Load: 0.0% (idle)
    Current:  0.0%
    Expected: < 5%
[OK] Thermal State: 43°C (cool)
    Current:  43°C
    Expected: < 80°C
[OK] GPU Power Mode: MAXN (maximum performance)

RESULT: PASSED
  System is ready for calibration.
======================================================================
System Information:
  CPU: aarch64
  Cores: 12 physical, 12 logical
  Memory: 61.3 GB
  Python: 3.8.10
  NumPy: 1.24.4
  PyTorch: 2.1.0a0+41361538.nv23.06

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 781 MHz (35% of max 2202 MHz)
  Governor: schedutil
  Power Mode: MAXN (nvpmodel 0)
  Clock Policy: dvfs

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock: 306 MHz (24% of max 1300 MHz)
  Power Mode: MAXN
  Clock Policy: dvfs

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    43.7 GB/s  (21.4%)    0.38 ms
  Size    16 MB...    49.0 GB/s  (23.9%)    0.68 ms
  Size    32 MB...    48.0 GB/s  (23.4%)    1.40 ms
  Size    64 MB...    55.6 GB/s  (27.2%)    2.41 ms
  Size   128 MB...    60.2 GB/s  (29.4%)    4.46 ms
  Size   256 MB...    63.0 GB/s  (30.8%)    8.52 ms
  Size   512 MB...    64.1 GB/s  (31.3%)   16.74 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    25.9 GB/s  (12.6%)    0.65 ms  |  3.2 GFLOPS
  Size    16 MB...    42.1 GB/s  (20.6%)    0.80 ms  |  5.3 GFLOPS
  Size    32 MB...    49.2 GB/s  (24.0%)    1.36 ms  |  6.2 GFLOPS
  Size    64 MB...    58.1 GB/s  (28.3%)    2.31 ms  |  7.3 GFLOPS
  Size   128 MB...    60.5 GB/s  (29.5%)    4.44 ms  |  7.6 GFLOPS
  Size   256 MB...    61.9 GB/s  (30.2%)    8.67 ms  |  7.7 GFLOPS
  Size   512 MB...    64.2 GB/s  (31.4%)   16.71 ms  |  8.0 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    34.8 GB/s  (17.0%)    0.72 ms  |  2.9 GFLOPS
  Size    16 MB...    45.7 GB/s  (22.3%)    1.10 ms  |  3.8 GFLOPS
  Size    32 MB...    51.2 GB/s  (25.0%)    1.96 ms  |  4.3 GFLOPS
  Size    64 MB...    58.1 GB/s  (28.4%)    3.47 ms  |  4.8 GFLOPS
  Size   128 MB...    60.6 GB/s  (29.6%)    6.64 ms  |  5.1 GFLOPS
  Size   256 MB...    62.7 GB/s  (30.6%)   12.85 ms  |  5.2 GFLOPS
  Size   512 MB...   162.6 GB/s  (79.4%)    9.91 ms  |  13.5 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    58.0 GB/s  (28.3%)    0.43 ms  |  9.7 GFLOPS
  Size    16 MB...    78.4 GB/s  (38.3%)    0.64 ms  |  13.1 GFLOPS
  Size    32 MB...    89.0 GB/s  (43.5%)    1.13 ms  |  14.8 GFLOPS
  Size    64 MB...    99.1 GB/s  (48.4%)    2.03 ms  |  16.5 GFLOPS
  Size   128 MB...   106.1 GB/s  (51.8%)    3.80 ms  |  17.7 GFLOPS
  Size   256 MB...   110.5 GB/s  (53.9%)    7.29 ms  |  18.4 GFLOPS
  Size   512 MB...   112.3 GB/s  (54.8%)   14.35 ms  |  18.7 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               64.1 GB/s    16.74 ms       31.3%  a[i] = b[i]
SCALE              64.2 GB/s    16.71 ms       31.4%  a[i] = q * b[i]
ADD               162.6 GB/s     9.91 ms       79.4%  a[i] = b[i] + c[i]
TRIAD             112.3 GB/s    14.35 ms       54.8%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 25.9 GB/s
================================================================================
STREAM Score (minimum bandwidth): 25.9 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.06ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.06ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.17ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.06ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.06ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.06ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           3.4 GFLOPS    0.06ms
    fp32                                           3.3 GFLOPS    0.06ms
    tf32                                           3.4 GFLOPS    0.06ms
    fp16                                           3.4 GFLOPS    0.06ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           3.4 GFLOPS    0.06ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                          13.0 GFLOPS    0.15ms
    fp32                                          18.9 GFLOPS    0.11ms
    tf32                                          19.0 GFLOPS    0.11ms
    fp16                                          35.4 GFLOPS    0.06ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          33.9 GFLOPS    0.06ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                          19.4 GFLOPS    1.03ms
    fp32                                          34.5 GFLOPS    0.58ms
    tf32                                          34.1 GFLOPS    0.59ms
    fp16                                          57.0 GFLOPS    0.35ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          56.6 GFLOPS    0.35ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    int64      [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int32      [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int16      [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int8       [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.6 GFLOPS    0.12ms
    fp32                                           1.6 GFLOPS    0.12ms
    tf32                                           1.6 GFLOPS    0.13ms
    fp16                                           1.6 GFLOPS    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.6 GFLOPS    0.12ms
    int64                                           1.6 GIOPS    0.12ms
    int32                                           1.7 GIOPS    0.12ms
    int16                                           1.6 GIOPS    0.12ms
    int8                                            1.7 GIOPS    0.12ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           5.3 GFLOPS    0.37ms
    fp32                                           9.0 GFLOPS    0.22ms
    tf32                                           8.9 GFLOPS    0.23ms
    fp16                                          15.5 GFLOPS    0.13ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          15.4 GFLOPS    0.13ms
    int64                                           5.4 GIOPS    0.37ms
    int32                                           8.9 GIOPS    0.23ms
    int16                                          16.0 GIOPS    0.12ms
    int8                                           16.3 GIOPS    0.12ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           6.6 GFLOPS    3.03ms
    fp32                                          12.5 GFLOPS    1.59ms
    tf32                                          12.1 GFLOPS    1.65ms
    fp16                                          21.1 GFLOPS    0.95ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          20.7 GFLOPS    0.97ms
    int64                                           6.0 GIOPS    3.33ms
    int32                                          10.2 GIOPS    1.95ms
    int16                                          19.0 GIOPS    1.05ms
    int8                                           27.9 GIOPS    0.72ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.42ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.35ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp32      [!] SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    tf32      [!] SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.6 GFLOPS    0.20ms
    fp32                                           2.6 GFLOPS    0.20ms
    tf32                                           2.6 GFLOPS    0.20ms
    fp16                                           2.6 GFLOPS    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.6 GFLOPS    0.20ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          10.3 GFLOPS    0.20ms
    fp32                                          10.4 GFLOPS    0.20ms
    tf32                                          10.5 GFLOPS    0.20ms
    fp16                                          10.5 GFLOPS    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          10.4 GFLOPS    0.20ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          26.6 GFLOPS    0.32ms
    fp32                                          37.8 GFLOPS    0.22ms
    tf32                                          37.9 GFLOPS    0.22ms
    fp16                                          37.5 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          37.8 GFLOPS    0.22ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.27ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           2.0 GFLOPS    0.26ms
    fp32                                           2.5 GFLOPS    0.21ms
    tf32                                           2.6 GFLOPS    0.20ms
    fp16                                           2.5 GFLOPS    0.21ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.5 GFLOPS    0.21ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                          10.4 GFLOPS    0.40ms
    fp32                                           9.1 GFLOPS    0.46ms
    tf32                                          18.9 GFLOPS    0.22ms
    fp16                                          18.8 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          18.2 GFLOPS    0.23ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                          15.1 GFLOPS    2.22ms
    fp32                                         150.6 GFLOPS    0.22ms
    tf32                                         153.2 GFLOPS    0.22ms
    fp16                                         121.8 GFLOPS    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         150.0 GFLOPS    0.22ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          30.3 GFLOPS    8.85ms
    fp32                                         636.4 GFLOPS    0.42ms
    tf32                                         787.5 GFLOPS    0.34ms
    fp16                                        1314.4 GFLOPS    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1294.8 GFLOPS    0.21ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)


+----------------------



================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
ERROR: Framework Selection Failed
================================================================================
  PyTorch framework requested but PyTorch is not installed


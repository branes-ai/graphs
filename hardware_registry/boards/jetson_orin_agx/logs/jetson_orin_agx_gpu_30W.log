(.venv) cjuser@sdhome-jetson:~/dev/branes/clones/graphs$ cli/calibrate_hardware.py --id jetson_orin_agx_gpu --framework pytorch

================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1216 MHz idle (DVFS will boost under load)
    Current:  1216 MHz (idle)
    Expected: Up to 1728 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[OK] System Load: 0.5% (idle)
    Current:  0.5%
    Expected: < 5%
[OK] Thermal State: 40°C (cool)
    Current:  40°C
    Expected: < 80°C
[!] GPU Power Mode: MODE_30W (unknown mode)
    Current:  MODE_30W
    Expected: Known mode (MAXN, 7W, 15W, 25W, etc.)

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

[!] Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 61.3 GB
  Python: 3.8.10
  NumPy: 1.24.4
  PyTorch: 2.1.0a0+41361538.nv23.06

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 550 MHz (32% of max 1728 MHz)
  Governor: schedutil
  Power Mode: MODE_30W (nvpmodel 30)

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock: 306 MHz (50% of max 612 MHz)
  Power Mode: MODE_30W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    39.3 GB/s  (19.2%)    0.43 ms
  Size    16 MB...    41.4 GB/s  (20.2%)    0.81 ms
  Size    32 MB...    44.1 GB/s  (21.6%)    1.52 ms
  Size    64 MB...    50.2 GB/s  (24.5%)    2.68 ms
  Size   128 MB...    51.4 GB/s  (25.1%)    5.22 ms
  Size   256 MB...    45.6 GB/s  (22.3%)   11.78 ms
  Size   512 MB...    50.6 GB/s  (24.7%)   21.22 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    39.3 GB/s  (19.2%)    0.43 ms  |  4.9 GFLOPS
  Size    16 MB...    57.6 GB/s  (28.1%)    0.58 ms  |  7.2 GFLOPS
  Size    32 MB...    57.3 GB/s  (28.0%)    1.17 ms  |  7.2 GFLOPS
  Size    64 MB...    81.7 GB/s  (39.9%)    1.64 ms  |  10.2 GFLOPS
  Size   128 MB...    86.7 GB/s  (42.4%)    3.09 ms  |  10.8 GFLOPS
  Size   256 MB...   108.6 GB/s  (53.0%)    4.94 ms  |  13.6 GFLOPS
  Size   512 MB...   113.4 GB/s  (55.3%)    9.47 ms  |  14.2 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    72.8 GB/s  (35.6%)    0.35 ms  |  6.1 GFLOPS
  Size    16 MB...    77.3 GB/s  (37.8%)    0.65 ms  |  6.4 GFLOPS
  Size    32 MB...   103.5 GB/s  (50.5%)    0.97 ms  |  8.6 GFLOPS
  Size    64 MB...   114.7 GB/s  (56.0%)    1.76 ms  |  9.6 GFLOPS
  Size   128 MB...   116.6 GB/s  (56.9%)    3.45 ms  |  9.7 GFLOPS
  Size   256 MB...   118.3 GB/s  (57.8%)    6.81 ms  |  9.9 GFLOPS
  Size   512 MB...   124.5 GB/s  (60.8%)   12.93 ms  |  10.4 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    41.1 GB/s  (20.1%)    0.61 ms  |  6.8 GFLOPS
  Size    16 MB...    55.8 GB/s  (27.3%)    0.90 ms  |  9.3 GFLOPS
  Size    32 MB...    64.1 GB/s  (31.3%)    1.57 ms  |  10.7 GFLOPS
  Size    64 MB...    69.3 GB/s  (33.8%)    2.90 ms  |  11.6 GFLOPS
  Size   128 MB...    72.7 GB/s  (35.5%)    5.54 ms  |  12.1 GFLOPS
  Size   256 MB...    71.6 GB/s  (35.0%)   11.24 ms  |  11.9 GFLOPS
  Size   512 MB...    73.2 GB/s  (35.8%)   22.00 ms  |  12.2 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               51.4 GB/s     5.22 ms       25.1%  a[i] = b[i]
SCALE             113.4 GB/s     9.47 ms       55.3%  a[i] = q * b[i]
ADD               124.5 GB/s    12.93 ms       60.8%  a[i] = b[i] + c[i]
TRIAD              73.2 GB/s    22.00 ms       35.8%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 39.3 GB/s
================================================================================
STREAM Score (minimum bandwidth): 39.3 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.17ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           2.6 GFLOPS    0.08ms
    fp32                                           2.6 GFLOPS    0.08ms
    tf32                                           2.7 GFLOPS    0.07ms
    fp16                                           2.7 GFLOPS    0.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.6 GFLOPS    0.08ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           8.7 GFLOPS    0.23ms
    fp32                                          11.9 GFLOPS    0.17ms
    tf32                                          11.9 GFLOPS    0.17ms
    fp16                                          16.3 GFLOPS    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          11.3 GFLOPS    0.18ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                          14.4 GFLOPS    1.39ms
    fp32                                          22.5 GFLOPS    0.89ms
    tf32                                          20.9 GFLOPS    0.96ms
    fp16                                          19.8 GFLOPS    1.01ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          20.4 GFLOPS    0.98ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.16ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.3 GFLOPS    0.15ms
    fp32                                           1.3 GFLOPS    0.15ms
    tf32                                           1.3 GFLOPS    0.15ms
    fp16                                           1.1 GFLOPS    0.18ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.3 GFLOPS    0.15ms
    int64                                           1.3 GIOPS    0.15ms
    int32                                           1.3 GIOPS    0.15ms
    int16                                           1.3 GIOPS    0.15ms
    int8                                            1.4 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.8 GFLOPS    0.71ms
    fp32                                           5.6 GFLOPS    0.36ms
    tf32                                           5.6 GFLOPS    0.36ms
    fp16                                           8.8 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.8 GFLOPS    0.23ms
    int64                                           3.0 GIOPS    0.66ms
    int32                                           5.6 GIOPS    0.35ms
    int16                                           8.8 GIOPS    0.23ms
    int8                                           12.7 GIOPS    0.16ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           3.1 GFLOPS    6.36ms
    fp32                                           7.0 GFLOPS    2.85ms
    tf32                                           7.0 GFLOPS    2.84ms
    fp16                                          12.8 GFLOPS    1.56ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.2 GFLOPS    1.41ms
    int64                                           3.2 GIOPS    6.31ms
    int32                                           7.6 GIOPS    2.64ms
    int16                                          13.8 GIOPS    1.45ms
    int8                                           18.5 GIOPS    1.08ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.38ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.34ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.32ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.1 GFLOPS    0.25ms
    fp32                                           2.1 GFLOPS    0.25ms
    tf32                                           2.1 GFLOPS    0.25ms
    fp16                                           2.1 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.1 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           8.1 GFLOPS    0.26ms
    fp32                                           8.5 GFLOPS    0.25ms
    tf32                                           8.6 GFLOPS    0.25ms
    fp16                                           8.5 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.5 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           9.7 GFLOPS    0.87ms
    fp32                                          30.4 GFLOPS    0.28ms
    tf32                                          30.4 GFLOPS    0.28ms
    fp16                                          29.9 GFLOPS    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          31.0 GFLOPS    0.27ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32... ALL PRECISIONS FAILED
  Size     64... ALL PRECISIONS FAILED
  Size    128... ALL PRECISIONS FAILED
  Size    256... ALL PRECISIONS FAILED
  Size    512... ALL PRECISIONS FAILED
  Size     1K... ALL PRECISIONS FAILED
  Size     2K... ALL PRECISIONS FAILED

==========================================================================================
BLAS Suite Complete: 17 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
Date: 2026-01-29T17:06:20.433717
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 5325.0
  Peak Bandwidth:     204.8 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       51.4 GB/s     5.22 ms       25.1%  a[i] = b[i]
  SCALE                  512      113.4 GB/s     9.47 ms       55.3%  a[i] = q * b[i]
  ADD                    512      124.5 GB/s    12.93 ms       60.8%  a[i] = b[i] + c[i]
  TRIAD                  512       73.2 GB/s    22.00 ms       35.8%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 39.3 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                3.1        7.0        7.0       12.8        N/A        N/A       14.2        3.2        7.6       13.8       18.5        N/A             int8
  DOT                14.4       22.5       20.9       19.8        N/A        N/A       20.4        N/A        N/A        N/A        N/A        N/A             fp32
  GEMV                9.7       30.4       30.4       29.9        N/A        N/A       31.0        N/A        N/A        N/A        N/A        N/A             bf16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         3.1 GFLOPS     6.36ms     0.33        1.9%
  fp32              10M         7.0 GFLOPS     2.85ms     0.17        0.1%
  tf32              10M         7.0 GFLOPS     2.84ms     0.17        0.1%
  fp16              10M        12.8 GFLOPS     1.56ms     0.33        0.1%
  bf16              10M        14.2 GFLOPS     1.41ms     0.33        0.1%
  int64             10M          3.2 GIOPS     6.31ms     0.33        0.0%
  int32             10M          7.6 GIOPS     2.64ms     0.33        0.1%
  int16             10M         13.8 GIOPS     1.45ms     0.33        0.1%
  int8              10M         18.5 GIOPS     1.08ms     0.33        0.0%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M        14.4 GFLOPS     1.39ms     0.50        8.7%
  fp32              10M        22.5 GFLOPS     0.89ms     0.25        0.4%
  tf32              10M        20.9 GFLOPS     0.96ms     0.25        0.2%
  fp16              10M        19.8 GFLOPS     1.01ms     0.50        0.1%
  bf16              10M        20.4 GFLOPS     0.98ms     0.50        0.1%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         9.7 GFLOPS     0.87ms     1.00        5.8%
  fp32               2K        30.4 GFLOPS     0.28ms     0.50        0.6%
  tf32               2K        30.4 GFLOPS     0.28ms     0.50        0.3%
  fp16               2K        29.9 GFLOPS     0.28ms     1.00        0.1%
  bf16               2K        31.0 GFLOPS     0.27ms     1.00        0.1%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/MODE30W_306MHz_pytorch_20260129T170710Z.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_agx_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_agx_gpu


================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1216 MHz idle (DVFS will boost under load)
    Current:  1216 MHz (idle)
    Expected: Up to 1728 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[OK] System Load: 1.0% (idle)
    Current:  1.0%
    Expected: < 5%
[OK] Thermal State: 40°C (cool)
    Current:  40°C
    Expected: < 80°C
[!] GPU Power Mode: MODE_30W (unknown mode)
    Current:  MODE_30W
    Expected: Known mode (MAXN, 7W, 15W, 25W, etc.)

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

[!] Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 61.3 GB
  Python: 3.8.10
  NumPy: 1.24.4
  PyTorch: 2.1.0a0+41361538.nv23.06

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 576 MHz (33% of max 1728 MHz)
  Governor: schedutil
  Power Mode: MODE_30W (nvpmodel 30)

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock: 306 MHz (50% of max 612 MHz)
  Power Mode: MODE_30W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    18.3 GB/s   (8.9%)    0.92 ms
  Size    16 MB...    28.8 GB/s  (14.1%)    1.16 ms
  Size    32 MB...    28.6 GB/s  (14.0%)    2.34 ms
  Size    64 MB...    33.0 GB/s  (16.1%)    4.06 ms
  Size   128 MB...    53.7 GB/s  (26.2%)    4.99 ms
  Size   256 MB...    57.5 GB/s  (28.1%)    9.34 ms
  Size   512 MB...    58.2 GB/s  (28.4%)   18.46 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    55.3 GB/s  (27.0%)    0.30 ms  |  6.9 GFLOPS
  Size    16 MB...    72.6 GB/s  (35.4%)    0.46 ms  |  9.1 GFLOPS
  Size    32 MB...    92.7 GB/s  (45.3%)    0.72 ms  |  11.6 GFLOPS
  Size    64 MB...   102.4 GB/s  (50.0%)    1.31 ms  |  12.8 GFLOPS
  Size   128 MB...   112.3 GB/s  (54.8%)    2.39 ms  |  14.0 GFLOPS
  Size   256 MB...    89.1 GB/s  (43.5%)    6.02 ms  |  11.1 GFLOPS
  Size   512 MB...    93.5 GB/s  (45.7%)   11.48 ms  |  11.7 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    50.7 GB/s  (24.8%)    0.50 ms  |  4.2 GFLOPS
  Size    16 MB...    90.2 GB/s  (44.0%)    0.56 ms  |  7.5 GFLOPS
  Size    32 MB...   102.5 GB/s  (50.0%)    0.98 ms  |  8.5 GFLOPS
  Size    64 MB...   116.1 GB/s  (56.7%)    1.73 ms  |  9.7 GFLOPS
  Size   128 MB...   116.9 GB/s  (57.1%)    3.44 ms  |  9.7 GFLOPS
  Size   256 MB...   123.3 GB/s  (60.2%)    6.53 ms  |  10.3 GFLOPS
  Size   512 MB...   123.8 GB/s  (60.5%)   13.01 ms  |  10.3 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    44.4 GB/s  (21.7%)    0.57 ms  |  7.4 GFLOPS
  Size    16 MB...    60.8 GB/s  (29.7%)    0.83 ms  |  10.1 GFLOPS
  Size    32 MB...    67.3 GB/s  (32.9%)    1.50 ms  |  11.2 GFLOPS
  Size    64 MB...    72.5 GB/s  (35.4%)    2.78 ms  |  12.1 GFLOPS
  Size   128 MB...    73.2 GB/s  (35.7%)    5.50 ms  |  12.2 GFLOPS
  Size   256 MB...    71.8 GB/s  (35.0%)   11.22 ms  |  12.0 GFLOPS
  Size   512 MB...    73.0 GB/s  (35.7%)   22.06 ms  |  12.2 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               58.2 GB/s    18.46 ms       28.4%  a[i] = b[i]
SCALE             112.3 GB/s     2.39 ms       54.8%  a[i] = q * b[i]
ADD               123.8 GB/s    13.01 ms       60.5%  a[i] = b[i] + c[i]
TRIAD              73.2 GB/s     5.50 ms       35.7%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 18.3 GB/s
================================================================================
STREAM Score (minimum bandwidth): 18.3 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.12ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           2.7 GFLOPS    0.08ms
    fp32                                           2.8 GFLOPS    0.07ms
    tf32                                           2.7 GFLOPS    0.07ms
    fp16                                           2.7 GFLOPS    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.7 GFLOPS    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           9.1 GFLOPS    0.22ms
    fp32                                          12.5 GFLOPS    0.16ms
    tf32                                          12.5 GFLOPS    0.16ms
    fp16                                          17.3 GFLOPS    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.7 GFLOPS    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                          14.5 GFLOPS    1.38ms
    fp32                                          22.5 GFLOPS    0.89ms
    tf32                                          22.5 GFLOPS    0.89ms
    fp16                                          21.8 GFLOPS    0.92ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          20.5 GFLOPS    0.98ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.4 GFLOPS    0.14ms
    fp32                                           1.4 GFLOPS    0.15ms
    tf32                                           1.4 GFLOPS    0.14ms
    fp16                                           1.4 GFLOPS    0.14ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.4 GFLOPS    0.14ms
    int64                                           1.4 GIOPS    0.14ms
    int32                                           1.4 GIOPS    0.14ms
    int16                                           1.4 GIOPS    0.14ms
    int8                                            1.4 GIOPS    0.14ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.9 GFLOPS    0.69ms
    fp32                                           5.7 GFLOPS    0.35ms
    tf32                                           5.7 GFLOPS    0.35ms
    fp16                                           9.0 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.9 GFLOPS    0.22ms
    int64                                           3.1 GIOPS    0.65ms
    int32                                           5.7 GIOPS    0.35ms
    int16                                           8.8 GIOPS    0.23ms
    int8                                           13.9 GIOPS    0.14ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           3.2 GFLOPS    6.24ms
    fp32                                           7.2 GFLOPS    2.78ms
    tf32                                           7.0 GFLOPS    2.86ms
    fp16                                          12.4 GFLOPS    1.61ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          12.5 GFLOPS    1.61ms
    int64                                           3.2 GIOPS    6.21ms
    int32                                           7.0 GIOPS    2.86ms
    int16                                          12.4 GIOPS    1.62ms
    int8                                           15.1 GIOPS    1.33ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.29ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.49ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.41ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    fp32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    fp16      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           1.9 GFLOPS    0.28ms
    fp32                                           2.2 GFLOPS    0.24ms
    tf32                                           2.2 GFLOPS    0.24ms
    fp16                                           2.3 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.2 GFLOPS    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           7.9 GFLOPS    0.27ms
    fp32                                           8.9 GFLOPS    0.24ms
    tf32                                           8.7 GFLOPS    0.24ms
    fp16                                           8.7 GFLOPS    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.9 GFLOPS    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           9.7 GFLOPS    0.86ms
    fp32                                          31.5 GFLOPS    0.27ms
    tf32                                          31.4 GFLOPS    0.27ms
    fp16                                          23.7 GFLOPS    0.35ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          31.9 GFLOPS    0.26ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.28ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.32ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.36ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.75ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           1.3 GFLOPS    0.39ms
    fp32                                           1.7 GFLOPS    0.31ms
    tf32                                           1.4 GFLOPS    0.36ms
    fp16                                           1.1 GFLOPS    0.47ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    1.18ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           5.7 GFLOPS    0.73ms
    fp32                                           8.5 GFLOPS    0.49ms
    tf32                                           5.9 GFLOPS    0.71ms
    fp16                                           7.2 GFLOPS    0.58ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           5.5 GFLOPS    0.77ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           7.6 GFLOPS    4.44ms
    fp32                                          48.8 GFLOPS    0.69ms
    tf32                                          53.6 GFLOPS    0.63ms
    fp16                                          56.2 GFLOPS    0.60ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          26.8 GFLOPS    1.25ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          15.3 GFLOPS   17.55ms
    fp32                                         185.1 GFLOPS    1.45ms
    tf32                                         251.2 GFLOPS    1.07ms
    fp16                                         733.0 GFLOPS    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         742.6 GFLOPS    0.36ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          16.6 GFLOPS  129.18ms
    fp32                                         384.9 GFLOPS    5.58ms
    tf32                                         713.9 GFLOPS    3.01ms
    fp16                                        1202.5 GFLOPS    1.79ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1155.4 GFLOPS    1.86ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          16.7 GFLOPS    1.03s
    fp32                                         714.6 GFLOPS   24.04ms
    tf32                                        2011.9 GFLOPS    8.54ms
    fp16                                        2990.0 GFLOPS    5.75ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2881.0 GFLOPS    5.96ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
Date: 2026-01-29T17:12:24.964452
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 5325.0
  Peak Bandwidth:     204.8 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   512       58.2 GB/s    18.46 ms       28.4%  a[i] = b[i]
  SCALE                  128      112.3 GB/s     2.39 ms       54.8%  a[i] = q * b[i]
  ADD                    512      123.8 GB/s    13.01 ms       60.5%  a[i] = b[i] + c[i]
  TRIAD                  128       73.2 GB/s     5.50 ms       35.7%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 18.3 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                3.2        7.2        7.0       12.4        N/A        N/A       12.5        3.2        7.0       12.4       15.1        N/A             int8
  DOT                14.5       22.5       22.5       21.8        N/A        N/A       20.5        N/A        N/A        N/A        N/A        N/A             tf32
  GEMV                9.7       31.5       31.4       23.7        N/A        N/A       31.9        N/A        N/A        N/A        N/A        N/A             bf16
  GEMM               16.7      714.6     2011.9     2990.0        N/A        N/A     2881.0        N/A        N/A        N/A        N/A        N/A             fp16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         3.2 GFLOPS     6.24ms     0.33        1.9%
  fp32              10M         7.2 GFLOPS     2.78ms     0.17        0.1%
  tf32              10M         7.0 GFLOPS     2.86ms     0.17        0.1%
  fp16              10M        12.4 GFLOPS     1.61ms     0.33        0.1%
  bf16              10M        12.5 GFLOPS     1.61ms     0.33        0.1%
  int64             10M          3.2 GIOPS     6.21ms     0.33        0.0%
  int32             10M          7.0 GIOPS     2.86ms     0.33        0.1%
  int16             10M         12.4 GIOPS     1.62ms     0.33        0.1%
  int8              10M         15.1 GIOPS     1.33ms     0.33        0.0%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M        14.5 GFLOPS     1.38ms     0.50        8.7%
  fp32              10M        22.5 GFLOPS     0.89ms     0.25        0.4%
  tf32              10M        22.5 GFLOPS     0.89ms     0.25        0.2%
  fp16              10M        21.8 GFLOPS     0.92ms     0.50        0.1%
  bf16              10M        20.5 GFLOPS     0.98ms     0.50        0.1%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         9.7 GFLOPS     0.86ms     1.00        5.9%
  fp32               2K        31.5 GFLOPS     0.27ms     0.50        0.6%
  tf32               2K        31.4 GFLOPS     0.27ms     0.50        0.3%
  fp16               2K        23.7 GFLOPS     0.35ms     1.00        0.1%
  bf16               2K        31.9 GFLOPS     0.26ms     1.00        0.1%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K        16.7 GFLOPS      1.03s   170.67       10.1%
  fp32               2K       714.6 GFLOPS    24.04ms   341.33       13.4%
  tf32               2K      2011.9 GFLOPS     8.54ms   341.33       18.9%
  fp16               2K      2990.0 GFLOPS     5.75ms   682.67       14.0%
  bf16               2K      2881.0 GFLOPS     5.96ms   682.67       13.5%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/MODE30W_306MHz_pytorch_20260129T171932Z.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_agx_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_agx_gpu


================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
ERROR: Framework Selection Failed
================================================================================
  PyTorch framework requested but PyTorch is not installed


================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1216 MHz idle (DVFS will boost under load)
    Current:  1216 MHz (idle)
    Expected: Up to 1728 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[OK] System Load: 0.3% (idle)
    Current:  0.3%
    Expected: < 5%
[OK] Thermal State: 41°C (cool)
    Current:  41°C
    Expected: < 80°C
[!] GPU Power Mode: MODE_30W (unknown mode)
    Current:  MODE_30W
    Expected: Known mode (MAXN, 7W, 15W, 25W, etc.)

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

[!] Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 61.3 GB
  Python: 3.8.10
  NumPy: 1.24.4
  PyTorch: 2.1.0a0+41361538.nv23.06

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 1216 MHz (70% of max 1728 MHz)
  Governor: schedutil
  Power Mode: MODE_30W (nvpmodel 30)

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock: 612 MHz (100% of max 612 MHz)
  Power Mode: MODE_30W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    61.9 GB/s  (30.2%)    0.27 ms
  Size    16 MB...    71.3 GB/s  (34.8%)    0.47 ms
  Size    32 MB...    76.9 GB/s  (37.5%)    0.87 ms
  Size    64 MB...    80.2 GB/s  (39.2%)    1.67 ms
  Size   128 MB...    80.8 GB/s  (39.5%)    3.32 ms
  Size   256 MB...    70.4 GB/s  (34.4%)    7.63 ms
  Size   512 MB...    70.7 GB/s  (34.5%)   15.20 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    62.7 GB/s  (30.6%)    0.27 ms  |  7.8 GFLOPS
  Size    16 MB...   102.1 GB/s  (49.9%)    0.33 ms  |  12.8 GFLOPS
  Size    32 MB...   114.9 GB/s  (56.1%)    0.58 ms  |  14.4 GFLOPS
  Size    64 MB...   123.5 GB/s  (60.3%)    1.09 ms  |  15.4 GFLOPS
  Size   128 MB...   126.8 GB/s  (61.9%)    2.12 ms  |  15.8 GFLOPS
  Size   256 MB...   115.8 GB/s  (56.6%)    4.64 ms  |  14.5 GFLOPS
  Size   512 MB...   117.0 GB/s  (57.1%)    9.17 ms  |  14.6 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    98.9 GB/s  (48.3%)    0.25 ms  |  8.2 GFLOPS
  Size    16 MB...   114.0 GB/s  (55.7%)    0.44 ms  |  9.5 GFLOPS
  Size    32 MB...   122.9 GB/s  (60.0%)    0.82 ms  |  10.2 GFLOPS
  Size    64 MB...   128.0 GB/s  (62.5%)    1.57 ms  |  10.7 GFLOPS
  Size   128 MB...   125.6 GB/s  (61.3%)    3.21 ms  |  10.5 GFLOPS
  Size   256 MB...   126.9 GB/s  (61.9%)    6.35 ms  |  10.6 GFLOPS
  Size   512 MB...   127.4 GB/s  (62.2%)   12.64 ms  |  10.6 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    63.1 GB/s  (30.8%)    0.40 ms  |  10.5 GFLOPS
  Size    16 MB...    70.5 GB/s  (34.4%)    0.71 ms  |  11.8 GFLOPS
  Size    32 MB...    74.9 GB/s  (36.6%)    1.34 ms  |  12.5 GFLOPS
  Size    64 MB...    77.1 GB/s  (37.6%)    2.61 ms  |  12.8 GFLOPS
  Size   128 MB...    76.5 GB/s  (37.4%)    5.26 ms  |  12.7 GFLOPS
  Size   256 MB...    74.4 GB/s  (36.3%)   10.82 ms  |  12.4 GFLOPS
  Size   512 MB...    74.7 GB/s  (36.5%)   21.56 ms  |  12.5 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               80.8 GB/s     3.32 ms       39.5%  a[i] = b[i]
SCALE             126.8 GB/s     2.12 ms       61.9%  a[i] = q * b[i]
ADD               128.0 GB/s     1.57 ms       62.5%  a[i] = b[i] + c[i]
TRIAD              77.1 GB/s     2.61 ms       37.6%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 61.9 GB/s
================================================================================
STREAM Score (minimum bandwidth): 61.9 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           2.6 GFLOPS    0.08ms
    fp32                                           2.7 GFLOPS    0.07ms
    tf32                                           2.7 GFLOPS    0.07ms
    fp16                                           2.7 GFLOPS    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.8 GFLOPS    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           9.1 GFLOPS    0.22ms
    fp32                                          12.5 GFLOPS    0.16ms
    tf32                                          12.5 GFLOPS    0.16ms
    fp16                                          17.1 GFLOPS    0.12ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.9 GFLOPS    0.13ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                          14.4 GFLOPS    1.39ms
    fp32                                          22.3 GFLOPS    0.90ms
    tf32                                          22.4 GFLOPS    0.89ms
    fp16                                          23.7 GFLOPS    0.84ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          23.6 GFLOPS    0.85ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.4 GFLOPS    0.15ms
    fp32                                           1.3 GFLOPS    0.15ms
    tf32                                           1.3 GFLOPS    0.15ms
    fp16                                           1.3 GFLOPS    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.3 GFLOPS    0.15ms
    int64                                           1.3 GIOPS    0.15ms
    int32                                           1.3 GIOPS    0.15ms
    int16                                           1.4 GIOPS    0.15ms
    int8                                            1.3 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.9 GFLOPS    0.70ms
    fp32                                           5.7 GFLOPS    0.35ms
    tf32                                           5.7 GFLOPS    0.35ms
    fp16                                           8.9 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           9.0 GFLOPS    0.22ms
    int64                                           3.1 GIOPS    0.65ms
    int32                                           5.8 GIOPS    0.35ms
    int16                                           9.0 GIOPS    0.22ms
    int8                                           13.2 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           3.3 GFLOPS    6.12ms
    fp32                                           7.8 GFLOPS    2.58ms
    tf32                                           7.8 GFLOPS    2.58ms
    fp16                                          14.2 GFLOPS    1.41ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.2 GFLOPS    1.41ms
    int64                                           3.4 GIOPS    5.92ms
    int32                                           7.8 GIOPS    2.58ms
    int16                                          14.2 GIOPS    1.41ms
    int8                                           19.1 GIOPS    1.05ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.1 GFLOPS    0.25ms
    fp32                                           2.1 GFLOPS    0.25ms
    tf32                                           2.1 GFLOPS    0.25ms
    fp16                                           2.1 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.1 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           8.2 GFLOPS    0.26ms
    fp32                                           8.5 GFLOPS    0.25ms
    tf32                                           8.4 GFLOPS    0.25ms
    fp16                                           8.4 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.4 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           9.4 GFLOPS    0.89ms
    fp32                                          30.4 GFLOPS    0.28ms
    tf32                                          30.8 GFLOPS    0.27ms
    fp16                                          29.9 GFLOPS    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          27.8 GFLOPS    0.30ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.27ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           2.0 GFLOPS    0.26ms
    fp32                                           1.9 GFLOPS    0.27ms
    tf32                                           2.0 GFLOPS    0.26ms
    fp16                                           1.8 GFLOPS    0.30ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.0 GFLOPS    0.26ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                          11.0 GFLOPS    0.38ms
    fp32                                          14.4 GFLOPS    0.29ms
    tf32                                          14.0 GFLOPS    0.30ms
    fp16                                          11.5 GFLOPS    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          11.0 GFLOPS    0.38ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                          13.9 GFLOPS    2.42ms
    fp32                                         122.8 GFLOPS    0.27ms
    tf32                                         121.1 GFLOPS    0.28ms
    fp16                                         113.8 GFLOPS    0.29ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         101.0 GFLOPS    0.33ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          16.3 GFLOPS   16.49ms
    fp32                                         393.8 GFLOPS    0.68ms
    tf32                                         761.3 GFLOPS    0.35ms
    fp16                                        1055.1 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1080.4 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          16.6 GFLOPS  129.19ms
    fp32                                         620.7 GFLOPS    3.46ms
    tf32                                        2121.8 GFLOPS    1.01ms
    fp16                                        2718.6 GFLOPS    0.79ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2609.3 GFLOPS    0.82ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          16.7 GFLOPS    1.03s
    fp32                                         752.9 GFLOPS   22.82ms
    tf32                                        3267.6 GFLOPS    5.26ms
    fp16                                        5627.3 GFLOPS    3.05ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        5563.9 GFLOPS    3.09ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
Date: 2026-01-29T20:46:36.380214
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 5325.0
  Peak Bandwidth:     204.8 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       80.8 GB/s     3.32 ms       39.5%  a[i] = b[i]
  SCALE                  128      126.8 GB/s     2.12 ms       61.9%  a[i] = q * b[i]
  ADD                     64      128.0 GB/s     1.57 ms       62.5%  a[i] = b[i] + c[i]
  TRIAD                   64       77.1 GB/s     2.61 ms       37.6%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 61.9 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                3.3        7.8        7.8       14.2        N/A        N/A       14.2        3.4        7.8       14.2       19.1        N/A             int8
  DOT                14.4       22.3       22.4       23.7        N/A        N/A       23.6        N/A        N/A        N/A        N/A        N/A             fp16
  GEMV                9.4       30.4       30.8       29.9        N/A        N/A       27.8        N/A        N/A        N/A        N/A        N/A             tf32
  GEMM               16.7      752.9     3267.6     5627.3        N/A        N/A     5563.9        N/A        N/A        N/A        N/A        N/A             fp16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         3.3 GFLOPS     6.12ms     0.33        2.0%
  fp32              10M         7.8 GFLOPS     2.58ms     0.17        0.1%
  tf32              10M         7.8 GFLOPS     2.58ms     0.17        0.1%
  fp16              10M        14.2 GFLOPS     1.41ms     0.33        0.1%
  bf16              10M        14.2 GFLOPS     1.41ms     0.33        0.1%
  int64             10M          3.4 GIOPS     5.92ms     0.33        0.0%
  int32             10M          7.8 GIOPS     2.58ms     0.33        0.1%
  int16             10M         14.2 GIOPS     1.41ms     0.33        0.1%
  int8              10M         19.1 GIOPS     1.05ms     0.33        0.0%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M        14.4 GFLOPS     1.39ms     0.50        8.7%
  fp32              10M        22.3 GFLOPS     0.90ms     0.25        0.4%
  tf32              10M        22.4 GFLOPS     0.89ms     0.25        0.2%
  fp16              10M        23.7 GFLOPS     0.84ms     0.50        0.1%
  bf16              10M        23.6 GFLOPS     0.85ms     0.50        0.1%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         9.4 GFLOPS     0.89ms     1.00        5.6%
  fp32               2K        30.4 GFLOPS     0.28ms     0.50        0.6%
  tf32               2K        30.8 GFLOPS     0.27ms     0.50        0.3%
  fp16               2K        29.9 GFLOPS     0.28ms     1.00        0.1%
  bf16               2K        27.8 GFLOPS     0.30ms     1.00        0.1%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K        16.7 GFLOPS      1.03s   170.67       10.1%
  fp32               2K       752.9 GFLOPS    22.82ms   341.33       14.1%
  tf32               2K      3267.6 GFLOPS     5.26ms   341.33       30.7%
  fp16               2K      5627.3 GFLOPS     3.05ms   682.67       26.4%
  bf16               2K      5563.9 GFLOPS     3.09ms   682.67       26.1%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/MODE30W_612MHz_pytorch_20260129T205140Z.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_agx_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_agx_gpu


================================================================================
Using Hardware from Registry: jetson_orin_agx_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin AGX (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1216 MHz idle (DVFS will boost under load)
    Current:  1216 MHz (idle)
    Expected: Up to 1728 MHz under load
[-] Turbo Boost: Could not determine (not Intel pstate)
[OK] System Load: 0.0% (idle)
    Current:  0.0%
    Expected: < 5%
[OK] Thermal State: 41°C (cool)
    Current:  41°C
    Expected: < 80°C
[!] GPU Power Mode: MODE_30W (unknown mode)
    Current:  MODE_30W
    Expected: Known mode (MAXN, 7W, 15W, 25W, etc.)

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

[!] Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 61.3 GB
  Python: 3.8.10
  NumPy: 1.24.4
  PyTorch: 2.1.0a0+41361538.nv23.06

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 1216 MHz (70% of max 1728 MHz)
  Governor: schedutil
  Power Mode: MODE_30W (nvpmodel 30)
  Clock Policy: dvfs

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock: 306 MHz (50% of max 612 MHz)
  Power Mode: MODE_30W
  Clock Policy: dvfs

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    43.9 GB/s  (21.4%)    0.38 ms
  Size    16 MB...    55.8 GB/s  (27.3%)    0.60 ms
  Size    32 MB...    59.2 GB/s  (28.9%)    1.13 ms
  Size    64 MB...    65.9 GB/s  (32.2%)    2.04 ms
  Size   128 MB...    73.1 GB/s  (35.7%)    3.67 ms
  Size   256 MB...    69.9 GB/s  (34.1%)    7.68 ms
  Size   512 MB...    70.1 GB/s  (34.2%)   15.31 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    84.1 GB/s  (41.1%)    0.20 ms  |  10.5 GFLOPS
  Size    16 MB...   103.0 GB/s  (50.3%)    0.33 ms  |  12.9 GFLOPS
  Size    32 MB...   115.9 GB/s  (56.6%)    0.58 ms  |  14.5 GFLOPS
  Size    64 MB...   123.0 GB/s  (60.1%)    1.09 ms  |  15.4 GFLOPS
  Size   128 MB...   127.0 GB/s  (62.0%)    2.11 ms  |  15.9 GFLOPS
  Size   256 MB...   114.9 GB/s  (56.1%)    4.67 ms  |  14.4 GFLOPS
  Size   512 MB...   115.7 GB/s  (56.5%)    9.28 ms  |  14.5 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    98.5 GB/s  (48.1%)    0.26 ms  |  8.2 GFLOPS
  Size    16 MB...   113.3 GB/s  (55.3%)    0.44 ms  |  9.4 GFLOPS
  Size    32 MB...   122.0 GB/s  (59.6%)    0.82 ms  |  10.2 GFLOPS
  Size    64 MB...   127.7 GB/s  (62.3%)    1.58 ms  |  10.6 GFLOPS
  Size   128 MB...   125.2 GB/s  (61.1%)    3.22 ms  |  10.4 GFLOPS
  Size   256 MB...   126.5 GB/s  (61.7%)    6.37 ms  |  10.5 GFLOPS
  Size   512 MB...   127.2 GB/s  (62.1%)   12.66 ms  |  10.6 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    63.5 GB/s  (31.0%)    0.40 ms  |  10.6 GFLOPS
  Size    16 MB...    70.4 GB/s  (34.4%)    0.71 ms  |  11.7 GFLOPS
  Size    32 MB...    75.0 GB/s  (36.6%)    1.34 ms  |  12.5 GFLOPS
  Size    64 MB...    77.2 GB/s  (37.7%)    2.61 ms  |  12.9 GFLOPS
  Size   128 MB...    76.4 GB/s  (37.3%)    5.27 ms  |  12.7 GFLOPS
  Size   256 MB...    74.1 GB/s  (36.2%)   10.87 ms  |  12.3 GFLOPS
  Size   512 MB...    74.4 GB/s  (36.3%)   21.66 ms  |  12.4 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               73.1 GB/s     3.67 ms       35.7%  a[i] = b[i]
SCALE             127.0 GB/s     2.11 ms       62.0%  a[i] = q * b[i]
ADD               127.7 GB/s     1.58 ms       62.3%  a[i] = b[i] + c[i]
TRIAD              77.2 GB/s     2.61 ms       37.7%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 43.9 GB/s
================================================================================
STREAM Score (minimum bandwidth): 43.9 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.1.0a0+41361538.nv23.06
  Device: Orin
  CUDA:   11.4

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           2.7 GFLOPS    0.07ms
    fp32                                           2.6 GFLOPS    0.08ms
    tf32                                           2.8 GFLOPS    0.07ms
    fp16                                           2.7 GFLOPS    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.8 GFLOPS    0.07ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           9.2 GFLOPS    0.22ms
    fp32                                          12.4 GFLOPS    0.16ms
    tf32                                          12.5 GFLOPS    0.16ms
    fp16                                          18.2 GFLOPS    0.11ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.7 GFLOPS    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                          14.2 GFLOPS    1.41ms
    fp32                                          22.5 GFLOPS    0.89ms
    tf32                                          22.5 GFLOPS    0.89ms
    fp16                                          23.6 GFLOPS    0.85ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          23.5 GFLOPS    0.85ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int32      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int16      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.4 GFLOPS    0.15ms
    fp32                                           1.3 GFLOPS    0.15ms
    tf32                                           1.4 GFLOPS    0.15ms
    fp16                                           1.3 GFLOPS    0.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.3 GFLOPS    0.15ms
    int64                                           1.3 GIOPS    0.15ms
    int32                                           1.4 GIOPS    0.15ms
    int16                                           1.4 GIOPS    0.15ms
    int8                                            1.3 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.9 GFLOPS    0.69ms
    fp32                                           5.7 GFLOPS    0.35ms
    tf32                                           5.7 GFLOPS    0.35ms
    fp16                                           9.1 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           9.1 GFLOPS    0.22ms
    int64                                           3.1 GIOPS    0.65ms
    int32                                           5.7 GIOPS    0.35ms
    int16                                           9.0 GIOPS    0.22ms
    int8                                           13.6 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           3.2 GFLOPS    6.15ms
    fp32                                           7.8 GFLOPS    2.58ms
    tf32                                           7.8 GFLOPS    2.58ms
    fp16                                          14.2 GFLOPS    1.40ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.3 GFLOPS    1.40ms
    int64                                           3.4 GIOPS    5.96ms
    int32                                           7.8 GIOPS    2.58ms
    int16                                          14.2 GIOPS    1.41ms
    int8                                           19.1 GIOPS    1.05ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.2 GFLOPS    0.24ms
    fp32                                           2.1 GFLOPS    0.25ms
    tf32                                           2.1 GFLOPS    0.25ms
    fp16                                           2.1 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.1 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           8.2 GFLOPS    0.26ms
    fp32                                           8.6 GFLOPS    0.24ms
    tf32                                           8.5 GFLOPS    0.25ms
    fp16                                           8.5 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.5 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           9.7 GFLOPS    0.86ms
    fp32                                          30.6 GFLOPS    0.27ms
    tf32                                          31.1 GFLOPS    0.27ms
    fp16                                          30.1 GFLOPS    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          30.3 GFLOPS    0.28ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           2.0 GFLOPS    0.26ms
    fp32                                           2.0 GFLOPS    0.26ms
    tf32                                           1.7 GFLOPS    0.32ms
    fp16                                           2.1 GFLOPS    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.1 GFLOPS    0.25ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           6.4 GFLOPS    0.65ms
    fp32                                          15.9 GFLOPS    0.26ms
    tf32                                           8.4 GFLOPS    0.50ms
    fp16                                           6.0 GFLOPS    0.70ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           6.0 GFLOPS    0.70ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           8.1 GFLOPS    4.16ms
    fp32                                         113.9 GFLOPS    0.29ms
    tf32                                          54.3 GFLOPS    0.62ms
    fp16                                         125.7 GFLOPS    0.27ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          92.0 GFLOPS    0.36ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          15.3 GFLOPS   17.52ms
    fp32                                         218.2 GFLOPS    1.23ms
    tf32                                         516.3 GFLOPS    0.52ms
    fp16                                         518.7 GFLOPS    0.52ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1047.4 GFLOPS    0.26ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          16.6 GFLOPS  129.15ms
    fp32                                         531.6 GFLOPS    4.04ms
    tf32                                        1597.4 GFLOPS    1.34ms
    fp16                                        1473.6 GFLOPS    1.46ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1371.4 GFLOPS    1.57ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          16.7 GFLOPS    1.03s
    fp32                                         756.1 GFLOPS   22.72ms
    tf32                                        2567.9 GFLOPS    6.69ms
    fp16                                        4887.5 GFLOPS    3.52ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        5048.3 GFLOPS    3.40ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin AGX (GPU)
Date: 2026-01-29T22:50:19.975732
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 5325.0
  Peak Bandwidth:     204.8 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       73.1 GB/s     3.67 ms       35.7%  a[i] = b[i]
  SCALE                  128      127.0 GB/s     2.11 ms       62.0%  a[i] = q * b[i]
  ADD                     64      127.7 GB/s     1.58 ms       62.3%  a[i] = b[i] + c[i]
  TRIAD                   64       77.2 GB/s     2.61 ms       37.7%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 43.9 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                3.2        7.8        7.8       14.2        N/A        N/A       14.3        3.4        7.8       14.2       19.1        N/A             int8
  DOT                14.2       22.5       22.5       23.6        N/A        N/A       23.5        N/A        N/A        N/A        N/A        N/A             fp16
  GEMV                9.7       30.6       31.1       30.1        N/A        N/A       30.3        N/A        N/A        N/A        N/A        N/A             tf32
  GEMM               16.7      756.1     2567.9     4887.5        N/A        N/A     5048.3        N/A        N/A        N/A        N/A        N/A             bf16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         3.2 GFLOPS     6.15ms     0.33        2.0%
  fp32              10M         7.8 GFLOPS     2.58ms     0.17        0.1%
  tf32              10M         7.8 GFLOPS     2.58ms     0.17        0.1%
  fp16              10M        14.2 GFLOPS     1.40ms     0.33        0.1%
  bf16              10M        14.3 GFLOPS     1.40ms     0.33        0.1%
  int64             10M          3.4 GIOPS     5.96ms     0.33        0.0%
  int32             10M          7.8 GIOPS     2.58ms     0.33        0.1%
  int16             10M         14.2 GIOPS     1.41ms     0.33        0.1%
  int8              10M         19.1 GIOPS     1.05ms     0.33        0.0%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M        14.2 GFLOPS     1.41ms     0.50        8.5%
  fp32              10M        22.5 GFLOPS     0.89ms     0.25        0.4%
  tf32              10M        22.5 GFLOPS     0.89ms     0.25        0.2%
  fp16              10M        23.6 GFLOPS     0.85ms     0.50        0.1%
  bf16              10M        23.5 GFLOPS     0.85ms     0.50        0.1%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         9.7 GFLOPS     0.86ms     1.00        5.8%
  fp32               2K        30.6 GFLOPS     0.27ms     0.50        0.6%
  tf32               2K        31.1 GFLOPS     0.27ms     0.50        0.3%
  fp16               2K        30.1 GFLOPS     0.28ms     1.00        0.1%
  bf16               2K        30.3 GFLOPS     0.28ms     1.00        0.1%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K        16.7 GFLOPS      1.03s   170.67       10.1%
  fp32               2K       756.1 GFLOPS    22.72ms   341.33       14.2%
  tf32               2K      2567.9 GFLOPS     6.69ms   341.33       24.1%
  fp16               2K      4887.5 GFLOPS     3.52ms   682.67       22.9%
  bf16               2K      5048.3 GFLOPS     3.40ms   682.67       23.7%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: hardware_registry/gpu/jetson_orin_agx_gpu/calibrations/MODE30W_306MHz_pytorch_20260129T225530Z.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_agx_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_agx_gpu



===============================================================================
                            COMPREHENSIVE ANALYSIS                             
===============================================================================
Model: vit_l_32
Hardware: Jetson-Orin-AGX
Precision: FP32
Batch Size: 1
===============================================================================

Creating model: vit_l_32 (batch_size=1)...
Creating hardware mapper: Jetson-Orin-AGX (precision=FP32)...
Tracing model with FX...
  PyTorch version: 2.1.0a0+41361538.nv23.06 (parsed: (2, 1))
  Dynamo export available: False
  Tracing model with torch.fx.symbolic_trace...
    [OK] symbolic_trace successful
  Running fusion-based partitioning...
    Partitioned into 288 subgraphs
    Total FLOPs: 0.72 GFLOPs
Partitioned into 288 subgraphs
Total FLOPs: 0.72 GFLOPs
Total memory traffic: 1333.02 MB
Running hardware mapping...
  Peak units allocated: 16/16
  Average utilization: 55.4%
Running roofline analysis (thermal_profile=30W)...
Running energy analysis (thermal_profile=30W)...
Running memory analysis...
Running operator-level EDP breakdown...
Analyzing vit_l_32 on 1 architectures...

  Analyzing Jetson-Orin-AGX-64GB...

Analysis complete!

  Found 288 operators across 288 subgraphs

Analysis complete!
===============================================================================
                 COMPREHENSIVE ANALYSIS REPORT
===============================================================================

EXECUTIVE SUMMARY
-------------------------------------------------------------------------------
Model:                   ViT-L/32
Hardware:                Jetson-Orin-AGX-64GB
Precision:               FP32
Batch Size:              1

Performance:             8.21 ms latency, 121.7 fps
Energy:                  95.3 mJ total (95.3 mJ/inference)
Energy per Inference:    95.3 mJ (78% static overhead)
Efficiency:              2.1% hardware utilization

Memory:                  Peak 1170.5 MB
                         (activations: 1.0 MB, weights: 1225.9 MB)
                         [X] Does not fit in L2 cache (4.2 MB)

PERFORMANCE ANALYSIS
-------------------------------------------------------------------------------
Total Latency:           8.21 ms
Throughput:              121.7 fps
Hardware Utilization:    2.1%
Total FLOPs:             0.72 GFLOPs
Subgraphs:               288
Bottlenecks:             288 compute-bound, 0 memory-bound

ENERGY ANALYSIS
-------------------------------------------------------------------------------
Total Energy:            95.3 mJ
  Compute Energy:        1.4 mJ
  Memory Energy:         20.0 mJ
  Static Energy:         73.9 mJ

Energy per Inference:    95.3 mJ
Average Power:           11.6 W
Peak Power:              12.1 W
Energy Efficiency:       10.2%

Power Management:
  Average Units Allocated: 8.9
  Allocated Units Idle:    51.1 mJ
  Unallocated Units Idle:  22.8 mJ
  Power Gating:            DISABLED (conservative estimate)

MEMORY ANALYSIS
-------------------------------------------------------------------------------
Peak Memory:             1170.5 MB
  Activations:           1.0 MB
  Weights:               1225.9 MB
  Workspace:             1.2 MB

Average Memory:          1226.3 MB
Memory Utilization:      99.9%

Hardware Fit:
  L2 Cache (4.2 MB):       [X] Does not fit
  Device Memory (68.7 GB):  [OK] Fits

RECOMMENDATIONS
-------------------------------------------------------------------------------
  1. Increase batch size to amortize static energy (78% overhead)
  2. Consider FP16 for 2Ã— speedup with minimal accuracy loss
  3. Consider tiling or model partitioning to improve cache locality


(.venv) lanner@lanner-desktop:~/dev/branes/clones/graphs$ cli/calibrate_hardware.py --id jetson_orin_nx_16gb_gpu --framework pytorch --force

================================================================================
Using Hardware from Registry: jetson_orin_nx_16gb_gpu
================================================================================
  Vendor:   NVIDIA
  Model:    Jetson Orin NX 16GB (GPU)
  Type:     gpu
  Arch:     Ampere


================================================================================
EXECUTION DEVICE
================================================================================
  Requested device: CUDA
  Actual device:    GPU (Orin)
  Framework:        PYTORCH

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

[OK] CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
[OK] CPU Frequency: 1984 MHz (100% of max)
    Current:  1984 MHz
    Expected: >= 1786 MHz (90%)
[-] Turbo Boost: Could not determine (not Intel pstate)
[X] System Load: 32.7% (high - will affect results)
    Current:  32.7%
    Expected: < 5%
    Fix: Close other applications or wait for background tasks
[OK] Thermal State: 45°C (cool)
    Current:  45°C
    Expected: < 80°C
[OK] GPU Power Mode: MAXN (maximum performance)

RESULT: FAILED
  Calibration aborted. Fix issues above or use --force to override.

  Fix commands:
    Close other applications or wait for background tasks
======================================================================

[!] WARNING: Proceeding with calibration despite failed pre-flight checks.
  Results will be flagged as non-representative of peak performance.

System Information:
  CPU: aarch64
  Cores: 8 physical, 8 logical
  Memory: 15.3 GB
  Python: 3.10.12
  NumPy: 1.26.4
  PyTorch: 2.4.0a0+3bcc3cddb5.nv24.07

Execution Device:
  Running on: GPU (Orin)
  Framework:  PYTORCH
              (PyTorch DL framework, GPU-accelerated)

Querying CPU clock frequencies...
  CPU Freq: 1549 MHz (78% of max 1984 MHz)
  Governor: schedutil

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock (idle): 306 MHz
  SM Clock (load): 918 MHz (100% of max 918 MHz)
  Power Mode: MAXN

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    30.7 GB/s  (30.1%)    0.55 ms
  Size    16 MB...    42.6 GB/s  (41.8%)    0.79 ms
  Size    32 MB...    38.4 GB/s  (37.7%)    1.75 ms
  Size    64 MB...    42.0 GB/s  (41.2%)    3.20 ms
  Size   128 MB...    50.3 GB/s  (49.4%)    5.33 ms
  Size   256 MB...    55.0 GB/s  (53.9%)    9.76 ms
  Size   512 MB...    51.1 GB/s  (50.1%)   21.01 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    44.6 GB/s  (43.7%)    0.38 ms  |  5.6 GFLOPS
  Size    16 MB...    73.3 GB/s  (71.8%)    0.46 ms  |  9.2 GFLOPS
  Size    32 MB...    52.6 GB/s  (51.6%)    1.28 ms  |  6.6 GFLOPS
  Size    64 MB...    66.5 GB/s  (65.2%)    2.02 ms  |  8.3 GFLOPS
  Size   128 MB...    70.1 GB/s  (68.7%)    3.83 ms  |  8.8 GFLOPS
  Size   256 MB...    79.4 GB/s  (77.9%)    6.76 ms  |  9.9 GFLOPS
  Size   512 MB...    86.4 GB/s  (84.7%)   12.42 ms  |  10.8 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    46.2 GB/s  (45.3%)    0.54 ms  |  3.9 GFLOPS
  Size    16 MB...    78.9 GB/s  (77.3%)    0.64 ms  |  6.6 GFLOPS
  Size    32 MB...    73.4 GB/s  (71.9%)    1.37 ms  |  6.1 GFLOPS
  Size    64 MB...    73.0 GB/s  (71.6%)    2.76 ms  |  6.1 GFLOPS
  Size   128 MB...    80.0 GB/s  (78.4%)    5.03 ms  |  6.7 GFLOPS
  Size   256 MB...    76.4 GB/s  (74.9%)   10.54 ms  |  6.4 GFLOPS
  Size   512 MB...    88.5 GB/s  (86.8%)   18.19 ms  |  7.4 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    46.6 GB/s  (45.6%)    0.54 ms  |  7.8 GFLOPS
  Size    16 MB...    45.5 GB/s  (44.6%)    1.11 ms  |  7.6 GFLOPS
  Size    32 MB...    44.6 GB/s  (43.7%)    2.26 ms  |  7.4 GFLOPS
  Size    64 MB...    44.0 GB/s  (43.2%)    4.57 ms  |  7.3 GFLOPS
  Size   128 MB...    46.8 GB/s  (45.9%)    8.60 ms  |  7.8 GFLOPS
  Size   256 MB...    52.1 GB/s  (51.1%)   15.45 ms  |  8.7 GFLOPS
  Size   512 MB...    50.5 GB/s  (49.5%)   31.89 ms  |  8.4 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               55.0 GB/s     9.76 ms       53.9%  a[i] = b[i]
SCALE              86.4 GB/s    12.42 ms       84.7%  a[i] = q * b[i]
ADD                88.5 GB/s    18.19 ms       86.8%  a[i] = b[i] + c[i]
TRIAD              52.1 GB/s    15.45 ms       51.1%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 30.7 GB/s
================================================================================
STREAM Score (minimum bandwidth): 30.7 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, tf32, fp16, fp8, fp4, bf16, int64, int32, int16, int8, int4

Framework: PyTorch 2.4.0a0+3bcc3cddb5.nv24.07
  Device: Orin
  CUDA:   12.2

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.07ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.17ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.08ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.09ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           2.8 GFLOPS    0.07ms
    fp32                                           2.1 GFLOPS    0.10ms
    tf32                                           1.9 GFLOPS    0.11ms
    fp16                                           3.0 GFLOPS    0.07ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.0 GFLOPS    0.10ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           4.8 GFLOPS    0.42ms
    fp32                                          12.6 GFLOPS    0.16ms
    tf32                                          13.3 GFLOPS    0.15ms
    fp16                                          17.4 GFLOPS    0.11ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          19.7 GFLOPS    0.10ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           9.0 GFLOPS    2.22ms
    fp32                                          10.6 GFLOPS    1.89ms
    tf32                                          18.8 GFLOPS    1.06ms
    fp16                                          30.9 GFLOPS    0.65ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          34.4 GFLOPS    0.58ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    int64      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int32      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.36ms
    int16      [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.36ms
    int8       [!] SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.30ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.32ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.13ms
    tf32      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.39ms
    int64      [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.28ms
    int32      [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.12ms
    int16      [!] SLOW (   0.2 GIOPS < 1.0 GFLOPS threshold)    0.13ms
    int8       [!] SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.14ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64                                           1.5 GFLOPS    0.13ms
    fp32                                           1.4 GFLOPS    0.14ms
    tf32                                           1.5 GFLOPS    0.13ms
    fp16                                           1.6 GFLOPS    0.13ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.5 GFLOPS    0.13ms
    int64                                           1.6 GIOPS    0.13ms
    int32                                           1.6 GIOPS    0.13ms
    int16      [!] SLOW (   0.8 GIOPS < 1.0 GFLOPS threshold)    0.25ms
    int8                                            1.5 GIOPS    0.13ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.2 GFLOPS    0.91ms
    fp32                                           4.3 GFLOPS    0.46ms
    tf32                                           2.8 GFLOPS    0.71ms
    fp16                                           5.3 GFLOPS    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           6.8 GFLOPS    0.30ms
    int64                                           2.4 GIOPS    0.82ms
    int32                                           4.9 GIOPS    0.41ms
    int16                                           8.1 GIOPS    0.25ms
    int8                                           13.5 GIOPS    0.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           2.4 GFLOPS    8.44ms
    fp32                                           5.1 GFLOPS    3.95ms
    tf32                                           4.7 GFLOPS    4.30ms
    fp16                                           9.1 GFLOPS    2.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          10.0 GFLOPS    2.00ms
    int64                                           2.5 GIOPS    7.93ms
    int32                                           5.2 GIOPS    3.86ms
    int16                                           9.8 GIOPS    2.05ms
    int8                                           17.4 GIOPS    1.15ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.48ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.51ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.34ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    tf32      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.29ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    tf32      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp16      [!] SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.47ms
    tf32      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp16      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.6 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.5 GFLOPS    0.21ms
    fp32                                           2.5 GFLOPS    0.21ms
    tf32                                           2.5 GFLOPS    0.21ms
    fp16                                           1.9 GFLOPS    0.28ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.3 GFLOPS    0.23ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           5.4 GFLOPS    0.39ms
    fp32                                           9.7 GFLOPS    0.22ms
    tf32                                          10.4 GFLOPS    0.20ms
    fp16                                           9.3 GFLOPS    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.6 GFLOPS    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           9.4 GFLOPS    0.89ms
    fp32                                           9.7 GFLOPS    0.87ms
    tf32                                          23.3 GFLOPS    0.36ms
    fp16                                          36.3 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          37.8 GFLOPS    0.22ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    tf32      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16      [!] SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           2.1 GFLOPS    0.25ms
    fp32                                           2.1 GFLOPS    0.25ms
    tf32                                           2.3 GFLOPS    0.23ms
    fp16                                           2.3 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.3 GFLOPS    0.23ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           6.1 GFLOPS    0.69ms
    fp32                                          14.7 GFLOPS    0.29ms
    tf32                                           8.8 GFLOPS    0.47ms
    fp16                                          18.2 GFLOPS    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          12.8 GFLOPS    0.33ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           7.7 GFLOPS    4.37ms
    fp32                                         112.9 GFLOPS    0.30ms
    tf32                                         141.4 GFLOPS    0.24ms
    fp16                                         140.1 GFLOPS    0.24ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         144.9 GFLOPS    0.23ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          20.3 GFLOPS   13.25ms
    fp32                                         194.3 GFLOPS    1.38ms
    tf32                                         442.5 GFLOPS    0.61ms
    fp16                                         717.5 GFLOPS    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         636.4 GFLOPS    0.42ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          24.2 GFLOPS   88.59ms
    fp32                                         368.9 GFLOPS    5.82ms
    tf32                                         772.1 GFLOPS    2.78ms
    fp16                                        1489.7 GFLOPS    1.44ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1270.5 GFLOPS    1.69ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                              SKIPPED  (Timeout: GEMM benchmark exceeded 5s timeout)
    fp32                                         897.0 GFLOPS   19.15ms
    tf32                                        1815.1 GFLOPS    9.46ms
    fp16                                        2971.1 GFLOPS    5.78ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2961.2 GFLOPS    5.80ms
    int64                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Lo)
    int32                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'In)
    int16                                             SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Sh)
    int8                                              SKIPPED  (RuntimeError: RuntimeError: "addmm_cuda" not implemented for 'Ch)
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

Calibration saved to: /home/lanner/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nx_16gb_gpu/calibrations/pending_pytorch.json

================================================================================
Hardware Calibration: Jetson Orin NX 16GB (GPU)
Date: 2026-01-27T17:58:50.079825
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 1882.0
  Peak Bandwidth:     102.0 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   256       55.0 GB/s     9.76 ms       53.9%  a[i] = b[i]
  SCALE                  512       86.4 GB/s    12.42 ms       84.7%  a[i] = q * b[i]
  ADD                    512       88.5 GB/s    18.19 ms       86.8%  a[i] = b[i] + c[i]
  TRIAD                  256       52.1 GB/s    15.45 ms       51.1%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 30.7 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                2.4        5.1        4.7        9.1        N/A        N/A       10.0        2.5        5.2        9.8       17.4        N/A             int8
  DOT                 9.0       12.6       18.8       30.9        N/A        N/A       34.4        N/A        N/A        N/A        N/A        N/A             bf16
  GEMV                9.4        9.7       23.3       36.3        N/A        N/A       37.8        N/A        N/A        N/A        N/A        N/A             bf16
  GEMM               24.2      897.0     1815.1     2971.1        N/A        N/A     2961.2        N/A        N/A        N/A        N/A        N/A             fp16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         2.4 GFLOPS     8.44ms     0.33        4.0%
  fp32              10M         5.1 GFLOPS     3.95ms     0.17        0.3%
  tf32              10M         4.7 GFLOPS     4.30ms     0.17        0.1%
  fp16              10M         9.1 GFLOPS     2.19ms     0.33        0.1%
  bf16              10M        10.0 GFLOPS     2.00ms     0.33        0.1%
  int64             10M          2.5 GIOPS     7.93ms     0.33        0.0%
  int32             10M          5.2 GIOPS     3.86ms     0.33        0.3%
  int16             10M          9.8 GIOPS     2.05ms     0.33        0.1%
  int8              10M         17.4 GIOPS     1.15ms     0.33        0.1%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         9.0 GFLOPS     2.22ms     0.50       15.2%
  fp32               1M        12.6 GFLOPS     0.16ms     0.25        0.7%
  tf32              10M        18.8 GFLOPS     1.06ms     0.25        0.5%
  fp16              10M        30.9 GFLOPS     0.65ms     0.50        0.4%
  bf16              10M        34.4 GFLOPS     0.58ms     0.50        0.5%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K         9.4 GFLOPS     0.89ms     1.00       16.0%
  fp32               1K         9.7 GFLOPS     0.22ms     0.50        0.5%
  tf32               2K        23.3 GFLOPS     0.36ms     0.50        0.6%
  fp16               2K        36.3 GFLOPS     0.23ms     1.00        0.5%
  bf16               2K        37.8 GFLOPS     0.22ms     1.00        0.5%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               1K        24.2 GFLOPS    88.59ms    85.33       41.1%
  fp32               2K       897.0 GFLOPS    19.15ms   341.33       47.7%
  tf32               2K      1815.1 GFLOPS     9.46ms   341.33       48.2%
  fp16               2K      2971.1 GFLOPS     5.78ms   682.67       39.5%
  bf16               2K      2961.2 GFLOPS     5.80ms   682.67       39.3%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


================================================================================
Calibration Complete!
================================================================================

Calibration file: /home/lanner/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nx_16gb_gpu/calibrations/MAXN_918MHz_pytorch.json

Next steps:
  1. Review the calibration results above
  2. View calibration efficiency:
     ./cli/show_calibration_efficiency.py --id jetson_orin_nx_16gb_gpu

  3. Use in analysis (calibration auto-loaded from registry):
     ./cli/analyze_comprehensive.py --model resnet18 --hardware jetson_orin_nx_16gb_gpu


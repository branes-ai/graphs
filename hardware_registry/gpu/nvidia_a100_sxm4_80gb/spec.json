{
  "id": "nvidia_a100_sxm4_80gb",
  "vendor": "NVIDIA",
  "model": "NVIDIA A100 SXM4 80GB",
  "device_type": "gpu",
  "product_category": "datacenter",
  "ops_per_clock": {
    "fp64": 138240,
    "fp32": 27648,
    "fp16": 221184,
    "fp8": 0,
    "fp4": 0,
    "bf16": 221184,
    "tf32": 110592,
    "int64": 0,
    "int32": 27648,
    "int16": 0,
    "int8": 442368,
    "int4": 0
  },
  "ops_per_clock_notes": {
    "fp64": "108 SMs x 4 Tensor Cores/SM x 256 ops/TC/clock = 110,592 FP64 ops/clock (3rd gen TC with FP64 support) + CUDA cores",
    "fp32": "108 SMs x 128 CUDA cores/SM x 2 (FMA) = 27,648 FP32 ops/clock",
    "tf32": "108 SMs x 4 Tensor Cores/SM x 256 ops/TC/clock = 110,592 TF32 ops/clock (new in Ampere)",
    "fp16": "108 SMs x 4 Tensor Cores/SM x 512 ops/TC/clock = 221,184 FP16 ops/clock (3rd gen TC)",
    "bf16": "108 SMs x 4 Tensor Cores/SM x 512 ops/TC/clock = 221,184 BF16 ops/clock (new in Ampere)",
    "int8": "108 SMs x 4 Tensor Cores/SM x 1024 ops/TC/clock = 442,368 INT8 ops/clock"
  },
  "theoretical_peaks": {
    "fp64": 9700.0,
    "fp32": 19500.0,
    "fp16": 312000.0,
    "fp8": 0.0,
    "fp4": 0.0,
    "bf16": 312000.0,
    "tf32": 156000.0,
    "int64": 0.0,
    "int32": 39000.0,
    "int16": 0.0,
    "int8": 624000.0,
    "int4": 0.0
  },
  "theoretical_peaks_notes": "Legacy field - calculated at boost_clock_mhz (1410 MHz). Use ops_per_clock x clock_mhz for accurate peaks.",
  "peak_bandwidth_gbps": 2039.0,
  "architecture": "Ampere",
  "compute_units": 108,
  "memory_gb": 80,
  "tdp_watts": 400,
  "base_clock_mhz": 1095.0,
  "boost_clock_mhz": 1410.0,
  "platform": "x86_64",
  "notes": "Ampere architecture. 3rd gen Tensor Cores with TF32, BF16, FP64 support (first with these). 108 SMs with 128 CUDA cores each. 80GB HBM2e at 2 TB/s. Multi-Instance GPU (MIG) support."
}
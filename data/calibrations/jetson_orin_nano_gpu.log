branes@Branes-Jetson:~/dev/branes/clones/graphs$ ./cli/calibrate.py --id jetson_orin_nano_gpu --framework pytorch
================================================================================
Hardware Calibration: Jetson Orin Nano (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

⚠ CPU Governor: schedutil (dynamic - may vary)
    Current:  schedutil
    Expected: performance
✓ CPU Frequency: 883 MHz (92% of max)
    Current:  883 MHz
    Expected: >= 864 MHz (90%)
○ Turbo Boost: Could not determine (not Intel pstate)
⚠ System Load: 6.5% (some background activity)
    Current:  6.5%
    Expected: < 5%
✓ Thermal State: 46°C (cool)
    Current:  46°C
    Expected: < 80°C
⚠ GPU Power Mode: 7W (not maximum)
    Current:  7W
    Expected: MAXN

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

⚠ Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 4 physical, 4 logical
  Memory: 7.4 GB
  Python: 3.10.12
  NumPy: 1.26.4
  PyTorch: 2.5.0a0+872d972e41.nv24.08

Target Device: CUDA
Framework:     PYTORCH

Querying CPU clock frequencies...
  CPU Freq: 768 MHz (80% of max 960 MHz)
  Governor: schedutil

Querying GPU clock frequencies...
  SM Clock: 306 MHz (75% of max 408 MHz)
  Power Mode: 7W

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.5.0a0+872d972e41.nv24.08
  Device: Orin
  CUDA:   12.6

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    10.3 GB/s  (15.1%)    1.63 ms
  Size    16 MB...    13.1 GB/s  (19.3%)    2.56 ms
  Size    32 MB...    14.4 GB/s  (21.2%)    4.65 ms
  Size    64 MB...    20.6 GB/s  (30.3%)    6.51 ms
  Size   128 MB...    24.6 GB/s  (36.2%)   10.91 ms
  Size   256 MB...    23.3 GB/s  (34.3%)   23.00 ms
  Size   512 MB...    23.0 GB/s  (33.8%)   46.66 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    10.2 GB/s  (15.0%)    1.65 ms  |  1.3 GFLOPS
  Size    16 MB...    12.8 GB/s  (18.8%)    2.63 ms  |  1.6 GFLOPS
  Size    32 MB...    19.7 GB/s  (29.0%)    3.40 ms  |  2.5 GFLOPS
  Size    64 MB...    25.0 GB/s  (36.8%)    5.36 ms  |  3.1 GFLOPS
  Size   128 MB...    37.8 GB/s  (55.5%)    7.11 ms  |  4.7 GFLOPS
  Size   256 MB...    39.0 GB/s  (57.4%)   13.76 ms  |  4.9 GFLOPS
  Size   512 MB...    37.6 GB/s  (55.3%)   28.54 ms  |  4.7 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    12.3 GB/s  (18.1%)    2.04 ms  |  1.0 GFLOPS
  Size    16 MB...    17.8 GB/s  (26.1%)    2.83 ms  |  1.5 GFLOPS
  Size    32 MB...    18.0 GB/s  (26.5%)    5.60 ms  |  1.5 GFLOPS
  Size    64 MB...    32.3 GB/s  (47.5%)    6.23 ms  |  2.7 GFLOPS
  Size   128 MB...    39.7 GB/s  (58.4%)   10.14 ms  |  3.3 GFLOPS
  Size   256 MB...    39.3 GB/s  (57.7%)   20.51 ms  |  3.3 GFLOPS
  Size   512 MB...    39.3 GB/s  (57.9%)   40.94 ms  |  3.3 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    19.8 GB/s  (29.1%)    1.27 ms  |  3.3 GFLOPS
  Size    16 MB...    12.7 GB/s  (18.6%)    3.98 ms  |  2.1 GFLOPS
  Size    32 MB...    14.2 GB/s  (20.9%)    7.09 ms  |  2.4 GFLOPS
  Size    64 MB...    23.0 GB/s  (33.8%)    8.75 ms  |  3.8 GFLOPS
  Size   128 MB...    22.3 GB/s  (32.8%)   18.06 ms  |  3.7 GFLOPS
  Size   256 MB...    22.6 GB/s  (33.2%)   35.69 ms  |  3.8 GFLOPS
  Size   512 MB...    23.2 GB/s  (34.2%)   69.31 ms  |  3.9 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               24.6 GB/s    10.91 ms       36.2%  a[i] = b[i]
SCALE              39.0 GB/s    13.76 ms       57.4%  a[i] = q * b[i]
ADD                39.7 GB/s    10.14 ms       58.4%  a[i] = b[i] + c[i]
TRIAD              23.2 GB/s    69.31 ms       34.2%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 10.2 GB/s
================================================================================
STREAM Score (minimum bandwidth): 10.2 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, fp16, fp8, fp4, bf16, tf32, int64, int32, int16, int8, int4

Framework: PyTorch 2.5.0a0+872d972e41.nv24.08
  Device: Orin
  CUDA:   12.6

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.18ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.19ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    fp32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.88ms
    fp32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.61ms
    tf32                                           1.1 GFLOPS    0.17ms
    fp16                                           1.1 GFLOPS    0.18ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    0.48ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.9 GFLOPS    0.69ms
    fp32                                           1.7 GFLOPS    1.17ms
    tf32                                           5.7 GFLOPS    0.35ms
    fp16                                           1.5 GFLOPS    1.35ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           7.3 GFLOPS    0.27ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           4.0 GFLOPS    5.06ms
    fp32                                           9.5 GFLOPS    2.11ms
    tf32                                           9.5 GFLOPS    2.10ms
    fp16                                          13.2 GFLOPS    1.51ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          11.6 GFLOPS    1.73ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    1.92ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.85ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    int64        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int32        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int16        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int8         ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.29ms
    fp32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    1.34ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    1.66ms
    int64        ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.32ms
    int32        ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int16        ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.29ms
    int8         ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.29ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64        ⚠ SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp32        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.88ms
    fp16        ⚠ SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.67ms
    int64        ⚠ SLOW (   0.6 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int32        ⚠ SLOW (   0.4 GIOPS < 1.0 GFLOPS threshold)    0.47ms
    int16        ⚠ SLOW (   0.8 GIOPS < 1.0 GFLOPS threshold)    0.24ms
    int8         ⚠ SLOW (   0.6 GIOPS < 1.0 GFLOPS threshold)    0.31ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64        ⚠ SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    2.93ms
    fp32        ⚠ SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    3.02ms
    tf32                                           2.2 GFLOPS    0.92ms
    fp16                                           2.9 GFLOPS    0.70ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           3.4 GFLOPS    0.58ms
    int64                                           1.1 GIOPS    1.83ms
    int32                                           2.2 GIOPS    0.92ms
    int16                                           1.5 GIOPS    1.37ms
    int8                                            4.5 GIOPS    0.44ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           1.0 GFLOPS   19.87ms
    fp32                                           2.3 GFLOPS    8.54ms
    tf32                                           2.5 GFLOPS    7.99ms
    fp16                                           4.1 GFLOPS    4.87ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.9 GFLOPS    4.09ms
    int64                                           1.0 GIOPS   19.45ms
    int32                                           2.6 GIOPS    7.82ms
    int16                                           4.7 GIOPS    4.27ms
    int8                                            5.6 GIOPS    3.57ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.42ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.44ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.42ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.53ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    2.33ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    2.12ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.50ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    1.78ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.44ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.59ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.43ms
    fp32        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.45ms
    tf32        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.50ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    2.15ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.27ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           1.2 GFLOPS    0.44ms
    fp32                                           1.2 GFLOPS    0.44ms
    tf32        ⚠ SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.75ms
    fp16        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    1.69ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.4 GFLOPS < 1.0 GFLOPS threshold)    1.42ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           1.4 GFLOPS    1.49ms
    fp32                                           1.2 GFLOPS    1.74ms
    tf32                                           4.8 GFLOPS    0.43ms
    fp16                                           4.3 GFLOPS    0.49ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           4.4 GFLOPS    0.48ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                           1.3 GFLOPS    6.24ms
    fp32                                           9.4 GFLOPS    0.89ms
    tf32                                          13.6 GFLOPS    0.61ms
    fp16                                          13.1 GFLOPS    0.64ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          18.0 GFLOPS    0.47ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.46ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    1.83ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.49ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    1.00ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.56ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           1.2 GFLOPS    0.44ms
    fp32                                           1.2 GFLOPS    0.44ms
    tf32        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.66ms
    fp16        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    2.00ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    1.06ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           4.4 GFLOPS    0.96ms
    fp32                                           2.3 GFLOPS    1.82ms
    tf32                                           9.5 GFLOPS    0.44ms
    fp16                                           3.5 GFLOPS    1.21ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           9.4 GFLOPS    0.45ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           3.1 GFLOPS   10.90ms
    fp32                                          48.5 GFLOPS    0.69ms
    tf32                                          53.7 GFLOPS    0.63ms
    fp16                                          62.4 GFLOPS    0.54ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          48.4 GFLOPS    0.69ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           4.9 GFLOPS   54.28ms
    fp32                                         170.4 GFLOPS    1.58ms
    tf32                                         349.5 GFLOPS    0.77ms
    fp16                                         490.9 GFLOPS    0.55ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         282.5 GFLOPS    0.95ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           5.0 GFLOPS  426.52ms
    fp32                                         210.7 GFLOPS   10.19ms
    tf32                                         600.1 GFLOPS    3.58ms
    fp16                                         995.5 GFLOPS    2.16ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1201.7 GFLOPS    1.79ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                              SKIPPED  (Timeout: Warmup exceeded 5s timeout)
    fp32                                         230.0 GFLOPS   74.70ms
    tf32                                        1002.1 GFLOPS   17.14ms
    fp16                                        2023.3 GFLOPS    8.49ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        1911.2 GFLOPS    8.99ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

================================================================================
Hardware Calibration: Jetson Orin Nano (GPU)
Date: 2025-11-27T14:11:26.719146
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 640.0
  Peak Bandwidth:     68.0 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       24.6 GB/s    10.91 ms       36.2%  a[i] = b[i]
  SCALE                  256       39.0 GB/s    13.76 ms       57.4%  a[i] = q * b[i]
  ADD                    128       39.7 GB/s    10.14 ms       58.4%  a[i] = b[i] + c[i]
  TRIAD                  512       23.2 GB/s    69.31 ms       34.2%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 10.2 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                1.0        2.3        2.5        4.1        N/A        N/A        4.9        1.1        2.6        4.7        5.6        N/A             int8
  DOT                 4.0        9.5        9.5       13.2        N/A        N/A       11.6        N/A        N/A        N/A        N/A        N/A             fp16
  GEMV                1.4        9.4       13.6       13.1        N/A        N/A       18.0        N/A        N/A        N/A        N/A        N/A             bf16
  GEMM                5.0      230.0     1002.1     2023.3        N/A        N/A     1911.2        N/A        N/A        N/A        N/A        N/A             fp16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         1.0 GFLOPS    19.87ms     0.33        5.0%
  fp32              10M         2.3 GFLOPS     8.54ms     0.17        0.4%
  tf32              10M         2.5 GFLOPS     7.99ms     0.17        0.2%
  fp16              10M         4.1 GFLOPS     4.87ms     0.33        0.3%
  bf16              10M         4.9 GFLOPS     4.09ms     0.33        0.4%
  int64              1M          1.1 GIOPS     1.83ms     0.33        0.0%
  int32             10M          2.6 GIOPS     7.82ms     0.33        0.4%
  int16             10M          4.7 GIOPS     4.27ms     0.33        0.4%
  int8              10M          5.6 GIOPS     3.57ms     0.33        0.2%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         4.0 GFLOPS     5.06ms     0.50       19.8%
  fp32              10M         9.5 GFLOPS     2.11ms     0.25        1.5%
  tf32              10M         9.5 GFLOPS     2.10ms     0.25        0.7%
  fp16              10M        13.2 GFLOPS     1.51ms     0.50        1.0%
  bf16              10M        11.6 GFLOPS     1.73ms     0.50        0.9%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               1K         1.4 GFLOPS     1.49ms     1.00        7.0%
  fp32               2K         9.4 GFLOPS     0.89ms     0.50        1.5%
  tf32               2K        13.6 GFLOPS     0.61ms     0.50        1.1%
  fp16               2K        13.1 GFLOPS     0.64ms     1.00        1.0%
  bf16               2K        18.0 GFLOPS     0.47ms     1.00        1.4%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               1K         5.0 GFLOPS   426.52ms   341.33       25.2%
  fp32               2K       230.0 GFLOPS    74.70ms   341.33       35.9%
  tf32               2K      1002.1 GFLOPS    17.14ms   341.33       78.3%
  fp16               2K      2023.3 GFLOPS     8.49ms   682.67      158.1%
  bf16               2K      1911.2 GFLOPS     8.99ms   682.67      149.3%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


============================================================
Calibration Complete!
============================================================

Profile saved: jetson_orin_nano_gpu
Location:      /home/branes/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nano_gpu

Use in analysis:
  ./cli/analyze.py --model resnet18 --hardware jetson_orin_nano_gpu


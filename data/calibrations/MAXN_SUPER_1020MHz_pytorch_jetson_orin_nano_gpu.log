branes@Branes-Jetson:~/dev/branes/clones/graphs$ ./cli/calibrate.py --id jetson_orin_nano_gpu --framework pytorch
================================================================================
Hardware Calibration: Jetson Orin Nano (GPU)
================================================================================

Running pre-flight checks...
======================================================================
PRE-FLIGHT CHECKS
======================================================================

✓ CPU Governor: schedutil (Jetson default)
    Current:  schedutil
    Expected: schedutil or performance
✓ CPU Frequency: 1395 MHz idle (DVFS will boost under load)
    Current:  1395 MHz (idle)
    Expected: Up to 1728 MHz under load
○ Turbo Boost: Could not determine (not Intel pstate)
✓ System Load: 3.0% (idle)
    Current:  3.0%
    Expected: < 5%
✓ Thermal State: 46°C (cool)
    Current:  46°C
    Expected: < 80°C
⚠ GPU Power Mode: MAXN_SUPER (unknown mode)
    Current:  MAXN_SUPER
    Expected: Known mode (MAXN, 7W, 15W, 25W, etc.)

RESULT: PASSED with warnings
  Calibration will proceed, but results may not represent peak performance.
======================================================================

⚠ Note: Some pre-flight checks have warnings.
  Results may not represent absolute peak performance.

System Information:
  CPU: aarch64
  Cores: 6 physical, 6 logical
  Memory: 7.4 GB
  Python: 3.10.12
  NumPy: 1.26.4
  PyTorch: 2.5.0a0+872d972e41.nv24.08

Target Device: CUDA
Framework:     PYTORCH

Querying CPU clock frequencies...
  CPU Freq: 794 MHz (46% of max 1728 MHz)
  Governor: schedutil

Querying GPU clock frequencies...
  Running warmup to capture clock under load...
  SM Clock (idle): 306 MHz
  SM Clock (load): 1020 MHz (100% of max 1020 MHz)
  Power Mode: MAXN_SUPER

Running calibration benchmarks...

1. STREAM Memory Bandwidth Benchmark
--------------------------------------------------------------------------------
Framework: PyTorch 2.5.0a0+872d972e41.nv24.08
  Device: Orin
  CUDA:   12.6

STREAM Benchmark Suite:
--------------------------------------------------------------------------------

COPY (a[i] = b[i]):
  Memory ops: 2, FLOPs/element: 0
  Size     8 MB...    16.9 GB/s  (24.9%)    0.99 ms
  Size    16 MB...    51.4 GB/s  (75.5%)    0.65 ms
  Size    32 MB...    37.2 GB/s  (54.7%)    1.80 ms
  Size    64 MB...    58.6 GB/s  (86.2%)    2.29 ms
  Size   128 MB...    60.0 GB/s  (88.2%)    4.48 ms
  Size   256 MB...    58.5 GB/s  (86.0%)    9.18 ms
  Size   512 MB...    58.6 GB/s  (86.2%)   18.33 ms

SCALE (a[i] = q * b[i]):
  Memory ops: 2, FLOPs/element: 1
  Size     8 MB...    24.0 GB/s  (35.3%)    0.70 ms  |  3.0 GFLOPS
  Size    16 MB...    35.9 GB/s  (52.7%)    0.94 ms  |  4.5 GFLOPS
  Size    32 MB...    51.8 GB/s  (76.2%)    1.30 ms  |  6.5 GFLOPS
  Size    64 MB...    76.7 GB/s (112.8%)    1.75 ms  |  9.6 GFLOPS
  Size   128 MB...    92.8 GB/s (136.5%)    2.89 ms  |  11.6 GFLOPS
  Size   256 MB...    82.4 GB/s (121.1%)    6.52 ms  |  10.3 GFLOPS
  Size   512 MB...    90.4 GB/s (132.9%)   11.88 ms  |  11.3 GFLOPS

ADD (a[i] = b[i] + c[i]):
  Memory ops: 3, FLOPs/element: 1
  Size     8 MB...    55.8 GB/s  (82.1%)    0.45 ms  |  4.7 GFLOPS
  Size    16 MB...    44.0 GB/s  (64.6%)    1.14 ms  |  3.7 GFLOPS
  Size    32 MB...    87.8 GB/s (129.1%)    1.15 ms  |  7.3 GFLOPS
  Size    64 MB...    88.1 GB/s (129.5%)    2.29 ms  |  7.3 GFLOPS
  Size   128 MB...    80.6 GB/s (118.6%)    4.99 ms  |  6.7 GFLOPS
  Size   256 MB...    94.0 GB/s (138.2%)    8.57 ms  |  7.8 GFLOPS
  Size   512 MB...    87.5 GB/s (128.7%)   18.41 ms  |  7.3 GFLOPS

TRIAD (a[i] = b[i] + q * c[i]):
  Memory ops: 3, FLOPs/element: 2
  Size     8 MB...    44.7 GB/s  (65.7%)    0.56 ms  |  7.4 GFLOPS
  Size    16 MB...    34.9 GB/s  (51.3%)    1.44 ms  |  5.8 GFLOPS
  Size    32 MB...    53.8 GB/s  (79.1%)    1.87 ms  |  9.0 GFLOPS
  Size    64 MB...    47.2 GB/s  (69.4%)    4.26 ms  |  7.9 GFLOPS
  Size   128 MB...    53.0 GB/s  (78.0%)    7.59 ms  |  8.8 GFLOPS
  Size   256 MB...    56.2 GB/s  (82.7%)   14.32 ms  |  9.4 GFLOPS
  Size   512 MB...    56.9 GB/s  (83.7%)   28.29 ms  |  9.5 GFLOPS

================================================================================
STREAM Summary:
--------------------------------------------------------------------------------
Kernel            Best BW    Latency   Efficiency Description
------------------------------------------------------------------------------------------
COPY               60.0 GB/s     4.48 ms       88.2%  a[i] = b[i]
SCALE              92.8 GB/s     2.89 ms      136.5%  a[i] = q * b[i]
ADD                94.0 GB/s     8.57 ms      138.2%  a[i] = b[i] + c[i]
TRIAD              56.9 GB/s    28.29 ms       83.7%  a[i] = b[i] + q * c[i]

STREAM Score (minimum): 16.9 GB/s
================================================================================
STREAM Score (minimum bandwidth): 16.9 GB/s

2. BLAS Compute Benchmark Suite
--------------------------------------------------------------------------------
Testing precisions: fp64, fp32, fp16, fp8, fp4, bf16, tf32, int64, int32, int16, int8, int4

Framework: PyTorch 2.5.0a0+872d972e41.nv24.08
  Device: Orin
  CUDA:   12.6

BLAS Benchmark Suite:
==========================================================================================

BLAS Level 1: DOT
------------------------------------------------------------------------------------------
  Size     1K:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.18ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.20ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.18ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    fp32        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.11ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp16        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.11ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.14ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp32        ⚠ SLOW (   0.7 GFLOPS < 1.0 GFLOPS threshold)    0.27ms
    tf32                                           1.3 GFLOPS    0.15ms
    fp16                                           2.6 GFLOPS    0.08ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.3 GFLOPS    0.15ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           5.3 GFLOPS    0.38ms
    fp32                                          14.2 GFLOPS    0.14ms
    tf32                                           7.3 GFLOPS    0.27ms
    fp16                                          14.7 GFLOPS    0.14ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.9 GFLOPS    0.68ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           8.0 GFLOPS    2.50ms
    fp32                                          17.7 GFLOPS    1.13ms
    tf32                                          19.1 GFLOPS    1.05ms
    fp16                                          32.6 GFLOPS    0.61ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          32.5 GFLOPS    0.62ms
    int64                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "dot" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "dot" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 1: AXPY
------------------------------------------------------------------------------------------
  Size     1K:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.58ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.31ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.30ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.52ms
    int64        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.32ms
    int32        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.42ms
    int16        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.16ms
    int8         ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10K:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.28ms
    fp32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.15ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.16ms
    int64        ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.33ms
    int32        ⚠ SLOW (   0.0 GIOPS < 1.0 GFLOPS threshold)    0.78ms
    int16        ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.25ms
    int8         ⚠ SLOW (   0.1 GIOPS < 1.0 GFLOPS threshold)    0.28ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size   100K:
    fp64        ⚠ SLOW (   0.8 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp32        ⚠ SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.23ms
    tf32        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.67ms
    fp16        ⚠ SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.22ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.9 GFLOPS < 1.0 GFLOPS threshold)    0.21ms
    int64                                           1.0 GIOPS    0.19ms
    int32        ⚠ SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.23ms
    int16                                           1.1 GIOPS    0.18ms
    int8         ⚠ SLOW (   0.9 GIOPS < 1.0 GFLOPS threshold)    0.22ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1M:
    fp64                                           2.6 GFLOPS    0.77ms
    fp32                                           2.1 GFLOPS    0.97ms
    tf32                                           4.8 GFLOPS    0.42ms
    fp16                                           5.6 GFLOPS    0.36ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.0 GFLOPS    0.25ms
    int64                                           2.6 GIOPS    0.76ms
    int32                                           4.7 GIOPS    0.43ms
    int16                                           8.0 GIOPS    0.25ms
    int8                                            8.8 GIOPS    0.23ms
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    10M:
    fp64                                           2.6 GFLOPS    7.63ms
    fp32                                           6.4 GFLOPS    3.14ms
    tf32                                           6.4 GFLOPS    3.14ms
    fp16                                          12.0 GFLOPS    1.67ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          12.0 GFLOPS    1.67ms
    int64                                           3.0 GIOPS    6.76ms
    int32                                           5.7 GIOPS    3.52ms
    int16                                          11.3 GIOPS    1.77ms
    int8                                           20.2 GIOPS    0.99ms
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 2: GEMV
------------------------------------------------------------------------------------------
  Size     32:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.33ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.33ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    fp16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.36ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.0 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.37ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.38ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp32        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    tf32        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.68ms
    fp16        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.24ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                           2.1 GFLOPS    0.25ms
    fp32                                           2.1 GFLOPS    0.24ms
    tf32        ⚠ SLOW (   0.5 GFLOPS < 1.0 GFLOPS threshold)    0.96ms
    fp16                                           2.0 GFLOPS    0.26ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           2.1 GFLOPS    0.26ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                           8.1 GFLOPS    0.26ms
    fp32                                           8.5 GFLOPS    0.25ms
    tf32                                           8.5 GFLOPS    0.25ms
    fp16                                           7.9 GFLOPS    0.27ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           8.0 GFLOPS    0.26ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          11.2 GFLOPS    0.75ms
    fp32                                          18.2 GFLOPS    0.46ms
    tf32                                          24.5 GFLOPS    0.34ms
    fp16                                          28.9 GFLOPS    0.29ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           7.2 GFLOPS    1.17ms
    int64                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmv_impl_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

BLAS Level 3: GEMM
------------------------------------------------------------------------------------------
  Size     32:
    fp64        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.40ms
    fp32        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.25ms
    tf32        ⚠ SLOW (   0.2 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    fp16        ⚠ SLOW (   0.1 GFLOPS < 1.0 GFLOPS threshold)    0.88ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16        ⚠ SLOW (   0.3 GFLOPS < 1.0 GFLOPS threshold)    0.26ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     64:
    fp64                                           1.4 GFLOPS    0.38ms
    fp32                                           2.1 GFLOPS    0.25ms
    tf32                                           2.1 GFLOPS    0.25ms
    fp16                                           1.3 GFLOPS    0.40ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                           1.4 GFLOPS    0.36ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    128:
    fp64                                           9.6 GFLOPS    0.44ms
    fp32                                          16.4 GFLOPS    0.26ms
    tf32                                          15.5 GFLOPS    0.27ms
    fp16                                          10.3 GFLOPS    0.41ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          14.8 GFLOPS    0.28ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    256:
    fp64                                           9.9 GFLOPS    3.38ms
    fp32                                          93.4 GFLOPS    0.36ms
    tf32                                         101.2 GFLOPS    0.33ms
    fp16                                          95.7 GFLOPS    0.35ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                          97.2 GFLOPS    0.35ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size    512:
    fp64                                          26.7 GFLOPS   10.06ms
    fp32                                         485.0 GFLOPS    0.55ms
    tf32                                         501.6 GFLOPS    0.54ms
    fp16                                         501.8 GFLOPS    0.53ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                         505.5 GFLOPS    0.53ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     1K:
    fp64                                          26.8 GFLOPS   80.07ms
    fp32                                         821.6 GFLOPS    2.61ms
    tf32                                        1435.8 GFLOPS    1.50ms
    fp16                                        2327.6 GFLOPS    0.92ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        2117.9 GFLOPS    1.01ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)
  Size     2K:
    fp64                                          27.1 GFLOPS  633.36ms
    fp32                                        1095.8 GFLOPS   15.68ms
    tf32                                        2307.4 GFLOPS    7.45ms
    fp16                                        4542.3 GFLOPS    3.78ms
    fp8                                               SKIPPED  (PyTorch does not support this precision)
    fp4                                               SKIPPED  (PyTorch does not support this precision)
    bf16                                        5678.6 GFLOPS    3.03ms
    int64                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Long')
    int32                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Int')
    int16                                             SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Short')
    int8                                              SKIPPED  (RuntimeError: "addmm_cuda" not implemented for 'Char')
    int4                                              SKIPPED  (PyTorch does not support this precision)

==========================================================================================
BLAS Suite Complete: 24 calibrations
==========================================================================================

Building precision capability matrix...

================================================================================
Hardware Calibration: Jetson Orin Nano (GPU)
Date: 2025-11-27T15:19:01.498710
================================================================================

Framework: PYTORCH
Device:    CUDA

Theoretical Specifications:
  Peak GFLOPS (FP32): 1880.0
  Peak Bandwidth:     68.0 GB/s

STREAM Memory Bandwidth Benchmark:
  Kernel           Size (MB)    Bandwidth    Latency   Efficiency Description
  -----------------------------------------------------------------------------------------------
  COPY                   128       60.0 GB/s     4.48 ms       88.2%  a[i] = b[i]
  SCALE                  128       92.8 GB/s     2.89 ms      136.5%  a[i] = q * b[i]
  ADD                    256       94.0 GB/s     8.57 ms      138.2%  a[i] = b[i] + c[i]
  TRIAD                  512       56.9 GB/s    28.29 ms       83.7%  a[i] = b[i] + q * c[i]

  STREAM Score (minimum): 16.9 GB/s

BLAS Performance Summary (Highest Throughput by Precision):
  Operation          fp64       fp32       tf32       fp16        fp8        fp4       bf16      int64      int32      int16       int8       int4   Best Precision
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  AXPY                2.6        6.4        6.4       12.0        N/A        N/A       12.0        3.0        5.7       11.3       20.2        N/A             int8
  DOT                 8.0       17.7       19.1       32.6        N/A        N/A       32.5        N/A        N/A        N/A        N/A        N/A             fp16
  GEMV               11.2       18.2       24.5       28.9        N/A        N/A        8.0        N/A        N/A        N/A        N/A        N/A             fp16
  GEMM               27.1     1095.8     2307.4     4542.3        N/A        N/A     5678.6        N/A        N/A        N/A        N/A        N/A             bf16

BLAS Compute Performance (by Operation and Precision):
========================================================================================================================

Level 1: Vector-Vector (O(n))
------------------------------------------------------------------------------------------------------------------------

AXPY:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         2.6 GFLOPS     7.63ms     0.33        4.4%
  fp32              10M         6.4 GFLOPS     3.14ms     0.17        0.3%
  tf32              10M         6.4 GFLOPS     3.14ms     0.17        0.2%
  fp16              10M        12.0 GFLOPS     1.67ms     0.33        0.2%
  bf16              10M        12.0 GFLOPS     1.67ms     0.33        0.2%
  int64             10M          3.0 GIOPS     6.76ms     0.33        0.0%
  int32             10M          5.7 GIOPS     3.52ms     0.33        0.3%
  int16             10M         11.3 GIOPS     1.77ms     0.33        0.2%
  int8              10M         20.2 GIOPS     0.99ms     0.33        0.1%

DOT:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64              10M         8.0 GFLOPS     2.50ms     0.50       13.3%
  fp32              10M        17.7 GFLOPS     1.13ms     0.25        0.9%
  tf32              10M        19.1 GFLOPS     1.05ms     0.25        0.5%
  fp16              10M        32.6 GFLOPS     0.61ms     0.50        0.4%
  bf16              10M        32.5 GFLOPS     0.62ms     0.50        0.4%

Level 2: Matrix-Vector (O(n²))
------------------------------------------------------------------------------------------------------------------------

GEMV:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K        11.2 GFLOPS     0.75ms     1.00       18.7%
  fp32               2K        18.2 GFLOPS     0.46ms     0.50        1.0%
  tf32               2K        24.5 GFLOPS     0.34ms     0.50        0.7%
  fp16               2K        28.9 GFLOPS     0.29ms     1.00        0.4%
  bf16               1K         8.0 GFLOPS     0.26ms     1.00        0.1%

Level 3: Matrix-Matrix (O(n³))
------------------------------------------------------------------------------------------------------------------------

GEMM:
  Precision   Best Size Highest Throughput    Latency       AI   Efficiency
  --------------------------------------------------------------------------------
  fp64               2K        27.1 GFLOPS   633.36ms   682.67       45.2%
  fp32               2K      1095.8 GFLOPS    15.68ms   341.33       58.3%
  tf32               2K      2307.4 GFLOPS     7.45ms   341.33       61.4%
  fp16               2K      4542.3 GFLOPS     3.78ms   682.67       60.4%
  bf16               2K      5678.6 GFLOPS     3.03ms   682.67       75.5%

Precision Support Summary:
  Supported:   fp64, fp32, tf32, fp16, bf16, int64, int32, int16, int8
  Unsupported: fp8, fp4, int4


============================================================
Calibration Complete!
============================================================

Profile saved: jetson_orin_nano_gpu
Location:      /home/branes/dev/branes/clones/graphs/hardware_registry/gpu/jetson_orin_nano_gpu

Use in analysis:
  ./cli/analyze.py --model resnet18 --hardware jetson_orin_nano_gpu


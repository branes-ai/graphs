# Calibration Workflow

Current state:
  - measure_efficiency.py + run_full_calibration.py collect per-subgraph timing data and aggregate into efficiency curves
  - Calibration data exists in calibration_data/ for Jetson Orin (4 power modes), i7-12700K, Ryzen 9
  - But: the efficiency curves are never consumed -- the unified analyzer uses hardcoded theoretical specs, not measured data
  - And: the validation tests (validation/estimators/) only check estimator sanity (no NaN, positive values), they never compare against measured ground
   truth
  - So every time you want to check "is my latency estimate right?", you must re-measure on hardware, which is the bottleneck you're hitting

Target architecture:

  PHYSICAL HARDWARE (run once)          DEVELOPMENT MACHINE (run repeatedly)

    Benchmark Sweep                       Estimator Development
    ─────────────                         ─────────────────────
    run_full_calibration.py               compare against ground truth
           │                              calibrate model params
           ▼                              regression-test estimates
    calibration_data/                            │
    └─ <hw_id>/                                  │
       └─ ground_truth/          ◄───────────────┘
          ├─ raw_measurements/        (reads stored data, no HW needed)
          ├─ summary.json
          └─ manifest.json

## Task breakdown:

Here's the dependency graph and execution plan:

```text
  Task 1: Ground-truth schema          ← START HERE (unblocks everything)
    ├──► Task 2: Benchmark sweep tool   (next hardware run uses new schema)
    ├──► Task 3: Convert existing data  (immediate value, no hardware needed)
    │      ├──► Task 6: Feed curves into roofline  (accuracy improvement)
    │      └──► Task 4: Offline validation framework  (the core workflow change)
    │             ├──► Task 5: Regression tests  (CI guard rails)
    │             └──► Task 7: Error diagnosis tooling  (daily dev workflow)

  Critical path: 1 → 3 → 4 → 5/7
```

Key insight: Tasks 3+4 are where the biggest workflow improvement lands. Once existing measurement data is in a canonical format and the offline
validation framework exists, you can iterate on estimator models purely on your dev machine. The Jetson only gets touched when you need to expand the
ground-truth dataset (Task 2).

What changes day-to-day:
  - Before: tweak estimator → ssh to Jetson → run model → wait → compare → repeat
  - After: tweak estimator → python cli/validate_estimators.py --all → see error table in seconds 
            → drill into worst subgraphs with diagnosis tool → repeat



● Implementation is complete. Here's a summary of what was done:

  Summary

  Files Created

  1. src/graphs/calibration/ground_truth.py - Schema v2.0 dataclasses and GroundTruthLoader
    - LatencyStats, EfficiencyStats, SubgraphMeasurement - moved from measure_efficiency.py to be self-contained
    - ModelSummary - whole-model aggregates with from_subgraphs() factory
    - SystemState - hardware conditions during measurement
    - MeasurementRecord - top-level record with v1.0 backward compat (from_dict() infers batch_size from filename, computes model_summary on the fly)
    - Manifest, ManifestEntry, ManifestCoverage - index format
    - GroundTruthLoader - query interface with list_hardware(), list_models(), list_configurations(), load(), load_model_summary(), load_all(),
  rebuild_manifest()
  2. cli/migrate_measurements.py - Migration script with --dry-run, scans both measurements/ and calibration_data/, converts v1.0 to v2.0, generates
  manifests

  Files Modified

  3. src/graphs/calibration/__init__.py - Exports new types (MeasurementRecord, ModelSummary, SystemState, GroundTruthLoader, etc.)
  4. cli/measure_efficiency.py - Now emits v2.0 JSON:
    - Imports dataclasses from ground_truth.py instead of defining locally
    - Adds batch_size, input_shape, model_summary, system_state, run_id to output
    - New capture_system_state() helper queries GPU/CPU clocks at measurement time
  5. cli/run_full_calibration.py - Uses canonical storage path (calibration_data/<hw>/measurements/<prec>/) and rebuilds manifest after calibration

  Verification

  - All 556 existing tests pass (0 failures)
  - GroundTruthLoader correctly finds 6 hardware IDs, 45 configs per Jetson mode
  - v1.0 backward compat: loads old files, infers batch_size, computes model_summary
  - Filename parsing correctly handles ambiguous names (efficientnet_b0, efficientnet_b1, vit_b_16)
  - Migration dry-run finds all 661 measurement files across both directories
  - Manifest generation produces correct coverage and summary data

● Here's where we stand:

  Completed: Ground-Truth Data Schema and Storage (Task 1)

  All code is written and verified:
▒ ┌────────────────────────────────────────┬───────────────────────────────────────────────┐
  │                  File                  │                    Status                     │
  ├────────────────────────────────────────┼───────────────────────────────────────────────┤
  │ src/graphs/calibration/ground_truth.py │ Created - schema v2.0 + GroundTruthLoader     │
  ├────────────────────────────────────────┼───────────────────────────────────────────────┤
  │ src/graphs/calibration/__init__.py     │ Modified - exports new types                  │
  ├────────────────────────────────────────┼───────────────────────────────────────────────┤
  │ cli/measure_efficiency.py              │ Modified - emits v2.0 JSON                    │
  ├────────────────────────────────────────┼───────────────────────────────────────────────┤
  │ cli/run_full_calibration.py            │ Modified - canonical paths + manifest rebuild │
  ├────────────────────────────────────────┼───────────────────────────────────────────────┤
  │ cli/migrate_measurements.py            │ Created - migration script                    │
  └────────────────────────────────────────┴───────────────────────────────────────────────┘
  Verification Results

  - 556 tests pass, 0 failures
  - GroundTruthLoader reads all 6 hardware targets, 216 total configurations
  - v1.0 backward compat works (infers batch_size, computes model_summary on the fly)
  - Filename ambiguity resolved (efficientnet_b1 vs _b1 batch suffix)
  - Migration dry-run finds all 661 measurement files

  What's NOT yet done

  - Migration not executed -- cli/migrate_measurements.py exists but hasn't been run (only --dry-run tested). Running it will consolidate measurements/
  into calibration_data/<hw>/measurements/<prec>/ and generate manifest.json per hardware.
  - No new measurements collected -- existing data is still v1.0 on disk. Next measure_efficiency.py run will emit v2.0.
  - Nothing committed -- all changes are local, uncommitted.


● Migration complete. Quick sanity check on the results:

● Bash(echo "=== Manifests generated ===" && find /home/stillwater/dev/branes/clones/graphs/calibration_data -name "manifest.json" | sort && echo "" &&
      echo "=== Sampl…)
  ⎿  === Manifests generated ===
     /home/stillwater/dev/branes/clones/graphs/calibration_data/i7_12700K/manifest.json
     /home/stillwater/dev/branes/clones/graphs/calibration_data/intel_core_i7_12700k/manifest.json
     … +31 lines (ctrl+o to expand)

● All good. Here's the result:

  - 661 files migrated, 0 failures, 0 duplicates
  - 11 manifest.json files generated (one per hardware ID)
  - All files upgraded to schema v2.0 with batch_size, model_summary, and tool_version: v2.0 (migrated)
  - Canonical layout: calibration_data/<hw>/measurements/<prec>/<model>_b<batch>.json

  The old data in measurements/ and calibration_data/<hw>/<prec>/measurements/ still exists (migration copies, doesn't delete). Want me to commit these
  changes?

❯ are the query tools now using the migrated database

● Good question. Let me check what actually reads measurement data today.

● Searched for 2 patterns (ctrl+o to expand)

● Explore(Find all measurement data consumers)
▒ ⎿  Done (15 tool uses · 89.5k tokens · 53s)

● No, they're not. The migration wrote v2.0 files to the canonical layout, but the existing query tools still use their own JSON reading code. Two tools
   are clear candidates for switching:
  ┌───────────────────────────────────────────┬───────────────────────────────────────┬────────────────────────────────────────────────┐
  │                   Tool                    │           Current approach            │         Should use GroundTruthLoader?          │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ cli/query_calibration_data.py             │ Hand-rolled Path.glob() + json.load() │ Yes -- this is exactly what the loader does    │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ cli/aggregate_efficiency.py               │ Glob patterns + json.load()           │ Yes -- could use load_all()                    │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ cli/calibrate_efficiency.py               │ Produces data (subprocess)            │ No -- writer, not reader                       │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ src/graphs/calibration/gpu_calibration.py │ Reads efficiency_curves.json          │ No -- reads aggregate curves, not measurements │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ cli/validate_efficiency_schema.py         │ Reads efficiency_curves.json          │ No -- validates curves, not measurements       │
  ├───────────────────────────────────────────┼───────────────────────────────────────┼────────────────────────────────────────────────┤
  │ Others                                    │ Different schema or one-time scripts  │ No                                             │
  └───────────────────────────────────────────┴───────────────────────────────────────┴────────────────────────────────────────────────┘
  Want me to update query_calibration_data.py and aggregate_efficiency.py to use the GroundTruthLoader?



● 556 passed, 0 failures. Now let me also quickly verify the aggregate_efficiency.py still works with the new default loader path:

● Bash(python cli/aggregate_efficiency.py --hardware-id jetson_orin_agx_maxn --precision FP32 --output /dev/null --dry-run 2>&1 | head -20)
  ⎿  Loaded 15 record(s) via GroundTruthLoader
       efficientnet_b0 (b1): 59 observations
       efficientnet_b1 (b1): 87 observations
     … +17 lines (ctrl+o to expand)

● Works. The --hardware-id flag now uses GroundTruthLoader by default without needing --from-loader.

● All three done. Here's a summary of what was changed:

  1. Deprecated old glob path in aggregate_efficiency.py
  - --hardware-id now uses GroundTruthLoader by default (no --from-loader needed)
  - --input, --input-dir, --hardware flags emit DeprecationWarning
  - Legacy glob path prints a visible WARNING: Using deprecated glob-based file loading banner

  2. Added CI check (ci/check_no_legacy_json_reads.sh)
  - New data-hygiene job in .github/workflows/ci.yml
  - Checks for: load_measurement_file() outside safe-listed files, json.load() near measurement contexts, glob patterns on measurement dirs, existence
  of old measurements/ directory
  - Safe-listed files: ground_truth.py, migration scripts, tools that read efficiency curves (not measurements)
  - CI will fail if anyone adds new direct JSON measurement reads

  3. Removed old measurements/ directory
  - git rm -r measurements/ -- 445 duplicate JSON files removed
  - All that data already exists in calibration_data/<hw>/measurements/<prec>/

  All 556 tests pass. CI check passes clean.


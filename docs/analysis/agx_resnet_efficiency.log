
================================================================================
  EFFICIENCY MEASUREMENT
================================================================================
  Model:     resnet152
  Hardware:  Jetson-Orin-AGX
  Thermal:   50W
  Device:    cuda
  Precision: FP32
  Runs:      10 warmup + 50 timed
================================================================================

Step 1: Creating model...
Step 2: Tracing with symbolic_trace...
Step 3: Partitioning...
  209 subgraphs, 23.04 GFLOPs
Step 4: Loading hardware specifications...
  Theoretical peak: 2.66TFLOPS
Step 5: Measuring node times (10 warmup + 50 timed)...
  Collected times for 517 nodes
Step 6: Computing efficiency statistics...
  209 subgraphs measured

  SG   Pattern                        OpType                FLOPs  Lat(ms)     Eff Eff Std
  ---------------------------------------------------------------------------------------------------------
  0    Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     236.0M    1.749   0.051   0.003
  1    Unfused                        unfused                   0    0.273   0.000   0.000
  2    Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm      25.7M    1.321   0.007   0.001
  3    Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.497   0.059   0.008
  4    Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.265   0.031   0.007
  5    Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.238   0.032   0.008
  6    add_ReLU                       activation             803K    0.356   0.001   0.000
  7    Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.361   0.029   0.005
  8    Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.165   0.076   0.013
  9    Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.168   0.035   0.010
  10   add_ReLU                       activation             803K    0.351   0.001   0.000
  11   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.290   0.031   0.007
  12   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.135   0.078   0.009
  13   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.012   0.042   0.015
  14   add_ReLU                       activation             803K    0.349   0.001   0.000
  15   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     205.5M    1.350   0.060   0.016
  16   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.275   0.071   0.015
  17   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.969   0.043   0.013
  18   Conv2d_BatchNorm2d             conv2d_batchnorm     205.5M    1.413   0.056   0.009
  19   add_ReLU                       activation             401K    0.276   0.001   0.000
  20   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.354   0.029   0.003
  21   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.120   0.080   0.015
  22   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.847   0.049   0.017
  23   add_ReLU                       activation             401K    0.273   0.001   0.000
  24   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.311   0.031   0.006
  25   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.078   0.082   0.014
  26   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.891   0.049   0.020
  27   add_ReLU                       activation             401K    0.273   0.001   0.000
  28   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.253   0.032   0.009
  29   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.064   0.082   0.009
  30   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.829   0.050   0.016
  31   add_ReLU                       activation             401K    0.274   0.001   0.000
  32   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.243   0.032   0.007
  33   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.056   0.083   0.011
  34   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.825   0.052   0.020
  35   add_ReLU                       activation             401K    0.274   0.001   0.000
  36   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.284   0.031   0.007
  37   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.072   0.082   0.010
  38   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.853   0.049   0.017
  39   add_ReLU                       activation             401K    0.271   0.001   0.000
  40   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.190   0.034   0.009
  41   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.105   0.079   0.007
  42   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.789   0.055   0.021
  43   add_ReLU                       activation             401K    0.274   0.001   0.000
  44   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.217   0.033   0.008
  45   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.099   0.081   0.012
  46   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.796   0.053   0.018
  47   add_ReLU                       activation             401K    0.271   0.001   0.000
  48   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     205.5M    1.311   0.061   0.013
  49   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.189   0.074   0.011
  50   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.020   0.048   0.054
  51   Conv2d_BatchNorm2d             conv2d_batchnorm     205.5M    1.389   0.057   0.009
  52   add_ReLU                       activation             201K    0.269   0.000   0.000
  53   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.244   0.033   0.009
  54   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.179   0.075   0.010
  55   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.986   0.049   0.053
  56   add_ReLU                       activation             201K    0.268   0.000   0.000
  57   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.102   0.038   0.011
  58   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.175   0.075   0.008
  59   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.986   0.042   0.015
  60   add_ReLU                       activation             201K    0.269   0.000   0.000
  61   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.120   0.037   0.010
  62   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.178   0.075   0.010
  63   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.956   0.046   0.020
  64   add_ReLU                       activation             201K    0.267   0.000   0.000
  65   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.146   0.036   0.012
  66   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.138   0.078   0.012
  67   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.876   0.056   0.054
  68   add_ReLU                       activation             201K    0.272   0.000   0.000
  69   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.130   0.037   0.013
  70   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.170   0.075   0.007
  71   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.917   0.047   0.018
  72   add_ReLU                       activation             201K    0.270   0.000   0.000
  73   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.063   0.038   0.010
  74   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.195   0.073   0.004
  75   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.972   0.044   0.017
  76   add_ReLU                       activation             201K    0.268   0.000   0.000
  77   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.140   0.037   0.013
  78   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.150   0.077   0.010
  79   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.908   0.048   0.019
  80   add_ReLU                       activation             201K    0.270   0.000   0.000
  81   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.067   0.040   0.014
  82   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.168   0.076   0.011
  83   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.814   0.052   0.018
  84   add_ReLU                       activation             201K    0.269   0.000   0.000
  85   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.157   0.035   0.009
  86   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.149   0.078   0.014
  87   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.849   0.051   0.019
  88   add_ReLU                       activation             201K    0.268   0.000   0.000
  89   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.197   0.034   0.009
  90   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.160   0.076   0.009
  91   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.874   0.049   0.018
  92   add_ReLU                       activation             201K    0.272   0.000   0.000
  93   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.155   0.036   0.011
  94   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.162   0.076   0.011
  95   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.835   0.054   0.023
  96   add_ReLU                       activation             201K    0.269   0.000   0.000
  97   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.105   0.038   0.013
  98   Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.185   0.074   0.006
  99   Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.948   0.044   0.013
  100  add_ReLU                       activation             201K    0.270   0.000   0.000
  101  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.139   0.037   0.013
  102  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.193   0.074   0.008
  103  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.877   0.050   0.020
  104  add_ReLU                       activation             201K    0.271   0.000   0.000
  105  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.114   0.037   0.011
  106  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.181   0.075   0.011
  107  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.922   0.047   0.019
  108  add_ReLU                       activation             201K    0.270   0.000   0.000
  109  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.096   0.039   0.014
  110  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.169   0.075   0.010
  111  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.914   0.046   0.015
  112  add_ReLU                       activation             201K    0.271   0.000   0.000
  113  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.182   0.035   0.013
  114  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.201   0.073   0.007
  115  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.880   0.048   0.017
  116  add_ReLU                       activation             201K    0.270   0.000   0.000
  117  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.156   0.036   0.013
  118  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.203   0.074   0.011
  119  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.856   0.049   0.017
  120  add_ReLU                       activation             201K    0.269   0.000   0.000
  121  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.138   0.035   0.008
  122  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.162   0.076   0.009
  123  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.847   0.051   0.019
  124  add_ReLU                       activation             201K    0.270   0.000   0.000
  125  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.206   0.034   0.009
  126  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.193   0.073   0.007
  127  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.901   0.049   0.020
  128  add_ReLU                       activation             201K    0.268   0.000   0.000
  129  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.043   0.041   0.014
  130  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.164   0.076   0.012
  131  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.963   0.044   0.017
  132  add_ReLU                       activation             201K    0.268   0.000   0.000
  133  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.123   0.037   0.012
  134  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.179   0.074   0.008
  135  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.905   0.047   0.018
  136  add_ReLU                       activation             201K    0.267   0.000   0.000
  137  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.184   0.035   0.009
  138  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.206   0.073   0.007
  139  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.925   0.047   0.018
  140  add_ReLU                       activation             201K    0.268   0.000   0.000
  141  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.227   0.033   0.009
  142  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.187   0.075   0.012
  143  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.914   0.046   0.016
  144  add_ReLU                       activation             201K    0.268   0.000   0.000
  145  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.098   0.039   0.014
  146  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.174   0.076   0.013
  147  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.899   0.047   0.018
  148  add_ReLU                       activation             201K    0.270   0.000   0.000
  149  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.183   0.034   0.008
  150  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.226   0.072   0.011
  151  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.890   0.047   0.016
  152  add_ReLU                       activation             201K    0.268   0.000   0.000
  153  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.111   0.037   0.010
  154  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.202   0.073   0.010
  155  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.839   0.051   0.019
  156  add_ReLU                       activation             201K    0.268   0.000   0.000
  157  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.149   0.036   0.010
  158  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.194   0.073   0.007
  159  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.842   0.052   0.021
  160  add_ReLU                       activation             201K    0.267   0.000   0.000
  161  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.129   0.037   0.012
  162  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.186   0.074   0.008
  163  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.882   0.051   0.022
  164  add_ReLU                       activation             201K    0.269   0.000   0.000
  165  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.123   0.037   0.012
  166  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.173   0.075   0.009
  167  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.932   0.047   0.020
  168  add_ReLU                       activation             201K    0.271   0.000   0.000
  169  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.073   0.038   0.011
  170  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.209   0.073   0.010
  171  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.871   0.048   0.016
  172  add_ReLU                       activation             201K    0.270   0.000   0.000
  173  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.273   0.033   0.011
  174  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.184   0.075   0.010
  175  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.827   0.053   0.022
  176  add_ReLU                       activation             201K    0.270   0.000   0.000
  177  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.130   0.036   0.011
  178  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.176   0.075   0.008
  179  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.907   0.046   0.016
  180  add_ReLU                       activation             201K    0.270   0.000   0.000
  181  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.142   0.036   0.011
  182  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.169   0.076   0.011
  183  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.880   0.049   0.020
  184  add_ReLU                       activation             201K    0.271   0.000   0.000
  185  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.119   0.036   0.010
  186  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.187   0.074   0.009
  187  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.783   0.055   0.021
  188  add_ReLU                       activation             201K    0.267   0.000   0.000
  189  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.149   0.035   0.010
  190  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.158   0.076   0.011
  191  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.801   0.056   0.024
  192  add_ReLU                       activation             201K    0.268   0.000   0.000
  193  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     205.5M    1.420   0.054   0.000
  194  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.501   0.059   0.008
  195  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    1.015   0.043   0.018
  196  Conv2d_BatchNorm2d             conv2d_batchnorm     205.5M    1.586   0.049   0.006
  197  add_ReLU                       activation             100K    0.270   0.000   0.000
  198  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.353   0.029   0.006
  199  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.494   0.059   0.009
  200  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.852   0.051   0.019
  201  add_ReLU                       activation             100K    0.271   0.000   0.000
  202  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     102.8M    1.249   0.033   0.009
  203  Conv2d_BatchNorm2d_ReLU        conv2d_batchnorm     231.2M    1.565   0.056   0.005
  204  Conv2d_BatchNorm2d             conv2d_batchnorm     102.8M    0.925   0.045   0.013
  205  add_ReLU                       activation             100K    0.269   0.000   0.000
  206  Unfused                        unfused                   0    0.589   0.000   0.000
  207  Unfused                        unfused                   0    0.112   0.000   0.000
  208  Unfused                        unfused                4.1M    0.679   0.002   0.000

Summary by Operation Type:
  Operation Type       Count  Total FLOPs  Avg Eff       Eff Range
  ----------------------------------------------------------------------
  activation              50        13.1M    0.000     0.000-0.001
  conv2d_batchnorm       155       23.02G    0.053     0.007-0.083
  unfused                  4         4.1M    0.001     0.000-0.002


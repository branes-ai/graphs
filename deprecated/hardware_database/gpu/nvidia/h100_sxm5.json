{
  "id": "h100_sxm5",
  "system": {
    "vendor": "NVIDIA",
    "model": "NVIDIA H100 SXM5 80GB",
    "architecture": "Hopper",
    "device_type": "gpu",
    "platform": "x86_64",
    "os_compatibility": ["linux", "windows"],
    "isa_extensions": ["CUDA", "Tensor Cores Gen4", "TMA", "DPX"],
    "special_features": [
      "CUDA Compute Capability 9.0",
      "NVLink 4.0 (900 GB/s bidirectional)",
      "PCIe Gen5",
      "Multi-Instance GPU (MIG)",
      "Transformer Engine with FP8",
      "HBM3 memory"
    ],
    "tdp_watts": 700.0,
    "max_power_watts": 700.0,
    "release_date": "2022-03-22",
    "manufacturer_url": "https://www.nvidia.com/en-us/data-center/h100/",
    "notes": "Flagship datacenter GPU with Hopper architecture. 80GB HBM3, 3.35 TB/s bandwidth. World's first GPU with Transformer Engine for FP8 training."
  },
  "detection_patterns": [
    "NVIDIA.*H100.*SXM5",
    "NVIDIA.*H100.*80GB",
    "H100.*SXM5",
    "H100"
  ],
  "core_info": {
    "cores": 132,
    "threads": 270336,
    "base_frequency_ghz": 1.095,
    "boost_frequency_ghz": 1.98,
    "core_clusters": [
      {
        "name": "Hopper SMs",
        "type": "compute",
        "count": 132,
        "architecture": "Hopper SM",
        "base_frequency_ghz": 1.095,
        "boost_frequency_ghz": 1.98,
        "cuda_cores_per_sm": 128,
        "tensor_cores_per_sm": 4,
        "fp32_units_per_sm": 64,
        "fp64_units_per_sm": 32,
        "int32_units_per_sm": 64,
        "load_store_units_per_sm": 32,
        "special_function_units_per_sm": 16,
        "warp_size": 32,
        "max_warps_per_sm": 64,
        "max_threads_per_sm": 2048,
        "registers_per_sm": 65536,
        "shared_memory_per_sm_kb": 228
      }
    ],
    "total_cuda_cores": 16896,
    "total_tensor_cores": 528,
    "total_sms": 132,
    "total_rt_cores": 0,
    "cuda_capability": "9.0"
  },
  "memory_subsystem": {
    "total_size_gb": 80,
    "peak_bandwidth_gbps": 3352.0,
    "memory_channels": [
      {
        "name": "HBM3 Stack 0",
        "type": "hbm3",
        "size_gb": 16,
        "frequency_mhz": 2619,
        "data_rate_mts": 5238,
        "bus_width_bits": 1024,
        "bandwidth_gbps": 670.4,
        "dies_per_stack": 8,
        "physical_position": 0
      },
      {
        "name": "HBM3 Stack 1",
        "type": "hbm3",
        "size_gb": 16,
        "frequency_mhz": 2619,
        "data_rate_mts": 5238,
        "bus_width_bits": 1024,
        "bandwidth_gbps": 670.4,
        "dies_per_stack": 8,
        "physical_position": 1
      },
      {
        "name": "HBM3 Stack 2",
        "type": "hbm3",
        "size_gb": 16,
        "frequency_mhz": 2619,
        "data_rate_mts": 5238,
        "bus_width_bits": 1024,
        "bandwidth_gbps": 670.4,
        "dies_per_stack": 8,
        "physical_position": 2
      },
      {
        "name": "HBM3 Stack 3",
        "type": "hbm3",
        "size_gb": 16,
        "frequency_mhz": 2619,
        "data_rate_mts": 5238,
        "bus_width_bits": 1024,
        "bandwidth_gbps": 670.4,
        "dies_per_stack": 8,
        "physical_position": 3
      },
      {
        "name": "HBM3 Stack 4",
        "type": "hbm3",
        "size_gb": 16,
        "frequency_mhz": 2619,
        "data_rate_mts": 5238,
        "bus_width_bits": 1024,
        "bandwidth_gbps": 670.4,
        "dies_per_stack": 8,
        "physical_position": 4
      }
    ]
  },
  "theoretical_peaks": {
    "fp64": 34000.0,
    "fp32": 67000.0,
    "fp16": 1979000.0,
    "fp8": 3958000.0,
    "fp4": 0.0,
    "bf16": 1979000.0,
    "tf32": 989000.0,
    "int64": 0.0,
    "int32": 67000.0,
    "int16": 0.0,
    "int8": 3958000.0,
    "int4": 0.0
  },
  "onchip_memory_hierarchy": {
    "cache_levels": [
      {
        "name": "L1 Data Cache/Shared Memory",
        "level": 1,
        "cache_type": "data",
        "scope": "per_sm",
        "size_per_unit_kb": 256,
        "associativity": null,
        "line_size_bytes": 128,
        "configurable": true,
        "note": "Configurable 256 KB split between L1 cache and shared memory (up to 228 KB shared memory)"
      },
      {
        "name": "L1 Instruction Cache",
        "level": 1,
        "cache_type": "instruction",
        "scope": "per_sm",
        "size_per_unit_kb": 32,
        "associativity": null,
        "line_size_bytes": 128
      },
      {
        "name": "L2 Cache",
        "level": 2,
        "cache_type": "unified",
        "scope": "shared",
        "total_size_kb": 51200,
        "associativity": null,
        "line_size_bytes": 128,
        "note": "50 MB unified L2 cache"
      }
    ]
  },
  "data_source": "manufacturer",
  "last_updated": "2025-11-26T12:00:00Z",
  "mapper": {
    "mapper_class": "GPUMapper",
    "mapper_config": {
      "prefer_tensor_cores": true,
      "enable_transformer_engine": true
    },
    "hints": {
      "preferred_block_size": [256, 1, 1],
      "max_threads_per_block": 1024,
      "warp_size": 32,
      "tensor_core_hint": "Use Tensor Cores for FP8/FP16/BF16/TF32 matrix operations (528 Gen4 Tensor Cores with Transformer Engine)",
      "memory_hint": "80 GB HBM3 with 3.35 TB/s bandwidth. Use large batch sizes to saturate memory bandwidth.",
      "occupancy_hint": "132 SMs Ã— 64 warps/SM = 8448 max concurrent warps. Target 50-75% occupancy for best performance.",
      "performance_notes": "Flagship Hopper GPU: 16896 CUDA cores, 528 Tensor Cores Gen4, 50 MB L2 cache. First GPU with Transformer Engine for FP8 training.",
      "architecture_notes": "Hopper architecture with Gen4 Tensor Cores supporting FP8. No RT cores (datacenter focus). Multi-Instance GPU (MIG) capable for workload isolation."
    }
  }
}
